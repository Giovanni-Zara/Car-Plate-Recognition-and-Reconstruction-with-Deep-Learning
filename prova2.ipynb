{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be60ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19731/2037165382.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND UTILS\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CTCLoss\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import math\n",
    "\n",
    "# DEVICE DEFINITION\n",
    "\n",
    "device=\"cpu\"\n",
    "\n",
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]\n",
    "\n",
    "# LOAD FASTER RCNN MODEL\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('model_weights/best_frcnn_model.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "\n",
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "MY_DICTIONARY = provinces + [c for c in alphabet if c not in provinces] + [c for c in ads if c not in provinces and c not in alphabet]    \n",
    "MY_DICTIONARY = list(dict.fromkeys(MY_DICTIONARY))  \n",
    "char2idx = {c: i for i, c in enumerate(MY_DICTIONARY)}\n",
    "idx2char = {i: c for i, c in enumerate(MY_DICTIONARY)}\n",
    "BLANK_IDX = len(MY_DICTIONARY)  \n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    return plate_text\n",
    "\n",
    "# CROP FUNCTION WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image)\n",
    "    cropped = img_tensor[:, int(bb[1]):int(bb[3]), int(bb[0]):int(bb[2])]\n",
    "    cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "    return cropped_resized.squeeze(0)\n",
    "\n",
    "def crop_folder_with_ground_truth(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text = get_text(file)  # Get ground truth text\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append([cropped_image, gt_text])  # Store image and text pair\n",
    "    return cropped_folder\n",
    "\n",
    "# Dataset\n",
    "class CroppedImages(Dataset):\n",
    "    def __init__(self, folder, transformations):\n",
    "        self.folder = folder\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = self.folder[idx][0]\n",
    "            gt_text = self.folder[idx][1]\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"None image at index {idx}\")\n",
    "                \n",
    "            if self.transformations:\n",
    "                image = self.transformations(image)\n",
    "                \n",
    "            return image, gt_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            return torch.zeros(3, 48, 144), \"\"\n",
    "\n",
    "def encode_labels(label_list, char2idx, max_len=8):\n",
    "    encoded = []\n",
    "    for label in label_list:\n",
    "        label = label[:max_len].ljust(max_len)\n",
    "        encoded.append([char2idx[c] for c in label])\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "# CTC collate function (per CRNN baseline)\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat([encode_labels([l], char2idx) for l in labels])\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "# PDLPR collate function (per PDLPR model)\n",
    "def pdlpr_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    \n",
    "    # Converti labels in sequenze di indici per PDLPR\n",
    "    max_length = 10  # Lunghezza massima targa (8 caratteri + start/end tokens)\n",
    "    blank_idx = len(MY_DICTIONARY)  # Blank token\n",
    "    \n",
    "    targets = []\n",
    "    for label in labels:\n",
    "        # Crea sequenza: [start_token, char1, char2, ..., end_token]\n",
    "        target = [blank_idx]  # Start token\n",
    "        for char in label:\n",
    "            if char in char2idx:\n",
    "                target.append(char2idx[char])\n",
    "        target.append(blank_idx)  # End token\n",
    "        \n",
    "        # Pad o tronca alla lunghezza massima\n",
    "        if len(target) > max_length:\n",
    "            target = target[:max_length]\n",
    "        else:\n",
    "            target.extend([blank_idx] * (max_length - len(target)))\n",
    "        \n",
    "        targets.append(target)\n",
    "    \n",
    "    targets = torch.tensor(targets, dtype=torch.long)\n",
    "    return images, targets\n",
    "\n",
    "def ctc_greedy_decoder(output, idx2char, blank=0):\n",
    "    out = output.permute(1, 0, 2) \n",
    "    pred_strings = []\n",
    "    for probs in out:\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        prev = -1\n",
    "        pred_str = []\n",
    "        for p in pred:\n",
    "            if p != blank and p != prev:\n",
    "                pred_str.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(''.join(pred_str))\n",
    "    return pred_strings\n",
    "\n",
    "trans = T.Compose([\n",
    "    T.Resize((48, 144)),\n",
    "    T.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "538bdf9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19731/2037165382.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model.pth'))\n"
     ]
    }
   ],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "model=load_Fasterrcnn(device=\"cpu\")\n",
    "device=\"cpu\"\n",
    "model.eval()\n",
    "\n",
    "cropped_folder_train=crop_folder_with_ground_truth(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train_small\")\n",
    "cropped_folder_eval=crop_folder_with_ground_truth(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval_small\")\n",
    "\n",
    "train_dataset = CroppedImages(cropped_folder_train, trans)\n",
    "eval_dataset = CroppedImages(cropped_folder_eval, trans)\n",
    "\n",
    "# CTC DataLoaders (per CRNN baseline)\n",
    "train_dataloader_ctc = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "eval_dataloader_ctc = DataLoader(eval_dataset, batch_size=8, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "# PDLPR DataLoaders (per PDLPR model)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=pdlpr_collate_fn)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False, collate_fn=pdlpr_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca6715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding per la sequenza di features\"\"\"\n",
    "    def __init__(self, d_model, max_len=100):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class CNNBackbone(nn.Module):\n",
    "    \"\"\"CNN Backbone per estrazione features come descritto nel paper\"\"\"\n",
    "    def __init__(self, input_channels=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Convolutional layers per feature extraction\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(2, 1), stride=(2, 1)),\n",
    "            \n",
    "            # Block 5\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, channels, height, width]\n",
    "        features = self.conv_layers(x)\n",
    "        # features shape: [batch_size, 512, H', W']\n",
    "        \n",
    "        # Reshape per creare sequenza di features\n",
    "        batch_size, channels, height, width = features.size()\n",
    "        # Combina height e channels, mantieni width come sequenza\n",
    "        features = features.view(batch_size, channels * height, width)\n",
    "        features = features.permute(2, 0, 1)  # [seq_len, batch_size, feature_dim]\n",
    "        \n",
    "        return features\n",
    "\n",
    "class AttentionDecoder(nn.Module):\n",
    "    \"\"\"Attention-based decoder come descritto nel PDLPR paper\"\"\"\n",
    "    def __init__(self, feature_dim, hidden_dim, num_classes, max_length=20):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # LSTM decoder\n",
    "        self.lstm = nn.LSTM(feature_dim + num_classes, hidden_dim, batch_first=False)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(hidden_dim + feature_dim, 1)\n",
    "        \n",
    "        # Output projection\n",
    "        self.out = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Embedding per previous character\n",
    "        self.embedding = nn.Embedding(num_classes, num_classes)\n",
    "        \n",
    "    def forward(self, encoder_features, target=None, max_length=None):\n",
    "        \"\"\"\n",
    "        encoder_features: [seq_len, batch_size, feature_dim]\n",
    "        target: [batch_size, target_length] per training\n",
    "        \"\"\"\n",
    "        if max_length is None:\n",
    "            max_length = self.max_length\n",
    "            \n",
    "        batch_size = encoder_features.size(1)\n",
    "        seq_len = encoder_features.size(0)\n",
    "        \n",
    "        # Initialize\n",
    "        hidden = self._init_hidden(batch_size, encoder_features.device)\n",
    "        outputs = []\n",
    "        \n",
    "        # Start token (blank)\n",
    "        input_char = torch.zeros(batch_size, self.num_classes).to(encoder_features.device)\n",
    "        \n",
    "        for t in range(max_length):\n",
    "            # Compute attention weights\n",
    "            attention_weights = self._compute_attention(hidden[0], encoder_features)\n",
    "            \n",
    "            # Apply attention to get context\n",
    "            context = torch.sum(attention_weights.unsqueeze(-1) * encoder_features, dim=0)\n",
    "            \n",
    "            # Concatenate context with previous character embedding\n",
    "            lstm_input = torch.cat([context, input_char], dim=1).unsqueeze(0)\n",
    "            \n",
    "            # LSTM step\n",
    "            output, hidden = self.lstm(lstm_input, hidden)\n",
    "            \n",
    "            # Predict next character\n",
    "            char_logits = self.out(output.squeeze(0))\n",
    "            outputs.append(char_logits)\n",
    "            \n",
    "            # Teacher forcing during training\n",
    "            if target is not None and t < target.size(1) - 1:\n",
    "                # Use ground truth\n",
    "                next_char_idx = target[:, t + 1]\n",
    "                input_char = self.embedding(next_char_idx)\n",
    "            else:\n",
    "                # Use prediction\n",
    "                next_char_idx = char_logits.argmax(dim=1)\n",
    "                input_char = self.embedding(next_char_idx)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)  # [batch_size, max_length, num_classes]\n",
    "    \n",
    "    def _init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)\n",
    "    \n",
    "    def _compute_attention(self, hidden, encoder_features):\n",
    "        \"\"\"\n",
    "        hidden: [batch_size, hidden_dim]\n",
    "        encoder_features: [seq_len, batch_size, feature_dim]\n",
    "        \"\"\"\n",
    "        seq_len, batch_size, feature_dim = encoder_features.size()\n",
    "        \n",
    "        # Expand hidden to match sequence length\n",
    "        hidden_expanded = hidden.unsqueeze(0).expand(seq_len, -1, -1)\n",
    "        \n",
    "        # Concatenate hidden with encoder features\n",
    "        combined = torch.cat([hidden_expanded, encoder_features], dim=2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = self.attention(combined).squeeze(-1)  # [seq_len, batch_size]\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(attention_scores, dim=0)\n",
    "        \n",
    "        return attention_weights\n",
    "\n",
    "class PDLPR(nn.Module):\n",
    "    \"\"\"\n",
    "    PDLPR: Progressive Dilated License Plate Recognition\n",
    "    Implementazione basata sul paper fornito\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, input_channels=3, hidden_dim=256, max_length=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # CNN Backbone per feature extraction\n",
    "        self.backbone = CNNBackbone(input_channels)\n",
    "        \n",
    "        # Calcola feature dimension dopo CNN\n",
    "        # Assumendo input 64x256, dopo conv layers: feature_dim = 512 * 4 = 2048\n",
    "        feature_dim = 512 * 4  # channels * remaining_height\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_encoding = PositionalEncoding(feature_dim)\n",
    "        \n",
    "        # Attention-based decoder\n",
    "        self.decoder = AttentionDecoder(feature_dim, hidden_dim, num_classes, max_length)\n",
    "        \n",
    "    def forward(self, x, target=None):\n",
    "        \"\"\"\n",
    "        x: [batch_size, channels, height, width]\n",
    "        target: [batch_size, target_length] per training (opzionale)\n",
    "        \"\"\"\n",
    "        # Extract features usando CNN backbone\n",
    "        encoder_features = self.backbone(x)  # [seq_len, batch_size, feature_dim]\n",
    "        \n",
    "        # Add positional encoding\n",
    "        encoder_features = self.pos_encoding(encoder_features)\n",
    "        \n",
    "        # Decode con attention\n",
    "        outputs = self.decoder(encoder_features, target)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def predict(self, x, idx2char, blank_idx):\n",
    "        \"\"\"Prediction senza teacher forcing\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(x)\n",
    "            predictions = outputs.argmax(dim=-1)  # [batch_size, max_length]\n",
    "            \n",
    "            # Decodifica predizioni\n",
    "            results = []\n",
    "            for pred in predictions:\n",
    "                chars = []\n",
    "                for char_idx in pred:\n",
    "                    char_idx = char_idx.item()\n",
    "                    if char_idx == blank_idx:  # Stop al blank\n",
    "                        break\n",
    "                    if char_idx in idx2char:\n",
    "                        chars.append(idx2char[char_idx])\n",
    "                results.append(''.join(chars))\n",
    "            \n",
    "            return results\n",
    "\n",
    "model=PDLPR(len(MY_DICTIONARY)+1)\n",
    "# Training function per PDLPR\n",
    "def train_pdlpr_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=model.num_classes)  # Ignora blank per loss\n",
    "    \n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass con teacher forcing\n",
    "        outputs = model(images, targets[:, :-1])  # Escludi ultimo token dal target\n",
    "        \n",
    "        # Reshape per loss computation\n",
    "        outputs = outputs.contiguous().view(-1, model.num_classes)\n",
    "        targets = targets[:, 1:].contiguous().view(-1)  # Escludi primo token (start)\n",
    "        \n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Batch {batch_idx}/{len(dataloader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_pdlpr(model, dataloader, idx2char, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    blank_idx = len(idx2char)  # Blank token index\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Predici\n",
    "            predictions = model.predict(images, idx2char, blank_idx)\n",
    "            \n",
    "            # Converti targets in stringhe per confronto\n",
    "            target_strings = []\n",
    "            for target in targets:\n",
    "                chars = []\n",
    "                for char_idx in target[1:-1]:  # Escludi start/end tokens (primi e ultimi blank)\n",
    "                    char_idx = char_idx.item()\n",
    "                    if char_idx < len(idx2char) and char_idx != blank_idx:  # Escludi blank tokens\n",
    "                        chars.append(idx2char[char_idx])\n",
    "                target_strings.append(''.join(chars))\n",
    "            \n",
    "            # Calcola accuracy\n",
    "            for pred, target in zip(predictions, target_strings):\n",
    "                total += 1\n",
    "                if pred == target:\n",
    "                    correct += 1\n",
    "                print(f\"Pred: '{pred}' | Target: '{target}' | Match: {pred == target}\")\n",
    "    \n",
    "    return correct / total if total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa2a83",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1536) must match the size of tensor b (2048) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m optimizer = torch.optim.Adam(model.parameters(), lr=\u001b[32m0.0005\u001b[39m, weight_decay=\u001b[32m0.0005\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain_pdlpr_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mtrain_pdlpr_epoch\u001b[39m\u001b[34m(model, dataloader, optimizer, device)\u001b[39m\n\u001b[32m    235\u001b[39m optimizer.zero_grad()\n\u001b[32m    237\u001b[39m \u001b[38;5;66;03m# Forward pass con teacher forcing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Escludi ultimo token dal target\u001b[39;00m\n\u001b[32m    240\u001b[39m \u001b[38;5;66;03m# Reshape per loss computation\u001b[39;00m\n\u001b[32m    241\u001b[39m outputs = outputs.contiguous().view(-\u001b[32m1\u001b[39m, model.num_classes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 196\u001b[39m, in \u001b[36mPDLPR.forward\u001b[39m\u001b[34m(self, x, target)\u001b[39m\n\u001b[32m    193\u001b[39m encoder_features = \u001b[38;5;28mself\u001b[39m.backbone(x)  \u001b[38;5;66;03m# [seq_len, batch_size, feature_dim]\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;66;03m# Add positional encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m encoder_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Decode con attention\u001b[39;00m\n\u001b[32m    199\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.decoder(encoder_features, target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mPositionalEncoding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (1536) must match the size of tensor b (2048) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n",
    "train_pdlpr_epoch(model,train_dataloader,optimizer,\"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
