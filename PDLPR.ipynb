{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aaf50d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNZIONI DA IMPORTARE \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    cropped_image = image.crop(bb)\n",
    "    return cropped_image\n",
    "\n",
    "def crop_folder(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append(cropped_image)\n",
    "    return cropped_folder\n",
    "\n",
    "cropped_folder=crop_folder(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a1d8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE EXTRACTION MODEL --> [BATCH,512,6,18]\n",
    "\n",
    "from torchvision.models import mobilenet_v2\n",
    "import torch.nn as nn\n",
    "\n",
    "def Mobilenet_V2_reshaped(image):\n",
    "    backbone = mobilenet_v2(pretrained=True)\n",
    "    backbone = nn.Sequential(*list(backbone.features.children()))\n",
    "    features=backbone(image)\n",
    "    projection = nn.Sequential(\n",
    "    nn.Conv2d(1280, 512, kernel_size=1),\n",
    "    nn.AdaptiveAvgPool2d((6, 18)),\n",
    "    )\n",
    "    output = projection(features)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43aa590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS FOR THE FOLDER\n",
    "\n",
    "class cropped_images(torch.utils.data.Dataset): # takes as input the cropped folder\n",
    "\n",
    "    def __init__(self,folder,transformations):\n",
    "        self.folder=folder\n",
    "        self.transformations=transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.folder[idx]\n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "\n",
    "        return Mobilenet_V2_reshaped(image.unsqueeze(0))\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "input_folder=cropped_images(cropped_folder, transformations=trans)\n",
    "input_dataloader=DataLoader(input_folder, batch_size=2)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=200):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)  # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # [max_len, 1]\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].to(x.device)\n",
    "        return x\n",
    "\n",
    "pos_encoder = PositionalEncoding(d_model=512)\n",
    "\n",
    "for image in input_dataloader: # creo dei batch, resizo in modo tale che ogni batch abbia come size: [batch,108,512]. Quindi da: [A,B,C,D] --> [A,C*D,B] e applico l'encoding\n",
    "    A,B,C,D,E=image.shape\n",
    "    image=image.view(A,C,D*E)\n",
    "    image=image.permute(0,2,1)\n",
    "    image=pos_encoder(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "encoder_layer = TransformerEncoderLayer(d_model=512, nhead=8, dim_feedforward=2048)\n",
    "transformer_encoder = TransformerEncoder(encoder_layer, num_layers=4)\n",
    "\n",
    "encoded = transformer_encoder(image)  # [B, 108, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e58b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHAT VERSION\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Dataset\n",
    "class CroppedImages(Dataset):\n",
    "    def __init__(self, folder, transformations):\n",
    "        self.folder = folder\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.folder[idx]\n",
    "        return self.transformations(image)\n",
    "\n",
    "# Feature extractor\n",
    "class MobilenetV2FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = mobilenet_v2(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(base.features.children()))\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d((6, 18))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# Positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=200):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Parallel Decoder\n",
    "\n",
    "class ParallelDecoder(nn.Module):\n",
    "    def __init__(self, d_model=512, num_chars=8, num_classes=68, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.query_embed = nn.Parameter(torch.randn(num_chars, d_model))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, memory):\n",
    "        # memory: [B, 108, 512]\n",
    "        B, _, D = memory.shape\n",
    "\n",
    "        # Repeat query embedding across batch\n",
    "        queries = self.query_embed.unsqueeze(1).repeat(1, B, 1)  # [num_chars, B, d_model]\n",
    "\n",
    "        # Prepare encoder memory: [seq_len, B, d_model]\n",
    "        memory = memory.permute(1, 0, 2)  # [108, B, 512]\n",
    "\n",
    "        # Decode\n",
    "        decoded = self.decoder(tgt=queries, memory=memory)  # [num_chars, B, d_model]\n",
    "\n",
    "        # Predict character class for each position\n",
    "        decoded = decoded.permute(1, 0, 2)  # [B, num_chars, d_model]\n",
    "        logits = self.classifier(decoded)   # [B, num_chars, num_classes]\n",
    "\n",
    "        return logits\n",
    "\n",
    "# Config\n",
    "trans = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = CroppedImages(cropped_folder, trans)\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Instantiate models\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "feature_extractor = MobilenetV2FeatureExtractor().to(device)\n",
    "pos_encoder = PositionalEncoding(d_model=512).to(device)\n",
    "\n",
    "# Transformer\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4).to(device)\n",
    "decoder = ParallelDecoder(d_model=512, num_chars=8, num_classes=68).to(device)\n",
    "\n",
    "# Inference loop\n",
    "for batch in dataloader:\n",
    "    batch = batch.to(device)\n",
    "    features = feature_extractor(batch)  # [B, 512, 6, 18]\n",
    "    B, C, H, W = features.shape\n",
    "    features = features.view(B, C, H * W).permute(0, 2, 1)  # [B, 108, 512]\n",
    "    features = pos_encoder(features)\n",
    "    memory = transformer_encoder(features)  # [B, 108, 512]\n",
    "    logits=decoder(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919601b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose ground truth is shape: [B, 8] (each value is class index from 0 to 67)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(logits.view(-1, 68), targets.view(-1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a2ca22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
