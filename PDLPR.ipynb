{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7490bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9441/3242271671.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_frcnn_model _final_version.pth',map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# IMPORTS AND UTILS\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CTCLoss\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# DEVICE DEFINITION\n",
    "\n",
    "device=\"cpu\"\n",
    "\n",
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]\n",
    "\n",
    "# LOAD FASTER RCNN MODEL\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('best_frcnn_model _final_version.pth',map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "\n",
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "MY_DICTIONARY = provinces + [c for c in alphabet if c not in provinces] + [c for c in ads if c not in provinces and c not in alphabet]    \n",
    "MY_DICTIONARY = list(dict.fromkeys(MY_DICTIONARY))  \n",
    "char2idx = {c: i for i, c in enumerate(MY_DICTIONARY)}\n",
    "idx2char = {i: c for i, c in enumerate(MY_DICTIONARY)}\n",
    "BLANK_IDX = len(MY_DICTIONARY)  \n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    return plate_text\n",
    "\n",
    "# CROP FUNCTION WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_RCNN(file):\n",
    "    image = Image.open(file).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)[0]\n",
    "        if len(prediction['boxes']) == 0:\n",
    "            print(f\"No box found for image: {file}\")\n",
    "            return None\n",
    "        best_bb = prediction['boxes'][0].to(device).int()\n",
    "        cropped = img_tensor[0, :, best_bb[1]:best_bb[3], best_bb[0]:best_bb[2]]\n",
    "        cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "        return cropped_resized.squeeze(0)  \n",
    "\n",
    "def crop_folder_with_RCNN(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text=get_text(full_path)\n",
    "        cropped_image = crop_image_with_RCNN(full_path)\n",
    "        if cropped_image is not None:\n",
    "            cropped_folder.append([cropped_image, gt_text])\n",
    "    return cropped_folder\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image)\n",
    "    cropped = img_tensor[:, int(bb[1]):int(bb[3]), int(bb[0]):int(bb[2])]\n",
    "    cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "    return cropped_resized.squeeze(0)\n",
    "\n",
    "def crop_folder_with_ground_truth(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text = get_text(file)  # Get ground truth text\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append([cropped_image, gt_text])  # Store image and text pair\n",
    "    return cropped_folder\n",
    "\n",
    "def encode_labels(label_list, char2idx, max_len=8):\n",
    "    encoded = []\n",
    "    for label in label_list:\n",
    "        label = label[:max_len].ljust(max_len)\n",
    "        encoded.append([char2idx[c] for c in label])\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat([encode_labels([l], char2idx) for l in labels])\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "def ctc_greedy_decoder(output, idx2char, blank=0):\n",
    "    out = output.permute(1, 0, 2) \n",
    "    pred_strings = []\n",
    "    for probs in out:\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        prev = -1\n",
    "        pred_str = []\n",
    "        for p in pred:\n",
    "            if p != blank and p != prev:\n",
    "                pred_str.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(''.join(pred_str))\n",
    "    return pred_strings\n",
    "\n",
    "trans = T.Compose([\n",
    "    T.Resize((48, 144)),\n",
    "    T.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDLPR MODEL FOLLOWING PAPER ARCHITECTURE\n",
    "\n",
    "# --- Focus Structure Module ---\n",
    "class Focus(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, dropout=0.1):\n",
    "        super(Focus, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Slice and concat\n",
    "        patch1 = x[..., ::2, ::2]\n",
    "        patch2 = x[..., ::2, 1::2]\n",
    "        patch3 = x[..., 1::2, ::2]\n",
    "        patch4 = x[..., 1::2, 1::2]\n",
    "        x = torch.cat([patch1, patch2, patch3, patch4], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- CNN Block used in RESBLOCK and downsampling ---\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Residual Block ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv_block(channels, channels),\n",
    "            conv_block(channels, channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# --- IGFE Module ---\n",
    "class IGFE(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(IGFE, self).__init__()\n",
    "        self.focus = Focus(3, 64, dropout)\n",
    "        self.down1 = conv_block(64, 128, stride=2)\n",
    "        self.res1 = ResBlock(128)\n",
    "        self.res2 = ResBlock(128)\n",
    "        self.down2 = conv_block(128, 256, stride=2)\n",
    "        self.res3 = ResBlock(256)\n",
    "        self.res4 = ResBlock(256)\n",
    "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.focus(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Positional Encoding for 2D feature maps ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.randn(1, d_model, height, width))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "\n",
    "# --- Transformer Encoder Block ---\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(d_model, d_model * 2, kernel_size=1)\n",
    "        self.attn = nn.MultiheadAttention(d_model * 2, num_heads=8, batch_first=True)\n",
    "        self.cnn2 = nn.Conv2d(d_model * 2, d_model, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.cnn1(x)  # [B, 1024, H, W]\n",
    "        x_ = x.flatten(2).transpose(1, 2)  # [B, HW, 1024]\n",
    "        x_, _ = self.attn(x_, x_, x_)\n",
    "        x_ = x_.transpose(1, 2).view(B, -1, H, W)  # [B, 1024, H, W]\n",
    "        x = self.cnn2(x_)\n",
    "        return self.norm(x.flatten(2).transpose(1, 2)).transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "# --- Parallel Decoder Block ---\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Masked self-attention (causal)\n",
    "        B, T, _ = tgt.shape\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(tgt.device)\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=mask)\n",
    "        tgt = self.norm1(tgt + tgt2)\n",
    "\n",
    "        # Cross-attention\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory)\n",
    "        tgt = self.norm2(tgt + tgt2)\n",
    "\n",
    "        # Feedforward\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + tgt2)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# --- Full PDLPR Recognition Model ---\n",
    "class PDLPR(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.1):\n",
    "        super(PDLPR, self).__init__()\n",
    "        self.igfe = IGFE(dropout)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
    "        self.pos_encoding = PositionalEncoding(512, 6, 18)\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(512) for _ in range(3)])\n",
    "        self.flatten = lambda x: x.flatten(2).transpose(1, 2)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(512) for _ in range(3)])\n",
    "        self.cls_head = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_seq):\n",
    "        # Clean up any GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = self.igfe(x)                     \n",
    "        x = self.adaptive_pool(x)            \n",
    "        x = self.dropout(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.encoder(x)\n",
    "        memory = self.flatten(x)             \n",
    "\n",
    "        tgt = tgt_seq                        \n",
    "        for block in self.decoder_blocks:\n",
    "            tgt = block(tgt, memory)\n",
    "            tgt = self.dropout(tgt)\n",
    "\n",
    "        logits = self.cls_head(tgt)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec71f832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9441/3242271671.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_frcnn_model _final_version.pth',map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "class CroppedImages(Dataset):\n",
    "    def __init__(self, folder, transformations):\n",
    "        self.folder = folder\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = self.folder[idx][0]\n",
    "            gt_text = self.folder[idx][1]\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"None image at index {idx}\")\n",
    "                \n",
    "            if self.transformations:\n",
    "                image = self.transformations(image)\n",
    "                \n",
    "            return image, gt_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            return torch.zeros(3, 48, 144), \"\"\n",
    "\n",
    "model=load_Fasterrcnn(device=\"cpu\")\n",
    "device=\"cpu\"\n",
    "model.eval()\n",
    "\n",
    "cropped_folder_train=crop_folder_with_RCNN(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/single_sample_train\")\n",
    "cropped_folder_eval=crop_folder_with_RCNN(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/single_sample_eval\")\n",
    "\n",
    "train_dataset = CroppedImages(cropped_folder_train, trans)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "eval_dataset = CroppedImages(cropped_folder_eval, trans)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False, collate_fn=ctc_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90dfa9fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'皖BD03960'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m train_item=\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# returns a dataloader in which each item is a tuple (element in position 0: cropped image with shape [3, 48, 144], element in position 1: a 1X8 tensor of encoded car text,\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# element in position 2: a tensor with (8))\u001b[39;00m\n\u001b[32m      4\u001b[39m image=train_item[\u001b[32m0\u001b[39m].squeeze()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 158\u001b[39m, in \u001b[36mctc_collate_fn\u001b[39m\u001b[34m(batch)\u001b[39m\n\u001b[32m    156\u001b[39m images = torch.stack(images)\n\u001b[32m    157\u001b[39m label_lengths = torch.tensor([\u001b[38;5;28mlen\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels], dtype=torch.long)\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m labels = torch.cat([\u001b[43mencode_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchar2idx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels])\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m images, labels, label_lengths\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mencode_labels\u001b[39m\u001b[34m(label_list, char2idx, max_len)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode_labels\u001b[39m(label_list, char2idx, max_len=\u001b[32m8\u001b[39m):\n\u001b[32m     17\u001b[39m     encoded = []\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     encoded.append([\u001b[43mchar2idx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m label_list])\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.tensor(encoded, dtype=torch.long)\n",
      "\u001b[31mKeyError\u001b[39m: '皖BD03960'"
     ]
    }
   ],
   "source": [
    "train_item=next(iter(train_dataloader)) \n",
    "# returns a dataloader in which each item is a tuple (element in position 0: cropped image with shape [3, 48, 144], element in position 1: a 1X8 tensor of encoded car text,\n",
    "# element in position 2: a tensor with (8))\n",
    "image=train_item[0].squeeze()\n",
    "text=train_item[1]\n",
    "image, text\n",
    "\n",
    "# showing the image (chii)\n",
    "plt.imshow(image.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Cropped Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cf61ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE PDLPR TRAINING AND EVALUATION WITH CTC LOSS\n",
    "\n",
    "# Device setup \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Model setup\n",
    "model = PDLPR(num_classes=len(MY_DICTIONARY)+1, dropout=0.1).to(device)\n",
    "criterion = CTCLoss(blank=BLANK_IDX, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min', \n",
    "    factor=0.7,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 15\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    true_train_accuracy = 0\n",
    "\n",
    "    for batch_idx, (batch_images, batch_labels, label_lengths) in enumerate(train_dataloader):\n",
    "        try:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            B = batch_images.size(0)\n",
    "            tgt_seq = torch.zeros(B, 8, 512).to(device)\n",
    "            \n",
    "            # Forward pass \n",
    "            logits = model(batch_images, tgt_seq)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "            \n",
    "            input_lengths = torch.full(size=(B,), \n",
    "                                    fill_value=8,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "            \n",
    "            # Calculate CTC loss\n",
    "            loss = criterion(log_probs.permute(1, 0, 2),\n",
    "                           batch_labels,\n",
    "                           input_lengths,\n",
    "                           label_lengths)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy using CTC decoder\n",
    "            pred_text = ctc_greedy_decoder(log_probs.permute(1, 0, 2), idx2char)\n",
    "            for i in range(B):\n",
    "                true_text = ''.join([idx2char[idx.item()] for idx in batch_labels[i][:label_lengths[i]]])\n",
    "                if pred_text[i] == true_text:\n",
    "                    true_train_accuracy += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    true_train_acc = true_train_accuracy / (len(train_dataloader) * B)\n",
    "\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    true_val_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels, label_lengths in eval_dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "\n",
    "            B = batch_images.size(0)\n",
    "            tgt_seq = torch.zeros(B, 8, 512).to(device)\n",
    "\n",
    "            logits = model(batch_images, tgt_seq)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(B,), \n",
    "                                    fill_value=8,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "\n",
    "            loss = criterion(log_probs.permute(1, 0, 2),\n",
    "                           batch_labels,\n",
    "                           input_lengths,\n",
    "                           label_lengths)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy using CTC decoder\n",
    "            pred_text = ctc_greedy_decoder(log_probs.permute(1, 0, 2), idx2char)\n",
    "            for i in range(B):\n",
    "                true_text = ''.join([idx2char[idx.item()] for idx in batch_labels[i][:label_lengths[i]]])\n",
    "                if pred_text[i] == true_text:\n",
    "                    true_val_accuracy += 1\n",
    "\n",
    "    avg_val_loss = val_loss / len(eval_dataloader)\n",
    "    true_val_acc = true_val_accuracy / (len(eval_dataloader) * B)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, 'best_pdlpr_model_ctc.pth')\n",
    "        print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | True Train Accuracy: {true_train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | True Val Accuracy: {true_val_acc:.4f}\")\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "70925e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9813/160895573.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_pdlpr_model_ctc.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted license plate: ['0', '0', '0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('best_pdlpr_model_ctc.pth', map_location=device)\n",
    "model = PDLPR(num_classes=len(MY_DICTIONARY) + 1, dropout=0.1)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "cropped_image=crop_image_with_ground_truth(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/single_sample_eval//03625-92_266-225&396_573&501-568&501_227&490_225&396_573&414-0_0_3_17_25_28_27_30-102-83.jpg\")\n",
    "transformed = trans(cropped_image).unsqueeze(0).to(device)  \n",
    "\n",
    "tgt_seq = torch.zeros(1, 8, 512).to(device)  # batch size = 1, seq len = 8\n",
    "\n",
    "# ---- Inference ----\n",
    "with torch.no_grad():\n",
    "    logits = model(transformed, tgt_seq)         # [B, T, num_classes]\n",
    "    log_probs = logits.log_softmax(2)\n",
    "pred_text = ctc_greedy_decoder(log_probs, idx2char, blank=0)\n",
    "print(f\"Predicted license plate: {pred_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
