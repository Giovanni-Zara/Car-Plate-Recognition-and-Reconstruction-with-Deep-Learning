{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e630b69",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0b0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5bf2a",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f97a0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS NEEDED FOR ENCODING AND DECODING \n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "seen = set()    #avoid duplicates\n",
    "MY_DICTIONARY = []\n",
    "for char_list in [provinces, alphabets, ads]:\n",
    "    for char in char_list:\n",
    "        if char not in seen:\n",
    "            MY_DICTIONARY.append(char)\n",
    "            seen.add(char)\n",
    "char2idx = {c: i for i, c in enumerate(MY_DICTIONARY)}\n",
    "idx2char = {i: c for i, c in enumerate(MY_DICTIONARY)}\n",
    "BLANK_IDX = len(MY_DICTIONARY)  # CTC needs +1 for \"blank\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da812e",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208c4eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6571/1960938398.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION TO LOAD THE DETECTION MODEL\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "device=\"cpu\"\n",
    "model=load_Fasterrcnn(device)\n",
    "\n",
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "def get_plate(filename):\n",
    "    fields = filename.split('-')\n",
    "\n",
    "    text=str(fields[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabets[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a07a89",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bbd19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET \n",
    "\n",
    "test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/single_sample_test\"\n",
    "# test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "# COLLATE FUNCTION\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    '''\n",
    "    basically what I do here is stacking all the images in a batch into a single tensor and\n",
    "    then computing the len of each label (assuming different lenght plate can happen). (I could actually avoid this but it's more general)\n",
    "    Finally just concatenating all the labels into a vector (pytorch CTC wantres them in a line, not list)\n",
    "    then returning image-label-its lenght.\n",
    "    I need this to tell CTC where labels finish and i do not care padding as CTC deals with that internally (NICE)\n",
    "    '''\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "# DECODER FUNCTION\n",
    "\n",
    "def ctc_greedy_decoder(output, idx2char, blank=BLANK_IDX):\n",
    "    '''\n",
    "    Now, I know the network returns probabilities, as it does a softmax with logits of characters.\n",
    "    I need to transform that probability into an actual char to compose the plate.\n",
    "    I take the argmax of the softmax (most prob char), remove blanks used by CTC and possible\n",
    "    duplicates CTC can actually produce.\n",
    "    At the end I simply use the  mappings char-index index-char deified at the beginning to compose the plate.\n",
    "    This is greedy as it just takes the argmax of every step, I think it's more than enough here.\n",
    "    '''\n",
    "    # output: [seq_len, batch, num_classes]\n",
    "    out = output.permute(1, 0, 2)  # [batch, seq_len, num_classes]\n",
    "    pred_strings = []\n",
    "    for probs in out:\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        prev = -1\n",
    "        pred_str = []\n",
    "        for p in pred:\n",
    "            if p != blank and p != prev:\n",
    "                pred_str.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(''.join(pred_str))\n",
    "    return pred_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30601e1",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c869eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDLPR MODEL FOLLOWING PAPER ARCHITECTURE\n",
    "\n",
    "# --- Focus Structure Module ---\n",
    "class Focus(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=64, dropout=0.1):  # Fixed __init__\n",
    "        super(Focus, self).__init__()  # Fixed __init__\n",
    "        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Slice and concat - Focus structure downsampling\n",
    "        patch1 = x[..., ::2, ::2]\n",
    "        patch2 = x[..., ::2, 1::2]\n",
    "        patch3 = x[..., 1::2, ::2]\n",
    "        patch4 = x[..., 1::2, 1::2]\n",
    "        x = torch.cat([patch1, patch2, patch3, patch4], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- CNN Block used in RESBLOCK and downsampling ---\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Residual Block ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):  # Fixed __init__\n",
    "        super(ResBlock, self).__init__()  # Fixed __init__\n",
    "        self.block = nn.Sequential(\n",
    "            conv_block(channels, channels),\n",
    "            conv_block(channels, channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# --- IGFE Module ---\n",
    "class IGFE(nn.Module):\n",
    "    def __init__(self, dropout=0.1):  # Fixed __init__\n",
    "        super(IGFE, self).__init__()  # Fixed __init__\n",
    "        self.focus = Focus(1, 64, dropout)  # Changed to 1 channel for grayscale\n",
    "        self.down1 = conv_block(64, 128, stride=2)\n",
    "        self.res1 = ResBlock(128)\n",
    "        self.res2 = ResBlock(128)\n",
    "        self.down2 = conv_block(128, 256, stride=2)\n",
    "        self.res3 = ResBlock(256)\n",
    "        self.res4 = ResBlock(256)\n",
    "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.focus(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Positional Encoding for 2D feature maps ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, height, width):  # Fixed __init__\n",
    "        super().__init__()  # Fixed __init__\n",
    "        self.pe = nn.Parameter(torch.randn(1, d_model, height, width))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "\n",
    "# --- Transformer Encoder Block ---\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):  # Fixed __init__\n",
    "        super(EncoderBlock, self).__init__()  # Fixed __init__\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # [B, HW, C]\n",
    "\n",
    "        # Self-attention\n",
    "        attn_out, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "\n",
    "        # FFN\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x.transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "\n",
    "# --- PDLPR Recognition Model (CTC-based) ---\n",
    "class PDLPR(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.1):  # Fixed __init__\n",
    "        super(PDLPR, self).__init__()  # Fixed __init__\n",
    "        self.igfe = IGFE(dropout)\n",
    "        self.pos_encoding = PositionalEncoding(512, 6, 18)\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(512) for _ in range(3)])\n",
    "\n",
    "        # CTC head - outputs sequence of characters\n",
    "        # After flattening, we have [B, W, C*H] = [B, 18, 512*6] = [B, 18, 3072]\n",
    "        self.ctc_head = nn.Sequential(\n",
    "            nn.Linear(512 * 6, 256),  # Fixed dimension: 512*6 = 3072\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.igfe(x)  # [B, 512, 6, 18]\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.encoder(x)  # [B, 512, 6, 18]\n",
    "\n",
    "        # Flatten to sequence for CTC\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  # [B, W, C, H]\n",
    "        x = x.contiguous().view(B, W, -1)  # [B, W, C*H] = [B, 18, 3072]\n",
    "\n",
    "        # Linear projection to reduce dimension\n",
    "        x = self.ctc_head(x)  # [B, W, num_classes]\n",
    "        x = x.permute(1, 0, 2)  # [W, B, num_classes] for CTC\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29cf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "\n",
    "class LicensePlateRecognitionPipeline:\n",
    "    def __init__(self, pdlpr_model_path, device='cpu'):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # Load Faster R-CNN model\n",
    "        self.detection_model = load_Fasterrcnn(self.device)\n",
    "\n",
    "        # Load PDLPR model\n",
    "        self.pdlpr_model = PDLPR(num_classes=BLANK_IDX + 1, dropout=0.1).to(self.device)\n",
    "        checkpoint = torch.load(pdlpr_model_path, map_location=self.device)\n",
    "        self.pdlpr_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.pdlpr_model.eval()\n",
    "\n",
    "        # Transform for PDLPR\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((64, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "        print(\"Pipeline initialized successfully!\")\n",
    "\n",
    "    def frcnn_crop(self, image_path):\n",
    "        \"\"\"\n",
    "        Detecting plates using Faster R-CNN\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image\n",
    "\n",
    "        Returns:\n",
    "            cropped image\n",
    "        \"\"\"\n",
    "        # Load and transform image\n",
    "        img = Image.open(image_path)\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        img_tensor = transform(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Get predictions\n",
    "        with torch.no_grad():\n",
    "            predictions = self.detection_model(img_tensor)\n",
    "\n",
    "        # Get the first (best) detection if any\n",
    "        if len(predictions[0]['boxes']) > 0:\n",
    "            box = predictions[0]['boxes'][0].cpu().numpy()\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cropped = img.crop((x1, y1, x2, y2))\n",
    "            return cropped\n",
    "        else:\n",
    "            print(\"No license plate detected!\")\n",
    "            return None\n",
    "\n",
    "    def recognize_plate(self, plate_crop):\n",
    "        \"\"\"\n",
    "        Recognizes text on a license plate using PDLPR\n",
    "\n",
    "        Args:\n",
    "            plate_crop: Cropped license plate (PIL Image)\n",
    "\n",
    "        Returns:\n",
    "            Recognized text\n",
    "        \"\"\"\n",
    "        if plate_crop is None:\n",
    "            return \"\"\n",
    "\n",
    "        plate_crop_tr = self.transform(plate_crop).unsqueeze(0)\n",
    "        plate_crop_tr = plate_crop_tr.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pdlpr_model(plate_crop_tr)\n",
    "            pred_string = ctc_greedy_decoder(outputs, idx2char)\n",
    "\n",
    "        return pred_string[0] if pred_string else \"\"\n",
    "\n",
    "    def __call__(self, img_path):\n",
    "        \"\"\"\n",
    "        Makes the object callable - main pipeline method\n",
    "\n",
    "        Args:\n",
    "            img_path: Path to the image\n",
    "\n",
    "        Returns:\n",
    "            Recognized license plate text\n",
    "        \"\"\"\n",
    "        crop = self.frcnn_crop(img_path)\n",
    "        plate = self.recognize_plate(crop)\n",
    "        return plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5bdb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6571/1960938398.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6571/1902644980.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pdlpr_model_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "pdlpr_model_path = '/home/filippo/Documents/Visual Studio Code/best_pdlpr_model.pth'  \n",
    "online_model = LicensePlateRecognitionPipeline(pdlpr_model_path=pdlpr_model_path, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885419f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1d214aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE IS NO NEED TO TRAIN ANYTHING HERE SINCE WE JUST HAVE TO MAKE INFERENCE WITH THE DETECTION AND RECOGNITION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2978fa9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c76474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on test set...\n",
      "\n",
      "Image: 03125-89_263-177&502_477&597-464&581_177&597_185&513_477&502-0_0_3_27_29_25_33_33-102-60.jpg\n",
      "Ground Truth: 皖AD35199\n",
      "Prediction: 皖AD35199\n",
      "Correct: True\n",
      "\n",
      "\n",
      "Final Results:\n",
      "Total images processed: 1\n",
      "Correct predictions: 1\n",
      "Accuracy: 100.0000%\n",
      "Length error rate: 0.0000 | Province acc: 1.0000 | Alphabet acc: 1.0000 | Character acc: 1.0000\n",
      "Image: 03125-89_263-177&502_477&597-464&581_177&597_185&513_477&502-0_0_3_27_29_25_33_33-102-60.jpg\n",
      "Ground Truth: 皖AD35199\n",
      "Prediction: 皖AD35199\n",
      "Correct: True\n",
      "\n",
      "\n",
      "Final Results:\n",
      "Total images processed: 1\n",
      "Correct predictions: 1\n",
      "Accuracy: 100.0000%\n",
      "Length error rate: 0.0000 | Province acc: 1.0000 | Alphabet acc: 1.0000 | Character acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_test_set(model, test_dir):\n",
    "    results = []\n",
    "    total_images = 0\n",
    "    correct_predictions = 0\n",
    "    processed_images = set()\n",
    "    \n",
    "    # Additional counters for new metrics\n",
    "    total_length_diff = 0\n",
    "    correct_province = 0\n",
    "    correct_alphabet = 0\n",
    "    total_characters = 0\n",
    "    correct_characters = 0\n",
    "    \n",
    "    # Get all image files\n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = sorted(f for f in os.listdir(test_dir) if f.lower().endswith(valid_extensions))\n",
    "    \n",
    "    print(\"Starting evaluation on test set...\\n\")\n",
    "    \n",
    "    for img_file in image_files:\n",
    "        if img_file in processed_images:\n",
    "            continue\n",
    "            \n",
    "        processed_images.add(img_file)\n",
    "        img_path = os.path.join(test_dir, img_file)\n",
    "        try:\n",
    "            # Get ground truth\n",
    "            ground_truth = get_plate(img_file)\n",
    "            \n",
    "            # Get prediction - handle case where detection fails\n",
    "            try:\n",
    "                predicted_plate = model(img_path)\n",
    "                if predicted_plate is None:\n",
    "                    predicted_plate = \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Detection failed for {img_file}, skipping...\")\n",
    "                predicted_plate = \"\"\n",
    "            \n",
    "            # Compare prediction with ground truth \n",
    "            is_correct = predicted_plate.strip() == ground_truth.strip()\n",
    "            \n",
    "            # Calculate length difference for all images (including failed detections)\n",
    "            total_length_diff += abs(len(predicted_plate) - len(ground_truth))\n",
    "            \n",
    "            # Calculate character-level accuracy\n",
    "            total_characters += len(ground_truth)\n",
    "            min_length = min(len(predicted_plate), len(ground_truth))\n",
    "            for i in range(min_length):\n",
    "                if predicted_plate[i] == ground_truth[i]:\n",
    "                    correct_characters += 1\n",
    "            \n",
    "            # Check province and alphabet accuracy for all images\n",
    "            if len(ground_truth) > 0:\n",
    "                if len(predicted_plate) > 0 and predicted_plate[0] == ground_truth[0]:\n",
    "                    correct_province += 1\n",
    "                if len(ground_truth) > 1:\n",
    "                    if len(predicted_plate) > 1 and predicted_plate[1] == ground_truth[1]:\n",
    "                        correct_alphabet += 1\n",
    "            \n",
    "            results.append({\n",
    "                'image': img_file,\n",
    "                'ground_truth': ground_truth,\n",
    "                'prediction': predicted_plate,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "            \n",
    "            total_images += 1\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"Image: {img_file}\")\n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "            print(f\"Prediction: {predicted_plate}\")\n",
    "            print(f\"Correct: {is_correct}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\\n\")\n",
    "    \n",
    "    # Calculate final metrics\n",
    "    accuracy = (correct_predictions / total_images) * 100 if total_images > 0 else 0\n",
    "    length_error_rate = total_length_diff / total_images if total_images > 0 else 0\n",
    "    province_accuracy = correct_province / total_images if total_images > 0 else 0\n",
    "    alphabet_accuracy = correct_alphabet / total_images if total_images > 0 else 0\n",
    "    character_accuracy = correct_characters / total_characters if total_characters > 0 else 0\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}%\")\n",
    "    print(f\"Length error rate: {length_error_rate:.4f} | Province acc: {province_accuracy:.4f} | Alphabet acc: {alphabet_accuracy:.4f} | Character acc: {character_accuracy:.4f}\")\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "# Run evaluation and capture results\n",
    "results, final_accuracy = evaluate_on_test_set(online_model, test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
