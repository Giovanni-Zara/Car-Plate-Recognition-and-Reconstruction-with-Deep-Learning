{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e630b69",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0b0437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e5bf2a",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a0ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS NEEDED FOR ENCODING AND DECODING \n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "seen = set()    # avoid duplicates\n",
    "MY_DICTIONARY = []\n",
    "for char_list in [provinces, alphabets, ads]:\n",
    "    for char in char_list:\n",
    "        if char not in seen:\n",
    "            MY_DICTIONARY.append(char)\n",
    "            seen.add(char)\n",
    "char2idx = {c: i for i, c in enumerate(MY_DICTIONARY)} # from characters to indices encoding\n",
    "idx2char = {i: c for i, c in enumerate(MY_DICTIONARY)} # from indices to characters encoding\n",
    "BLANK_IDX = len(MY_DICTIONARY) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da812e",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "208c4eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6571/1960938398.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION TO LOAD THE DETECTION MODEL\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "device=\"cpu\"\n",
    "model=load_Fasterrcnn(device)\n",
    "\n",
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "def get_plate(filename):\n",
    "    fields = filename.split('-')\n",
    "\n",
    "    text=str(fields[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabets[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a07a89",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET \n",
    "\n",
    "test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "# COLLATE FUNCTION\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Basically what I do here is stacking all the images in a batch into a single tensor and\n",
    "    then computing the len of each label (assuming different lenght plate can happen). (I could actually avoid this but it's more general)\n",
    "    Finally just concatenating all the labels into a vector (pytorch CTC wantres them in a line, not list) then returning image-label-its lenght.\n",
    "    I need this to tell CTC where labels finish and i do not care about padding as CTC deals with that internally \n",
    "\n",
    "    '''\n",
    "\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "# DECODER FUNCTION\n",
    "\n",
    "def ctc_greedy_decoder(output, idx2char, blank=BLANK_IDX):\n",
    "\n",
    "    '''\n",
    "\n",
    "    Now, I know the network returns probabilities, as it does a softmax with logits of characters.\n",
    "    I need to transform that probability into an actual char to compose the plate.\n",
    "    I take the argmax of the softmax (most prob char), remove blanks used by CTC and possible\n",
    "    duplicates CTC can actually produce.\n",
    "    At the end I simply use the  mappings char-index index-char deified at the beginning to compose the plate.\n",
    "    This is greedy as it just takes the argmax of every step --> should be enough\n",
    "\n",
    "    '''\n",
    "\n",
    "    out = output.permute(1, 0, 2)  # [batch, seq_len, num_classes]\n",
    "    pred_strings = []\n",
    "    for probs in out:\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        prev = -1\n",
    "        pred_str = []\n",
    "        for p in pred:\n",
    "            if p != blank and p != prev:\n",
    "                pred_str.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(''.join(pred_str))\n",
    "    return pred_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30601e1",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c869eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDLPR MODEL FOLLOWING PAPER ARCHITECTURE\n",
    "\n",
    "# FOCUS MODULE\n",
    "\n",
    "class Focus(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=64, dropout=0.1):  \n",
    "        super(Focus, self).__init__()  \n",
    "        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    # weights initialization\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        patch1 = x[..., ::2, ::2]\n",
    "        patch2 = x[..., ::2, 1::2]\n",
    "        patch3 = x[..., 1::2, ::2]\n",
    "        patch4 = x[..., 1::2, 1::2]\n",
    "        x = torch.cat([patch1, patch2, patch3, patch4], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# CNN BLOCK USED IN RESBLOCKS AND IN THE DOWNSAMPLING PART\n",
    "\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    \n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# RESBLOCK\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):  # Fixed __init__\n",
    "        super(ResBlock, self).__init__()  # Fixed __init__\n",
    "        self.block = nn.Sequential(\n",
    "            conv_block(channels, channels),\n",
    "            conv_block(channels, channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# FULL IMPROVED GLOBAL FEATURE EXTRACTOR MODULE\n",
    "\n",
    "class IGFE(nn.Module):\n",
    "    def __init__(self, dropout=0.1):  # Fixed __init__\n",
    "        super(IGFE, self).__init__()  # Fixed __init__\n",
    "        self.focus = Focus(1, 64, dropout)  # Changed to 1 channel for grayscale\n",
    "        self.down1 = conv_block(64, 128, stride=2)\n",
    "        self.res1 = ResBlock(128)\n",
    "        self.res2 = ResBlock(128)\n",
    "        self.down2 = conv_block(128, 256, stride=2)\n",
    "        self.res3 = ResBlock(256)\n",
    "        self.res4 = ResBlock(256)\n",
    "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.focus(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# POSITIONAL ENCODING\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, height, width): \n",
    "        super().__init__()  \n",
    "        self.pe = nn.Parameter(torch.randn(1, d_model, height, width))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "\n",
    "# ENCODER BLOCK\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):  \n",
    "        super(EncoderBlock, self).__init__() \n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  \n",
    "        attn_out, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "\n",
    "        return x.transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "\n",
    "# FULL PDLPR MODEL\n",
    "\n",
    "class PDLPR(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.1):  \n",
    "        super(PDLPR, self).__init__()  \n",
    "        self.igfe = IGFE(dropout)\n",
    "        self.pos_encoding = PositionalEncoding(512, 6, 18)\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(512) for _ in range(3)])\n",
    "        self.ctc_head = nn.Sequential(\n",
    "            nn.Linear(512 * 6, 256),  \n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.igfe(x) \n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.encoder(x)  \n",
    "        B, C, H, W = x.shape\n",
    "        x = x.permute(0, 3, 1, 2)  \n",
    "        x = x.contiguous().view(B, W, -1)  \n",
    "        x = self.ctc_head(x) \n",
    "        x = x.permute(1, 0, 2)  \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE\n",
    "\n",
    "class LicensePlateRecognitionPipeline:\n",
    "    def __init__(self, pdlpr_model_path, device='cpu'):\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.detection_model = load_Fasterrcnn(self.device)\n",
    "\n",
    "        self.pdlpr_model = PDLPR(num_classes=BLANK_IDX + 1, dropout=0.1).to(self.device)\n",
    "        checkpoint = torch.load(pdlpr_model_path, map_location=self.device)\n",
    "        self.pdlpr_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.pdlpr_model.eval()\n",
    "\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((64, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "        print(\"Pipeline initialized successfully!\")\n",
    "\n",
    "    def frcnn_crop(self, image_path):\n",
    "\n",
    "        img = Image.open(image_path)\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        img_tensor = transform(img).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions = self.detection_model(img_tensor)\n",
    "\n",
    "        if len(predictions[0]['boxes']) > 0:\n",
    "            box = predictions[0]['boxes'][0].cpu().numpy()\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            cropped = img.crop((x1, y1, x2, y2))\n",
    "            return cropped\n",
    "        else:\n",
    "            print(\"No license plate detected!\")\n",
    "            return None\n",
    "\n",
    "    def recognize_plate(self, plate_crop):\n",
    "        \n",
    "        if plate_crop is None:\n",
    "            return \"\"\n",
    "\n",
    "        plate_crop_tr = self.transform(plate_crop).unsqueeze(0)\n",
    "        plate_crop_tr = plate_crop_tr.to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.pdlpr_model(plate_crop_tr)\n",
    "            pred_string = ctc_greedy_decoder(outputs, idx2char)\n",
    "\n",
    "        return pred_string[0] if pred_string else \"\"\n",
    "\n",
    "    def __call__(self, img_path):\n",
    "        crop = self.frcnn_crop(img_path)\n",
    "        plate = self.recognize_plate(crop)\n",
    "        \n",
    "        return plate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f5bdb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6571/1960938398.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline initialized successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6571/1902644980.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pdlpr_model_path, map_location=self.device)\n"
     ]
    }
   ],
   "source": [
    "pdlpr_model_path = '/home/filippo/Documents/Visual Studio Code/best_pdlpr_model.pth'  \n",
    "online_model = LicensePlateRecognitionPipeline(pdlpr_model_path=pdlpr_model_path, device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885419f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d214aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE IS NO NEED TO TRAIN ANYTHING HERE SINCE WE JUST HAVE TO MAKE INFERENCE WITH THE DETECTION AND PDLPR MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2978fa9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32c76474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation on test set...\n",
      "\n",
      "Image: 03125-89_263-177&502_477&597-464&581_177&597_185&513_477&502-0_0_3_27_29_25_33_33-102-60.jpg\n",
      "Ground Truth: 皖AD35199\n",
      "Prediction: 皖AD35199\n",
      "Correct: True\n",
      "\n",
      "\n",
      "Final Results:\n",
      "Total images processed: 1\n",
      "Correct predictions: 1\n",
      "Accuracy: 100.0000%\n",
      "Length error rate: 0.0000 | Province acc: 1.0000 | Alphabet acc: 1.0000 | Character acc: 1.0000\n",
      "Image: 03125-89_263-177&502_477&597-464&581_177&597_185&513_477&502-0_0_3_27_29_25_33_33-102-60.jpg\n",
      "Ground Truth: 皖AD35199\n",
      "Prediction: 皖AD35199\n",
      "Correct: True\n",
      "\n",
      "\n",
      "Final Results:\n",
      "Total images processed: 1\n",
      "Correct predictions: 1\n",
      "Accuracy: 100.0000%\n",
      "Length error rate: 0.0000 | Province acc: 1.0000 | Alphabet acc: 1.0000 | Character acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_on_test_set(model, test_dir):\n",
    "    results = []\n",
    "    total_images = 0\n",
    "    correct_predictions = 0\n",
    "    processed_images = set()\n",
    "    \n",
    "    total_length_diff = 0\n",
    "    correct_province = 0\n",
    "    correct_alphabet = 0\n",
    "    total_characters = 0\n",
    "    correct_characters = 0\n",
    "    \n",
    "    valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "    image_files = sorted(f for f in os.listdir(test_dir) if f.lower().endswith(valid_extensions))\n",
    "        \n",
    "    for img_file in image_files:\n",
    "        if img_file in processed_images:\n",
    "            continue\n",
    "            \n",
    "        processed_images.add(img_file)\n",
    "        img_path = os.path.join(test_dir, img_file)\n",
    "        try:\n",
    "            ground_truth = get_plate(img_file)\n",
    "            \n",
    "            try:\n",
    "                predicted_plate = model(img_path)\n",
    "                if predicted_plate is None:\n",
    "                    predicted_plate = \"\"\n",
    "            except Exception as e:\n",
    "                print(f\"Detection failed for {img_file}\")\n",
    "                predicted_plate = \"\"\n",
    "            \n",
    "            is_correct = predicted_plate.strip() == ground_truth.strip()\n",
    "            \n",
    "            total_length_diff += abs(len(predicted_plate) - len(ground_truth))\n",
    "            \n",
    "            total_characters += len(ground_truth)\n",
    "            min_length = min(len(predicted_plate), len(ground_truth))\n",
    "            for i in range(min_length):\n",
    "                if predicted_plate[i] == ground_truth[i]:\n",
    "                    correct_characters += 1\n",
    "            \n",
    "            if len(ground_truth) > 0:\n",
    "                if len(predicted_plate) > 0 and predicted_plate[0] == ground_truth[0]:\n",
    "                    correct_province += 1\n",
    "                if len(ground_truth) > 1:\n",
    "                    if len(predicted_plate) > 1 and predicted_plate[1] == ground_truth[1]:\n",
    "                        correct_alphabet += 1\n",
    "            \n",
    "            results.append({\n",
    "                'image': img_file,\n",
    "                'ground_truth': ground_truth,\n",
    "                'prediction': predicted_plate,\n",
    "                'correct': is_correct\n",
    "            })\n",
    "            \n",
    "            total_images += 1\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            \n",
    "            print(f\"Image: {img_file}\")\n",
    "            print(f\"Ground Truth: {ground_truth}\")\n",
    "            print(f\"Prediction: {predicted_plate}\")\n",
    "            print(f\"Correct: {is_correct}\\n\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_file}: {str(e)}\\n\")\n",
    "    \n",
    "    accuracy = (correct_predictions / total_images) * 100 if total_images > 0 else 0\n",
    "    length_error_rate = total_length_diff / total_images if total_images > 0 else 0\n",
    "    province_accuracy = correct_province / total_images if total_images > 0 else 0\n",
    "    alphabet_accuracy = correct_alphabet / total_images if total_images > 0 else 0\n",
    "    character_accuracy = correct_characters / total_characters if total_characters > 0 else 0\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    print(f\"Total images processed: {total_images}\")\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}%\")\n",
    "    print(f\"Length error rate: {length_error_rate:.4f} | Province acc: {province_accuracy:.4f} | Alphabet acc: {alphabet_accuracy:.4f} | Character acc: {character_accuracy:.4f}\")\n",
    "    \n",
    "    return results, accuracy\n",
    "\n",
    "# Run evaluation and capture results\n",
    "results, final_accuracy = evaluate_on_test_set(online_model, test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
