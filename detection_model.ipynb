{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f290448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11695bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATASET STRUCTURE\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "train_folder=os.listdir(train_path)\n",
    "eval_folder=os.listdir(eval_path)\n",
    "test_folder=os.listdir(test_path)\n",
    "\n",
    "print(f\"Nmumber of training samples: {len(train_folder)}\")\n",
    "print(f\"Nmumber of evaluation samples: {len(eval_folder)}\")\n",
    "print(f\"Nmumber of testing samples: {len(test_folder)}\")\n",
    "print(f\"Total number of samples: {len(train_folder)+len(test_folder)+len(eval_folder)}\")\n",
    "\n",
    "# CHECKING ERRORS FOR FUNCTION: get_bounding_box\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "\n",
    "    # if len(values) != 31:                                                                   questo controllo non serve in quanto dipende da quante cifre ogni numero ha\n",
    "    #     raise ValueError(f\" 1 st exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v2=values.split(\"&\")\n",
    "\n",
    "    if len(values_v2) != 5:\n",
    "        raise ValueError(f\"2 nd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "\n",
    "    if len(values_v3) != 3:\n",
    "        raise ValueError(f\"3 rd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "\n",
    "    if len(x_coords) != 4:\n",
    "        raise ValueError(f\"4 th exception: Invalid plate text format in filename: {file}\")\n",
    "\n",
    " # CHECKING ERRORS FOR FUNCTION: get_text\n",
    "    \n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "        values=file.split(\"-\")\n",
    "\n",
    "        if len(values)!=7:\n",
    "            raise ValueError(f\"1 st exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        text=str(values[4])\n",
    "\n",
    "        # if len(text)!=7:                                                                                questo controllo non serve in quanto dipende da quante cifre ogni numero ha cifre diverse\n",
    "        #     raise ValueError(f\"2 nd exception occurred: Invalid plate text format in filename: {file}\") \n",
    "        \n",
    "        indices=text.split(\"_\")\n",
    "\n",
    "        if len(indices) != 8:\n",
    "            raise ValueError(f\"3 rd exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        province_character=provinces[int(indices[0])]\n",
    "        alphabet_character=alphabet[int(indices[1])]\n",
    "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "        if len(plate_text) != 8:\n",
    "            raise ValueError(f\"4 th exception occurred: Invalid plate text format in filename: {file}\")\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_bounding_box\n",
    "\n",
    "file=\"308069444444444444-91_94-8&424_589&564-578&564_27&553_8&431_589&424-0_0_3_24_32_25_32_25-100-241.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_text\n",
    "\n",
    "file=\"30237890625-87_88-197&472_449&567-449&553_201&567_197&472_448&477-10_2_3_24_31_33_26_24-192-75.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file2.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d76f51ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of original training samples: 5291\n",
      "number of original test samples: 3329\n",
      "number of new training samples: 6635\n",
      "number of new training samples: 1985\n"
     ]
    }
   ],
   "source": [
    "# NUOVO DATASET CON PIU ELEMENTI DI TRAIN E MENO DI TEST\n",
    "\n",
    "path_train_og=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "path_test_og=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "path_train_tf=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/train\"\n",
    "path_test_tf=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\"\n",
    "folder_train_og=os.listdir(path_train_og)\n",
    "folder_test_og=os.listdir(path_test_og)\n",
    "folder_train_tf=os.listdir(path_train_tf)\n",
    "folder_test_tf=os.listdir(path_test_tf)\n",
    "print(f\"number of original training samples: {len(folder_train_og)}\")\n",
    "print(f\"number of original test samples: {len(folder_test_og)}\")\n",
    "print(f\"number of new training samples: {len(folder_train_tf)}\")\n",
    "print(f\"number of new training samples: {len(folder_test_tf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccab6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9344421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERSECTION OVER UNION FUNCTION\n",
    "\n",
    "def compute_IoU(box1, box2):\n",
    "\n",
    "    box1=box1.squeeze()\n",
    "    box2=box2.squeeze()\n",
    "\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    area_of_intersection = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3]- box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    IoU = area_of_intersection / float(area_box1 + area_box2 - area_of_intersection)\n",
    "\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET   \n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "smaller_train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train-20250508T142228Z-1-001/train\"\n",
    "folder=os.listdir(smaller_train_path)\n",
    "\n",
    "class CCPD_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, transforms=None):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.folder = os.listdir(path)\n",
    "        self.images = [f for f in self.folder if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        full_path = os.path.join(self.path, file)\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "\n",
    "        bbox = get_bounding_box(file)\n",
    "        box = torch.tensor([bbox], dtype=torch.float32)\n",
    "        label = torch.tensor([1], dtype=torch.int64)  \n",
    "\n",
    "        target = {\"boxes\": box, \"labels\": label}\n",
    "\n",
    "        if target[\"boxes\"].shape[0] != target[\"labels\"].shape[0]:\n",
    "            raise ValueError(f\"Mismatch in number of boxes and labels for file: {file}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train2\"\n",
    "eval_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval2\"\n",
    "test_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\"\n",
    "    \n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = CCPD_Dataset(train_path2, transforms=transform)\n",
    "eval_dataset = CCPD_Dataset(eval_path2, transforms=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "# model.to(device)\n",
    "device=\"cpu\" \n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, targets in train_dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "    print(f\"Epoch {epoch} Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    with torch.no_grad():\n",
    "        total_iou = 0\n",
    "        count = 0\n",
    "        for images, targets in eval_dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes'].to(device)\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    iou = compute_IoU(pred_boxes[0], gt_boxes[0])\n",
    "                    total_iou += iou\n",
    "                    count += 1\n",
    "\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "    print(f\"Average IoU on eval set: {avg_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cb7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "model=fasterrcnn_resnet50_fpn()\n",
    "model.load_state_dict(torch.load(\"model_weights.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF THE MODEL\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "img_path = \"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train/04-90_267-158&448_542&553-541&553_162&551_158&448_542&450-0_1_3_24_27_33_30_24-99-116.jpg\"  # Replace with a valid image path\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "transform = T.ToTensor()\n",
    "img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction = model(img_tensor) # PREDICTION IS A LIST OF DICTIONARIES; EACH DICTIONARY CONTAINS: \"BOXES\": LIST CONTAINING THE COORDINATES OF THE BOUNDING \n",
    "    # BOXES COMPUTED BY THE MODEL AND \"SCORES\":  LIST CONTAINING THE CONFIDENCE VALUES OF ALL THE BOUNDING BOXES FOUND BY THE MODEL\n",
    "\n",
    "boxes = prediction[0]['boxes'].cpu()\n",
    "scores = prediction[0]['scores'].cpu()\n",
    "threshold = 0.1\n",
    "keep = scores >= threshold\n",
    "boxes = boxes[keep]\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)\n",
    "ax = plt.gca()\n",
    "for box in boxes:\n",
    "    x1, y1, x2, y2 = box\n",
    "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                         fill=False, edgecolor='red', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "plt.title(\"Predicted Car Plate Bounding Box\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3c4196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP FUNCTION WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_RCNN(file):\n",
    "    \n",
    "    image = Image.open(file).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "\n",
    "    boxes = prediction[0]['boxes']\n",
    "    scores = prediction[0]['scores']\n",
    "    max_confidence=0\n",
    "    idx=-1\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        if scores[i].item()>=max_confidence:\n",
    "            max_confidence=scores[i].item()\n",
    "            idx=i\n",
    "\n",
    "    best_bb=boxes[idx]\n",
    "    best_bb=best_bb.int()\n",
    "    cropped_image = img_tensor[0, :, best_bb[1]:best_bb[3], best_bb[0]:best_bb[2]]\n",
    "\n",
    "    return cropped_image\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  # extract just the filename for parsing\n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    cropped_image = image.crop(bb)\n",
    "    return cropped_image\n",
    "\n",
    "def crop_folder(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append(cropped_image)\n",
    "    return cropped_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7215ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROPPED IMAGE EXAMPLE\n",
    "\n",
    "path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train-20250508T142228Z-1-001/train\"\n",
    "cropped_folder=crop_folder(path)\n",
    "cropped_folder[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
