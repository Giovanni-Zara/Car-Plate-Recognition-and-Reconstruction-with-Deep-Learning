{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11695bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATASET STRUCTURE\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "train_folder=os.listdir(train_path)\n",
    "eval_folder=os.listdir(eval_path)\n",
    "test_folder=os.listdir(test_path)\n",
    "\n",
    "print(f\"Nmumber of training samples: {len(train_folder)}\")\n",
    "print(f\"Nmumber of evaluation samples: {len(eval_folder)}\")\n",
    "print(f\"Nmumber of testing samples: {len(test_folder)}\")\n",
    "print(f\"Total number of samples: {len(train_folder)+len(test_folder)+len(eval_folder)}\")\n",
    "\n",
    "# CHECKING ERRORS FOR FUNCTION: get_bounding_box\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "\n",
    "    # if len(values) != 31:                                                                   questo controllo non serve in quanto dipende da quante cifre ogni numero ha\n",
    "    #     raise ValueError(f\" 1 st exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v2=values.split(\"&\")\n",
    "\n",
    "    if len(values_v2) != 5:\n",
    "        raise ValueError(f\"2 nd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "\n",
    "    if len(values_v3) != 3:\n",
    "        raise ValueError(f\"3 rd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "\n",
    "    if len(x_coords) != 4:\n",
    "        raise ValueError(f\"4 th exception: Invalid plate text format in filename: {file}\")\n",
    "\n",
    " # CHECKING ERRORS FOR FUNCTION: get_text\n",
    "    \n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "        values=file.split(\"-\")\n",
    "\n",
    "        if len(values)!=7:\n",
    "            raise ValueError(f\"1 st exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        text=str(values[4])\n",
    "\n",
    "        # if len(text)!=7:                                                                                questo controllo non serve in quanto dipende da quante cifre ogni numero ha cifre diverse\n",
    "        #     raise ValueError(f\"2 nd exception occurred: Invalid plate text format in filename: {file}\") \n",
    "        \n",
    "        indices=text.split(\"_\")\n",
    "\n",
    "        if len(indices) != 8:\n",
    "            raise ValueError(f\"3 rd exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        province_character=provinces[int(indices[0])]\n",
    "        alphabet_character=alphabet[int(indices[1])]\n",
    "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "        if len(plate_text) != 8:\n",
    "            raise ValueError(f\"4 th exception occurred: Invalid plate text format in filename: {file}\")\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_bounding_box\n",
    "\n",
    "file=\"308069444444444444-91_94-8&424_589&564-578&564_27&553_8&431_589&424-0_0_3_24_32_25_32_25-100-241.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_text\n",
    "\n",
    "file=\"30237890625-87_88-197&472_449&567-449&553_201&567_197&472_448&477-10_2_3_24_31_33_26_24-192-75.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file2.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUOVO DATASET CON PIU ELEMENTI DI TRAIN E MENO DI TEST\n",
    "\n",
    "path_train_og=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "path_test_og=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "path_train_tf=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/train\"\n",
    "path_test_tf=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\"\n",
    "folder_train_og=os.listdir(path_train_og)\n",
    "folder_test_og=os.listdir(path_test_og)\n",
    "folder_train_tf=os.listdir(path_train_tf)\n",
    "folder_test_tf=os.listdir(path_test_tf)\n",
    "print(f\"number of original training samples: {len(folder_train_og)}\")\n",
    "print(f\"number of original test samples: {len(folder_test_og)}\")\n",
    "print(f\"number of new training samples: {len(folder_train_tf)}\")\n",
    "print(f\"number of new training samples: {len(folder_test_tf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9344421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERSECTION OVER UNION FUNCTION\n",
    "\n",
    "def compute_IoU(box1, box2):\n",
    "\n",
    "    box1=box1.squeeze()\n",
    "    box2=box2.squeeze()\n",
    "\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    area_of_intersection = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3]- box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    IoU = area_of_intersection / float(area_box1 + area_box2 - area_of_intersection)\n",
    "\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET   \n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "smaller_train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train-20250508T142228Z-1-001/train\"\n",
    "train_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train2\"\n",
    "eval_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval2\"\n",
    "test_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\"\n",
    "\n",
    "class CCPD_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, transforms=None):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.folder = os.listdir(path)\n",
    "        self.images = [f for f in self.folder if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        full_path = os.path.join(self.path, file)\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "\n",
    "        bbox = get_bounding_box(file)\n",
    "        tensor_bbox = torch.tensor([bbox], dtype=torch.float32)\n",
    "        label = torch.tensor([1], dtype=torch.int64)  \n",
    "\n",
    "        target = {\"boxes\": tensor_bbox, \"labels\": label}\n",
    "\n",
    "        if target[\"boxes\"].shape[0] != target[\"labels\"].shape[0]:\n",
    "            raise ValueError(f\"Mismatch in number of boxes and labels for file: {file}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# EXAMPLE OF THE OUTPUT:\n",
    "\n",
    "# image_path_example=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train/04-90_267-158&448_542&553-541&553_162&551_158&448_542&450-0_1_3_24_27_33_30_24-99-116.jpg\"\n",
    "# images, targets =next(iter(train_dataset))\n",
    "# print(f\"images: {images}, targets: {targets}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd23f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train_medium\"\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval_medium\"\n",
    "test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\"\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = CCPD_Dataset(train_path, transforms=transform)\n",
    "eval_dataset = CCPD_Dataset(eval_path, transforms=transform)\n",
    "test_dataset = CCPD_Dataset(test_path, transforms=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=0) # watch out for the batch size, it is set to 1 for compatibility with the model\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)  # watch out for the batch size, it is set to 1 for compatibility with the model\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)  # watch out for the batch size, it is set to 1 for compatibility with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "model.rpn.post_nms_top_n_train = 500\n",
    "model.rpn.post_nms_top_n_test = 500\n",
    "model.roi_heads.detections_per_img = 50  \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "best_boxes=[]\n",
    "best_avg_iou = 0.0\n",
    "num_epochs=1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    count=1\n",
    "    for images, targets in train_dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "        print(f\"Batch: {count} Training Loss: {losses.item():.4f}\")\n",
    "        count+=1\n",
    "\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in eval_dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes'].to(device)\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    best_iou = 0.0\n",
    "                    best_box = None\n",
    "                    for box, score in zip(pred['boxes'], pred['scores']):\n",
    "                        if score > 0.0:\n",
    "                            iou = compute_IoU(box, gt_boxes[0])\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_box = box\n",
    "                    if best_iou > 0:\n",
    "                        total_iou += best_iou\n",
    "                        count += 1\n",
    "                        best_boxes.append(best_box.cpu())\n",
    "\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "\n",
    "    if avg_iou > best_avg_iou:\n",
    "        best_avg_iou = avg_iou\n",
    "        torch.save(model.state_dict(), \"best_frcnn_model.pth\")\n",
    "        print(f\"Saved best model with IoU: {best_avg_iou:.4f}\")\n",
    "\n",
    "    print(f\"Average IoU on eval set: {avg_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b143031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE MODEL\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('best_frcnn_model _final_version.pth', map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE TRAINED MODEL USING THE BEST BOUNDING BOX\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "model.load_state_dict(torch.load('best_frcnn_model _final_version.pth'))\n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\")\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4459ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE UNTRAINED MODEL USING THE BEST BOUNDING BOX\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\")\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING AVERAGE IOU ON TEST SET USING THE TRAINED MODEL\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "model.load_state_dict(torch.load('best_frcnn_model _final_version.pth'))\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\")\n",
    "total_iou=0\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        if len(output[0]['scores'])>0:\n",
    "            score = output[0]['scores'][0].item()        \n",
    "            best_bb=pred['boxes'][0].to(device)\n",
    "            gt_box=get_bounding_box(image_path)\n",
    "            tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "            iou=compute_IoU(best_bb, tensor_bbox)\n",
    "            total_iou+=iou\n",
    "print(f\"Average IoU: {total_iou/len(folder)}\") # 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b0620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP FUNCTION WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_RCNN(file):\n",
    "    image = Image.open(file).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)[0]\n",
    "        if len(prediction['boxes']) == 0:\n",
    "            print(f\"No box found for image: {file}\")\n",
    "            return None\n",
    "        best_bb = prediction['boxes'][0].to(device).int()\n",
    "        cropped = img_tensor[0, :, best_bb[1]:best_bb[3], best_bb[0]:best_bb[2]]\n",
    "        cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "        return cropped_resized.squeeze(0)  \n",
    "\n",
    "def crop_folder_with_RCNN(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text=get_text(full_path)\n",
    "        cropped_image = crop_image_with_RCNN(full_path)\n",
    "        if cropped_image is not None:\n",
    "            cropped_folder.append([cropped_image, gt_text])\n",
    "    return cropped_folder\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image)\n",
    "    cropped = img_tensor[:, int(bb[1]):int(bb[3]), int(bb[0]):int(bb[2])]\n",
    "    cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "    return cropped_resized.squeeze(0)\n",
    "\n",
    "def crop_folder_with_ground_truth(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text = get_text(file)  # Get ground truth text\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append([cropped_image, gt_text])  # Store image and text pair\n",
    "    return cropped_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE RESULTS OF THE TRAINED MODEL\n",
    "\n",
    "for image in os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\"):\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\", image)\n",
    "        cropped = crop_image_with_RCNN(image_path)\n",
    "        plt.imshow(cropped.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Cropped Image\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57dcd811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10227/49714682.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model.pth'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m model.eval()\n\u001b[32m     11\u001b[39m eval_path=\u001b[33m\"\u001b[39m\u001b[33m/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval_small\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m cropped_folder=\u001b[43mcrop_folder_with_RCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m image=cropped_folder[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m     15\u001b[39m image_np = image.permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m).cpu().numpy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcrop_folder_with_RCNN\u001b[39m\u001b[34m(folder_path)\u001b[39m\n\u001b[32m     21\u001b[39m full_path = os.path.join(folder_path, file)\n\u001b[32m     22\u001b[39m gt_text=get_text(full_path)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m cropped_image = \u001b[43mcrop_image_with_RCNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cropped_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     25\u001b[39m     cropped_folder.append([cropped_image, gt_text])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcrop_image_with_RCNN\u001b[39m\u001b[34m(file)\u001b[39m\n\u001b[32m      6\u001b[39m img_tensor = transform(image).unsqueeze(\u001b[32m0\u001b[39m).to(device) \n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     prediction = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_tensor\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prediction[\u001b[33m'\u001b[39m\u001b[33mboxes\u001b[39m\u001b[33m'\u001b[39m]) == \u001b[32m0\u001b[39m:\n\u001b[32m     10\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo box found for image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[39m, in \u001b[36mGeneralizedRCNN.forward\u001b[39m\u001b[34m(self, images, targets)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(features, torch.Tensor):\n\u001b[32m    103\u001b[39m     features = OrderedDict([(\u001b[33m\"\u001b[39m\u001b[33m0\u001b[39m\u001b[33m\"\u001b[39m, features)])\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m proposals, proposal_losses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrpn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m detections, detector_losses = \u001b[38;5;28mself\u001b[39m.roi_heads(features, proposals, images.image_sizes, targets)\n\u001b[32m    106\u001b[39m detections = \u001b[38;5;28mself\u001b[39m.transform.postprocess(detections, images.image_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torchvision/models/detection/rpn.py:361\u001b[39m, in \u001b[36mRegionProposalNetwork.forward\u001b[39m\u001b[34m(self, images, features, targets)\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;66;03m# RPN uses all feature maps that are available\u001b[39;00m\n\u001b[32m    360\u001b[39m features = \u001b[38;5;28mlist\u001b[39m(features.values())\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m objectness, pred_bbox_deltas = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    362\u001b[39m anchors = \u001b[38;5;28mself\u001b[39m.anchor_generator(images, features)\n\u001b[32m    364\u001b[39m num_images = \u001b[38;5;28mlen\u001b[39m(anchors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torchvision/models/detection/rpn.py:77\u001b[39m, in \u001b[36mRPNHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     75\u001b[39m     t = \u001b[38;5;28mself\u001b[39m.conv(feature)\n\u001b[32m     76\u001b[39m     logits.append(\u001b[38;5;28mself\u001b[39m.cls_logits(t))\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     bbox_reg.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbbox_pred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logits, bbox_reg\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    538\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    539\u001b[39m         F.pad(\n\u001b[32m    540\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    547\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    548\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF CROPPING A FOLDER WITH \"crop_folder_with_RCNN\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "model.load_state_dict(torch.load('model_weights/best_frcnn_model.pth'))\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval_small\"\n",
    "cropped_folder=crop_folder_with_RCNN(eval_path)\n",
    "\n",
    "image=cropped_folder[0][0]\n",
    "image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"ground truth text: {cropped_folder[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a173180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE PDLPR\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torchvision.models import mobilenet_v2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "all_characters = sorted(set(provinces + alphabet + ads))\n",
    "char_list = sorted(set(provinces + alphabet + ads))\n",
    "char2idx = {'<blank>': 0}  # blank at index 0\n",
    "char2idx.update({char: idx + 1 for idx, char in enumerate(char_list)})\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class CroppedImages(Dataset):\n",
    "    def __init__(self, folder, transformations):\n",
    "        self.folder = folder\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.folder[idx][0]\n",
    "        gt_text=self.folder[idx][1]\n",
    "        return self.transformations(image), gt_text\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, labels= zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    return images, labels\n",
    "\n",
    "class MobilenetV2FeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        base = mobilenet_v2(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(base.features.children()))\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(1280, 512, kernel_size=1),\n",
    "            nn.AdaptiveAvgPool2d((6, 18))\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=200):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "class ParallelDecoder(nn.Module):\n",
    "    def __init__(self, d_model=512, num_chars=8, num_classes=68, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.query_embed = nn.Parameter(torch.randn(num_chars, d_model))\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, memory):\n",
    "        B, _, D = memory.shape\n",
    "        queries = self.query_embed.unsqueeze(1).repeat(1, B, 1)  \n",
    "        memory = memory.permute(1, 0, 2)  \n",
    "        decoded = self.decoder(tgt=queries, memory=memory)  \n",
    "        decoded = decoded.permute(1, 0, 2)  \n",
    "        logits = self.classifier(decoded)   \n",
    "        return logits\n",
    "\n",
    "trans_train = transforms.Compose([\n",
    "    transforms.Resize((64, 256)),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1)),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trans_eval = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# model.eval()\n",
    "# cropped_folder_train=crop_folder_with_RCNN(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train2\")\n",
    "# cropped_folder_eval=crop_folder_with_RCNN(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval2\")\n",
    "\n",
    "# train_dataset = CroppedImages(cropped_folder_train, trans_train)\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# eval_dataset = CroppedImages(cropped_folder_eval, trans_eval)\n",
    "# eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Instantiate models\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# feature_extractor = MobilenetV2FeatureExtractor().to(device)\n",
    "# pos_encoder = PositionalEncoding(d_model=512).to(device)\n",
    "\n",
    "# # Transformer\n",
    "# encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "# transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4).to(device)\n",
    "# decoder = ParallelDecoder(d_model=512, num_chars=8, num_classes=68).to(device)\n",
    "\n",
    "def encode_labels(label_list, char2idx, max_len=8):\n",
    "    encoded = []\n",
    "    for label in label_list:\n",
    "        label = label[:max_len].ljust(max_len)\n",
    "        encoded.append([char2idx[c] for c in label])\n",
    "    return torch.tensor(encoded, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63bd668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF WHAT THE DATASET RETURNS AND CHECK ON ENCODED LABELS\n",
    "\n",
    "cropped_folder = crop_folder_with_ground_truth(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/single_sample_eval\")\n",
    "dataset = CroppedImages(cropped_folder, trans_eval)\n",
    "train_dataloader = DataLoader(dataset, batch_size=1, collate_fn=collate_fn)\n",
    "item=next(iter(train_dataloader))\n",
    "cropped = crop_image_with_ground_truth('/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/single_sample_eval/03625-92_266-225&396_573&501-568&501_227&490_225&396_573&414-0_0_3_17_25_28_27_30-102-83.jpg')\n",
    "plt.imshow(cropped.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Cropped Image\")\n",
    "plt.show()\n",
    "print(f\"Encoded labels: {encode_labels(item[1],char2idx)}\")\n",
    "predicted_indices= encode_labels(item[1], char2idx).squeeze().tolist()\n",
    "plate = ''.join([idx2char[i] for i in predicted_indices])\n",
    "print(f\"Decoded labels: {plate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF UNTRAINED PDLPR PREDICTION\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Get cropped image and process through model\n",
    "    cropped_image = crop_image_with_RCNN('/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2/03125-89_263-177&502_477&597-464&581_177&597_185&513_477&502-0_0_3_27_29_25_33_33-102-60.jpg')\n",
    "    cropped_image = cropped_image.to(device)\n",
    "    features = feature_extractor(cropped_image.unsqueeze(0).to(device))  \n",
    "    B, C, H, W = features.shape\n",
    "    features = features.view(B, C, H * W).permute(0, 2, 1)\n",
    "    features = pos_encoder(features)\n",
    "    memory = transformer_encoder(features)\n",
    "    logits = decoder(memory)\n",
    "    \n",
    "    # Get predictions\n",
    "    predicted_indices = logits.argmax(dim=-1)[0]  # Take first batch item\n",
    "    predicted_text = ''.join([idx2char[idx.item()] for idx in predicted_indices])\n",
    "    \n",
    "    # Get ground truth text from filename\n",
    "    filename = os.path.basename('/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2/03125-89_263-177&502_477&597-464&581_177&597_185&513_477&502-0_0_3_27_29_25_33_33-102-60.jpg')\n",
    "    ground_truth = get_text(filename)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Predicted text: {predicted_text}\")\n",
    "    print(f\"Ground truth: {ground_truth}\")\n",
    "    \n",
    "    # Optionally visualize the cropped image\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(cropped_image.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dfd5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE SIMPLE PDLPR TRAINING AND EVALUATION \n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model components setup \n",
    "feature_extractor = MobilenetV2FeatureExtractor().to(device)\n",
    "pos_encoder = PositionalEncoding(d_model=512).to(device)\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dropout=0.3)\n",
    "transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4).to(device)\n",
    "decoder = ParallelDecoder(d_model=512, num_chars=8, num_classes=68).to(device)\n",
    "\n",
    "# Create optimizer and scheduler\n",
    "params = (list(feature_extractor.parameters()) + \n",
    "         list(transformer_encoder.parameters()) + \n",
    "         list(decoder.parameters()))\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 40\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    feature_extractor.train()\n",
    "    transformer_encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for batch_images, batch_labels in train_dataloader:\n",
    "        batch_images = batch_images.to(device)\n",
    "        batch_labels_encoded = encode_labels(batch_labels, char2idx).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        features = feature_extractor(batch_images)\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.view(B, C, H * W).permute(0, 2, 1)\n",
    "        features = pos_encoder(features)\n",
    "        memory = transformer_encoder(features)\n",
    "        logits = decoder(memory)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = 0\n",
    "        for i in range(8):\n",
    "            loss += criterion(logits[:, i, :], batch_labels_encoded[:, i])\n",
    "        loss /= 8\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(params, max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        pred = logits.argmax(dim=-1)\n",
    "        train_correct += (pred == batch_labels_encoded).sum().item()\n",
    "        train_total += batch_labels_encoded.numel()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    train_accuracy = train_correct / train_total\n",
    "\n",
    "    # Evaluation phase\n",
    "    feature_extractor.eval()\n",
    "    transformer_encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_correct = 0 \n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in eval_dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels_encoded = encode_labels(batch_labels, char2idx).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            features = feature_extractor(batch_images)\n",
    "            B, C, H, W = features.shape\n",
    "            features = features.view(B, C, H * W).permute(0, 2, 1)\n",
    "            features = pos_encoder(features)\n",
    "            memory = transformer_encoder(features)\n",
    "            logits = decoder(memory)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = 0\n",
    "            for i in range(8):\n",
    "                loss += criterion(logits[:, i, :], batch_labels_encoded[:, i])\n",
    "            loss /= 8\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            val_correct += (pred == batch_labels_encoded).sum().item()\n",
    "            val_total += batch_labels_encoded.numel()\n",
    "\n",
    "    avg_val_loss = val_loss / len(eval_dataloader)\n",
    "    val_accuracy = val_correct / val_total\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save best model and early stopping check\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'feature_extractor_state_dict': feature_extractor.state_dict(),\n",
    "            'pos_encoder_state_dict': pos_encoder.state_dict(),\n",
    "            'transformer_encoder_state_dict': transformer_encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, 'best_pdlpr_model.pth')\n",
    "        print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442e1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE OF TRAINED PDLPR PREDICTION\n",
    "\n",
    "def pdlpr_inference(image_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform inference on a single image\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((48, 144)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Load image and model checkpoint\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize models\n",
    "    feature_extractor = MobilenetV2FeatureExtractor().to(device)\n",
    "    pos_encoder = PositionalEncoding(d_model=512).to(device)\n",
    "    encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, dropout=0.3)\n",
    "    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4).to(device)\n",
    "    decoder = ParallelDecoder(d_model=512, num_chars=8, num_classes=68).to(device)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    try:\n",
    "        checkpoint = torch.load('best_pdlpr_model.pth', map_location=device)\n",
    "        feature_extractor.load_state_dict(checkpoint['feature_extractor_state_dict'])\n",
    "        pos_encoder.load_state_dict(checkpoint['pos_encoder_state_dict'])\n",
    "        transformer_encoder.load_state_dict(checkpoint['transformer_encoder_state_dict'])\n",
    "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Set models to eval mode\n",
    "    feature_extractor.eval()\n",
    "    transformer_encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Inference\n",
    "    with torch.no_grad():\n",
    "        # Feature extraction\n",
    "        features = feature_extractor(image_tensor)\n",
    "        B, C, H, W = features.shape\n",
    "        features = features.view(B, C, H * W).permute(0, 2, 1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        features = pos_encoder(features)\n",
    "        \n",
    "        # Transform features\n",
    "        memory = transformer_encoder(features)\n",
    "        \n",
    "        # Decode\n",
    "        logits = decoder(memory)\n",
    "        \n",
    "        # Get predictions\n",
    "        predicted_indices = logits.argmax(dim=-1)[0]  # Take first batch item\n",
    "        plate_text = ''.join([idx2char[idx.item()] for idx in predicted_indices])\n",
    "        \n",
    "        return plate_text\n",
    "\n",
    "# Test function to process multiple images\n",
    "def test_multiple_images(image_paths):\n",
    "    \"\"\"\n",
    "    Test PDLPR on multiple images and show results\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        # Get ground truth text from filename\n",
    "        gt_text = get_text(os.path.basename(img_path))\n",
    "        \n",
    "        # Get prediction\n",
    "        pred_text = pdlpr_inference(img_path, device)\n",
    "        \n",
    "        # Display results\n",
    "        img = Image.open(img_path)\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"GT: {gt_text}\\nPred: {pred_text}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print(f\"Image: {os.path.basename(img_path)}\")\n",
    "        print(f\"Ground truth: {gt_text}\")\n",
    "        print(f\"Prediction: {pred_text}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage\n",
    "\n",
    "test_images = [\n",
    "    \"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2/04-91_254-145&472_529&567-529&567_164&552_145&472_529&485-0_0_3_25_24_24_24_29-148-355.jpg\",\n",
    "    \"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2/05-90_257-137&507_572&612-567&611_158&612_137&507_572&512-0_0_3_28_32_25_24_32-144-135.jpg\",\n",
    "    \"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2/035-93_261-170&516_506&611-502&611_177&594_170&516_506&536-0_0_3_28_25_33_32_32-99-116.jpg\"\n",
    "]\n",
    "\n",
    "test_multiple_images(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41e0fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1614040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDLPR MODEL FOLLOWING PAPER ARCHITECTURE\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "# --- Focus Structure Module ---\n",
    "class Focus(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64, dropout=0.1):\n",
    "        super(Focus, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Slice and concat\n",
    "        patch1 = x[..., ::2, ::2]\n",
    "        patch2 = x[..., ::2, 1::2]\n",
    "        patch3 = x[..., 1::2, ::2]\n",
    "        patch4 = x[..., 1::2, 1::2]\n",
    "        x = torch.cat([patch1, patch2, patch3, patch4], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- CNN Block used in RESBLOCK and downsampling ---\n",
    "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "# --- Residual Block ---\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            conv_block(channels, channels),\n",
    "            conv_block(channels, channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "# --- IGFE Module ---\n",
    "class IGFE(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(IGFE, self).__init__()\n",
    "        self.focus = Focus(3, 64, dropout)\n",
    "        self.down1 = conv_block(64, 128, stride=2)\n",
    "        self.res1 = ResBlock(128)\n",
    "        self.res2 = ResBlock(128)\n",
    "        self.down2 = conv_block(128, 256, stride=2)\n",
    "        self.res3 = ResBlock(256)\n",
    "        self.res4 = ResBlock(256)\n",
    "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.focus(x)\n",
    "        x = self.down1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.down2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.res4(x)\n",
    "        x = self.final_conv(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# --- Positional Encoding for 2D feature maps ---\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, height, width):\n",
    "        super().__init__()\n",
    "        self.pe = nn.Parameter(torch.randn(1, d_model, height, width))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe\n",
    "\n",
    "\n",
    "# --- Transformer Encoder Block ---\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(d_model, d_model * 2, kernel_size=1)\n",
    "        self.attn = nn.MultiheadAttention(d_model * 2, num_heads=8, batch_first=True)\n",
    "        self.cnn2 = nn.Conv2d(d_model * 2, d_model, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.cnn1(x)  # [B, 1024, H, W]\n",
    "        x_ = x.flatten(2).transpose(1, 2)  # [B, HW, 1024]\n",
    "        x_, _ = self.attn(x_, x_, x_)\n",
    "        x_ = x_.transpose(1, 2).view(B, -1, H, W)  # [B, 1024, H, W]\n",
    "        x = self.cnn2(x_)\n",
    "        return self.norm(x.flatten(2).transpose(1, 2)).transpose(1, 2).view(B, C, H, W)\n",
    "\n",
    "# --- Parallel Decoder Block ---\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.cross_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 2, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, tgt, memory):\n",
    "        # Masked self-attention (causal)\n",
    "        B, T, _ = tgt.shape\n",
    "        mask = torch.triu(torch.ones(T, T), diagonal=1).bool().to(tgt.device)\n",
    "        tgt2, _ = self.self_attn(tgt, tgt, tgt, attn_mask=mask)\n",
    "        tgt = self.norm1(tgt + tgt2)\n",
    "\n",
    "        # Cross-attention\n",
    "        tgt2, _ = self.cross_attn(tgt, memory, memory)\n",
    "        tgt = self.norm2(tgt + tgt2)\n",
    "\n",
    "        # Feedforward\n",
    "        tgt2 = self.ffn(tgt)\n",
    "        tgt = self.norm3(tgt + tgt2)\n",
    "        return tgt\n",
    "\n",
    "\n",
    "# --- Full PDLPR Recognition Model ---\n",
    "class PDLPR(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0.1):\n",
    "        super(PDLPR, self).__init__()\n",
    "        self.igfe = IGFE(dropout)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
    "        self.pos_encoding = PositionalEncoding(512, 6, 18)\n",
    "        self.encoder = nn.Sequential(*[EncoderBlock(512) for _ in range(3)])\n",
    "        self.flatten = lambda x: x.flatten(2).transpose(1, 2)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderBlock(512) for _ in range(3)])\n",
    "        self.cls_head = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, tgt_seq):\n",
    "        # Clean up any GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        x = self.igfe(x)                     \n",
    "        x = self.adaptive_pool(x)            \n",
    "        x = self.dropout(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        x = self.encoder(x)\n",
    "        memory = self.flatten(x)             \n",
    "\n",
    "        tgt = tgt_seq                        \n",
    "        for block in self.decoder_blocks:\n",
    "            tgt = block(tgt, memory)\n",
    "            tgt = self.dropout(tgt)\n",
    "\n",
    "        logits = self.cls_head(tgt)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4223d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FULL SETUP\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('model_weights/best_frcnn_model.pth'))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model=load_Fasterrcnn(device=\"cpu\")\n",
    "device=\"cpu\"\n",
    "model.eval()\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "all_characters = sorted(set(provinces + alphabet + ads))\n",
    "char_list = sorted(set(provinces + alphabet + ads))\n",
    "char2idx = {'<blank>': 0}  # blank at index 0\n",
    "char2idx.update({char: idx + 1 for idx, char in enumerate(char_list)})\n",
    "idx2char = {idx: char for char, idx in char2idx.items()}\n",
    "\n",
    "# Dataset\n",
    "class CroppedImages(Dataset):\n",
    "    def __init__(self, folder, transformations):\n",
    "        self.folder = folder\n",
    "        self.transformations = transformations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = self.folder[idx][0]\n",
    "            gt_text = self.folder[idx][1]\n",
    "            \n",
    "            if image is None:\n",
    "                raise ValueError(f\"None image at index {idx}\")\n",
    "                \n",
    "            if self.transformations:\n",
    "                image = self.transformations(image)\n",
    "                \n",
    "            return image, gt_text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            return torch.zeros(3, 48, 144), \"\"\n",
    "\n",
    "def encode_labels(label_list, char2idx, max_len=8):\n",
    "    encoded = []\n",
    "    for label in label_list:\n",
    "        label = label[:max_len].ljust(max_len)\n",
    "        encoded.append([char2idx[c] for c in label])\n",
    "    return torch.tensor(encoded, dtype=torch.long)\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat([encode_labels([l], char2idx) for l in labels])\n",
    "    return images, labels, label_lengths\n",
    "\n",
    "def ctc_greedy_decoder(output, idx2char, blank=0):\n",
    "    '''\n",
    "    Now, I know the network returns probabilities, as it does a softmax with logits of characters.\n",
    "    I need to transform that probability into an actual char to compose the plate.\n",
    "    I take the argmax of the softmax (most prob char), remove blanks used by CTC and possible\n",
    "    duplicates CTC can actually produce.\n",
    "    At the end I simply use the  mappings char-index index-char deefined at the beginning to compose the plate.\n",
    "    This is greedy as it just takes the argmax of every step, I think it's more than enough here.\n",
    "    '''\n",
    "    # output: [seq_len, batch, num_classes]\n",
    "    out = output.permute(1, 0, 2)  # [batch, seq_len, num_classes]\n",
    "    pred_strings = []\n",
    "    for probs in out:\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        #\n",
    "        prev = -1\n",
    "        pred_str = []\n",
    "        for p in pred:\n",
    "            if p != blank and p != prev:\n",
    "                pred_str.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(''.join(pred_str))\n",
    "    return pred_strings\n",
    "\n",
    "trans_train = transforms.Compose([\n",
    "    transforms.Resize((48, 144)),\n",
    "    transforms.RandomApply([\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2),\n",
    "        transforms.GaussianBlur(3, sigma=(0.1, 2.0))\n",
    "    ], p=0.5),\n",
    "    transforms.RandomRotation(degrees=(-5, 5)),\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0, \n",
    "        translate=(0.1, 0.1),\n",
    "        scale=(0.9, 1.1)\n",
    "    ),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "trans_eval = transforms.Compose([\n",
    "    transforms.Resize((48, 144)),  # Fixed input size\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "model=load_Fasterrcnn(device=\"cpu\")\n",
    "device=\"cpu\"\n",
    "model.eval()\n",
    "\n",
    "cropped_folder_train=crop_folder_with_ground_truth(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train_small\")\n",
    "cropped_folder_eval=crop_folder_with_ground_truth(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval_small\")\n",
    "\n",
    "train_dataset = CroppedImages(cropped_folder_train, trans_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "eval_dataset = CroppedImages(cropped_folder_eval, trans_eval)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=8, shuffle=False, collate_fn=ctc_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277c8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPLETE PDLPR TRAINING AND EVALUATION WITH CTC LOSS\n",
    "\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "# Device setup \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Model setup\n",
    "model = PDLPR(num_classes=len(char2idx), dropout=0.1).to(device)\n",
    "criterion = CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.0005)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min', \n",
    "    factor=0.7,\n",
    "    patience=3,\n",
    "    verbose=True,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "# CTC Loss initialization\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 15\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    true_train_accuracy = 0\n",
    "\n",
    "    for batch_idx, (batch_images, batch_labels, label_lengths) in enumerate(train_dataloader):\n",
    "        try:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            B = batch_images.size(0)\n",
    "            tgt_seq = torch.zeros(B, 8, 512).to(device)\n",
    "            \n",
    "            # Forward pass \n",
    "            logits = model(batch_images, tgt_seq)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "            \n",
    "            input_lengths = torch.full(size=(B,), \n",
    "                                    fill_value=8,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "            \n",
    "            # Calculate CTC loss\n",
    "            loss = criterion(log_probs.permute(1, 0, 2),\n",
    "                           batch_labels,\n",
    "                           input_lengths,\n",
    "                           label_lengths)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy using CTC decoder\n",
    "            pred_text = ctc_greedy_decoder(log_probs.permute(1, 0, 2), idx2char)\n",
    "            for i in range(B):\n",
    "                true_text = ''.join([idx2char[idx.item()] for idx in batch_labels[i][:label_lengths[i]]])\n",
    "                if pred_text[i] == true_text:\n",
    "                    true_train_accuracy += 1\n",
    "\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "    true_train_acc = true_train_accuracy / (len(train_dataloader) * B)\n",
    "\n",
    "    # Evaluation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    true_val_accuracy = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels, label_lengths in eval_dataloader:\n",
    "            batch_images = batch_images.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "            label_lengths = label_lengths.to(device)\n",
    "\n",
    "            B = batch_images.size(0)\n",
    "            tgt_seq = torch.zeros(B, 8, 512).to(device)\n",
    "\n",
    "            logits = model(batch_images, tgt_seq)\n",
    "            log_probs = logits.log_softmax(2)\n",
    "\n",
    "            input_lengths = torch.full(size=(B,), \n",
    "                                    fill_value=8,\n",
    "                                    dtype=torch.long,\n",
    "                                    device=device)\n",
    "\n",
    "            loss = criterion(log_probs.permute(1, 0, 2),\n",
    "                           batch_labels,\n",
    "                           input_lengths,\n",
    "                           label_lengths)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy using CTC decoder\n",
    "            pred_text = ctc_greedy_decoder(log_probs.permute(1, 0, 2), idx2char)\n",
    "            for i in range(B):\n",
    "                true_text = ''.join([idx2char[idx.item()] for idx in batch_labels[i][:label_lengths[i]]])\n",
    "                if pred_text[i] == true_text:\n",
    "                    true_val_accuracy += 1\n",
    "\n",
    "    avg_val_loss = val_loss / len(eval_dataloader)\n",
    "    true_val_acc = true_val_accuracy / (len(eval_dataloader) * B)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }, 'best_pdlpr_model_ctc.pth')\n",
    "        print(f\"Saved new best model with validation loss: {best_val_loss:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f} | True Train Accuracy: {true_train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f} | True Val Accuracy: {true_val_acc:.4f}\")\n",
    "    print(f\"Current Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2564dd64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
