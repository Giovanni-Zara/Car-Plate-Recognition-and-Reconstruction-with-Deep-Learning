{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66a3593",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68e0bc",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ea5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS FOR THE TEXT OF THE CAR PLATE\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "# FULL DATASETS\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "eval_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "test_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "train_folder=os.listdir(train_path) \n",
    "eval_folder=os.listdir(eval_path_medium)\n",
    "test_folder=os.listdir(test_path_medium)\n",
    "\n",
    "# MEDIUM DATASETS\n",
    "\n",
    "train_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train_medium\"\n",
    "eval_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval_medium\"\n",
    "test_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b18dc",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a549a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]\n",
    "\n",
    "# INTERSECTION OVER UNION FUNCTION\n",
    "\n",
    "def compute_IoU(box1, box2):\n",
    "\n",
    "    box1=box1.squeeze()\n",
    "    box2=box2.squeeze()\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    area_of_intersection = max(0, xB - xA) * max(0, yB - yA)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3]- box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    IoU = area_of_intersection / float(area_box1 + area_box2 - area_of_intersection)\n",
    "\n",
    "    return IoU\n",
    "\n",
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    \n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d38499",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11695bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATASET STRUCTURE\n",
    "\n",
    "print(f\"Nmumber of training samples: {len(train_folder)}\")\n",
    "print(f\"Nmumber of evaluation samples: {len(eval_folder)}\")\n",
    "print(f\"Nmumber of testing samples: {len(test_folder)}\")\n",
    "print(f\"Total number of samples: {len(train_folder)+len(test_folder)+len(eval_folder)}\")\n",
    "\n",
    "# CHECKING ERRORS FOR FUNCTION: \"get_bounding_box\"\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "\n",
    "    # if len(values) != 31:                                                                   questo controllo non serve in quanto dipende da quante cifre ogni numero ha\n",
    "    #     raise ValueError(f\" 1 st exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v2=values.split(\"&\")\n",
    "\n",
    "    if len(values_v2) != 5:\n",
    "        raise ValueError(f\"2 nd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "\n",
    "    if len(values_v3) != 3:\n",
    "        raise ValueError(f\"3 rd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "\n",
    "    if len(x_coords) != 4:\n",
    "        raise ValueError(f\"4 th exception: Invalid plate text format in filename: {file}\")\n",
    "\n",
    " # CHECKING ERRORS FOR FUNCTION: \"get_text\"\n",
    " \n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "        values=file.split(\"-\")\n",
    "\n",
    "        if len(values)!=7:\n",
    "            raise ValueError(f\"1 st exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        text=str(values[4])\n",
    "\n",
    "        # if len(text)!=7:                                                                                questo controllo non serve in quanto dipende da quante cifre ogni numero ha cifre diverse\n",
    "        #     raise ValueError(f\"2 nd exception occurred: Invalid plate text format in filename: {file}\") \n",
    "        \n",
    "        indices=text.split(\"_\")\n",
    "\n",
    "        if len(indices) != 8:\n",
    "            raise ValueError(f\"3 rd exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        province_character=provinces[int(indices[0])]\n",
    "        alphabet_character=alphabet[int(indices[1])]\n",
    "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "        if len(plate_text) != 8:\n",
    "            raise ValueError(f\"4 th exception occurred: Invalid plate text format in filename: {file}\")\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: \"get_bounding_box\"\n",
    "\n",
    "file=\"308069444444444444-91_94-8&424_589&564-578&564_27&553_8&431_589&424-0_0_3_24_32_25_32_25-100-241.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_text\n",
    "\n",
    "file=\"30237890625-87_88-197&472_449&567-449&553_201&567_197&472_448&477-10_2_3_24_31_33_26_24-192-75.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file2.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET  CLASS  \n",
    "\n",
    "class CCPD_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, transforms=None):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.folder = os.listdir(path)\n",
    "        self.images = [f for f in self.folder if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        full_path = os.path.join(self.path, file)\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "\n",
    "        bbox = get_bounding_box(file)\n",
    "        tensor_bbox = torch.tensor([bbox], dtype=torch.float32)\n",
    "        label = torch.tensor([1], dtype=torch.int64)  \n",
    "\n",
    "        target = {\"boxes\": tensor_bbox, \"labels\": label}\n",
    "\n",
    "        if target[\"boxes\"].shape[0] != target[\"labels\"].shape[0]:\n",
    "            raise ValueError(f\"Mismatch in number of boxes and labels for file: {file}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "# COLLATE FUNCTION\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "transform = T.ToTensor()\n",
    "train_dataset_medium = CCPD_Dataset(train_path_medium, transforms=transform)\n",
    "eval_dataset_medium = CCPD_Dataset(eval_path_medium, transforms=transform)\n",
    "test_dataset_medium = CCPD_Dataset(test_path_medium, transforms=transform)\n",
    "\n",
    "train_dataloader_medium = DataLoader(train_dataset_medium, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=0) \n",
    "eval_dataloader_medium = DataLoader(eval_dataset_medium, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)  \n",
    "test_dataloader_medium = DataLoader(test_dataset_medium, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbd0ca",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "model.rpn.post_nms_top_n_train = 500\n",
    "model.rpn.post_nms_top_n_test = 500\n",
    "model.roi_heads.detections_per_img = 50  \n",
    "device=\"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def8649",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "best_boxes=[]\n",
    "best_avg_iou = 0.0\n",
    "num_epochs=1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    count=1\n",
    "    for images, targets in train_dataloader_medium:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "        print(f\"Batch: {count} Training Loss: {losses.item():.4f}\")\n",
    "        count+=1\n",
    "\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in eval_dataloader_medium:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes'].to(device)\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    best_iou = 0.0\n",
    "                    best_box = None\n",
    "                    for box, score in zip(pred['boxes'], pred['scores']):\n",
    "                        if score > 0.0:\n",
    "                            iou = compute_IoU(box, gt_boxes[0])\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_box = box\n",
    "                    if best_iou > 0:\n",
    "                        total_iou += best_iou\n",
    "                        count += 1\n",
    "                        best_boxes.append(best_box.cpu())\n",
    "\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "\n",
    "    if avg_iou > best_avg_iou:\n",
    "        best_avg_iou = avg_iou\n",
    "        torch.save(model.state_dict(), \"best_frcnn_model.pth\")\n",
    "        print(f\"Saved best model with IoU: {best_avg_iou:.4f}\")\n",
    "\n",
    "    print(f\"Average IoU on eval set: {avg_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b143031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION TO LOAD THE MODEL\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09b2ba",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP FUNCTIONS WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_RCNN(file):\n",
    "    image = Image.open(file).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)[0]\n",
    "        if len(prediction['boxes']) == 0:\n",
    "            print(f\"No box found for image: {file}\")\n",
    "            return None\n",
    "        best_bb = prediction['boxes'][0].to(device).int()\n",
    "        cropped = img_tensor[0, :, best_bb[1]:best_bb[3], best_bb[0]:best_bb[2]]\n",
    "        cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "        return cropped_resized.squeeze(0)  \n",
    "\n",
    "def crop_folder_with_RCNN(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text=get_text(full_path)\n",
    "        cropped_image = crop_image_with_RCNN(full_path)\n",
    "        if cropped_image is not None:\n",
    "            cropped_folder.append([cropped_image, gt_text])\n",
    "    return cropped_folder\n",
    "\n",
    "# CROP FUNCTIONS WITH GROUND TRUTH BOUNDING BOX\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image)\n",
    "    cropped = img_tensor[:, int(bb[1]):int(bb[3]), int(bb[0]):int(bb[2])]\n",
    "    cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "    return cropped_resized.squeeze(0)\n",
    "\n",
    "def crop_folder_with_ground_truth(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text = get_text(file)  # Get ground truth text\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append([cropped_image, gt_text])  # Store image and text pair\n",
    "    return cropped_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE UNTRAINED MODEL USING THE BEST BOUNDING BOX (OUTPUTS THE IMAGE WITH THE PREDICTED BOUNDING BOX)\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(test_path_medium)\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(test_path_medium, image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE TRAINED MODEL USING THE BEST BOUNDING BOX (OUTPUTS THE IMAGE WITH THE PREDICTED BOUNDING BOX)\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "folder=os.listdir(test_path_medium)\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(test_path_medium, image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING AVERAGE INTESECTION OVER UNION ON TEST SET USING THE TRAINED MODEL\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\")\n",
    "total_iou=0\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        if len(output[0]['scores'])>0:\n",
    "            score = output[0]['scores'][0].item()        \n",
    "            best_bb=pred['boxes'][0].to(device)\n",
    "            gt_box=get_bounding_box(image_path)\n",
    "            tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "            iou=compute_IoU(best_bb, tensor_bbox)\n",
    "            total_iou+=iou\n",
    "print(f\"Average IoU: {total_iou/len(folder)}\") # 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE RESULTS OF THE TRAINED MODEL USING \"crop_image_with_RCNN\" (OUTPUTS THE CROPPED IMAGE AROUND THE PREDICTED BOUNDING BOX)\n",
    "\n",
    "for image in os.listdir(test_path_medium):\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(test_path_medium, image)\n",
    "        cropped = crop_image_with_RCNN(image_path)\n",
    "        plt.imshow(cropped.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Cropped Image\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE RESULTS OF THE TRAINED MODEL USING \"crop_folder_with_RCNN\" (OUTPUTS THE CROPPED IMAGE AROUND THE PREDICTED BOUNDING BOX FOR A WHOLE SET OF IMAGES)\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "eval_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval_small\"\n",
    "cropped_folder=crop_folder_with_RCNN(test_path_medium)\n",
    "image=cropped_folder[0][0]\n",
    "image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"ground truth text: {cropped_folder[0][1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
