{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc0a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DETECTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11695bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATASET STRUCTURE\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "test_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "train_folder=os.listdir(train_path)\n",
    "eval_folder=os.listdir(eval_path)\n",
    "test_folder=os.listdir(test_path)\n",
    "\n",
    "print(f\"Nmumber of training samples: {len(train_folder)}\")\n",
    "print(f\"Nmumber of evaluation samples: {len(eval_folder)}\")\n",
    "print(f\"Nmumber of testing samples: {len(test_folder)}\")\n",
    "print(f\"Total number of samples: {len(train_folder)+len(test_folder)+len(eval_folder)}\")\n",
    "\n",
    "# CHECKING ERRORS FOR FUNCTION: get_bounding_box\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "\n",
    "    # if len(values) != 31:                                                                   questo controllo non serve in quanto dipende da quante cifre ogni numero ha\n",
    "    #     raise ValueError(f\" 1 st exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v2=values.split(\"&\")\n",
    "\n",
    "    if len(values_v2) != 5:\n",
    "        raise ValueError(f\"2 nd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "\n",
    "    if len(values_v3) != 3:\n",
    "        raise ValueError(f\"3 rd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "\n",
    "    if len(x_coords) != 4:\n",
    "        raise ValueError(f\"4 th exception: Invalid plate text format in filename: {file}\")\n",
    "\n",
    " # CHECKING ERRORS FOR FUNCTION: get_text\n",
    "    \n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "        values=file.split(\"-\")\n",
    "\n",
    "        if len(values)!=7:\n",
    "            raise ValueError(f\"1 st exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        text=str(values[4])\n",
    "\n",
    "        # if len(text)!=7:                                                                                questo controllo non serve in quanto dipende da quante cifre ogni numero ha cifre diverse\n",
    "        #     raise ValueError(f\"2 nd exception occurred: Invalid plate text format in filename: {file}\") \n",
    "        \n",
    "        indices=text.split(\"_\")\n",
    "\n",
    "        if len(indices) != 8:\n",
    "            raise ValueError(f\"3 rd exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        province_character=provinces[int(indices[0])]\n",
    "        alphabet_character=alphabet[int(indices[1])]\n",
    "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "        if len(plate_text) != 8:\n",
    "            raise ValueError(f\"4 th exception occurred: Invalid plate text format in filename: {file}\")\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_bounding_box\n",
    "\n",
    "file=\"308069444444444444-91_94-8&424_589&564-578&564_27&553_8&431_589&424-0_0_3_24_32_25_32_25-100-241.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_text\n",
    "\n",
    "file=\"30237890625-87_88-197&472_449&567-449&553_201&567_197&472_448&477-10_2_3_24_31_33_26_24-192-75.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file2.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76f51ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUOVO DATASET CON PIU ELEMENTI DI TRAIN E MENO DI TEST\n",
    "\n",
    "path_train_og=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "path_test_og=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "path_train_tf=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/train\"\n",
    "path_test_tf=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\"\n",
    "folder_train_og=os.listdir(path_train_og)\n",
    "folder_test_og=os.listdir(path_test_og)\n",
    "folder_train_tf=os.listdir(path_train_tf)\n",
    "folder_test_tf=os.listdir(path_test_tf)\n",
    "print(f\"number of original training samples: {len(folder_train_og)}\")\n",
    "print(f\"number of original test samples: {len(folder_test_og)}\")\n",
    "print(f\"number of new training samples: {len(folder_train_tf)}\")\n",
    "print(f\"number of new training samples: {len(folder_test_tf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9344421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTERSECTION OVER UNION FUNCTION\n",
    "\n",
    "def compute_IoU(box1, box2):\n",
    "\n",
    "    box1=box1.squeeze()\n",
    "    box2=box2.squeeze()\n",
    "\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "\n",
    "    area_of_intersection = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3]- box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "\n",
    "    IoU = area_of_intersection / float(area_box1 + area_box2 - area_of_intersection)\n",
    "\n",
    "    return IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET   \n",
    "\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "smaller_train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train-20250508T142228Z-1-001/train\"\n",
    "train_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/train2\"\n",
    "eval_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/eval2\"\n",
    "test_path2=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\"\n",
    "\n",
    "class CCPD_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, transforms=None):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.folder = os.listdir(path)\n",
    "        self.images = [f for f in self.folder if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        full_path = os.path.join(self.path, file)\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "\n",
    "        bbox = get_bounding_box(file)\n",
    "        tensor_bbox = torch.tensor([bbox], dtype=torch.float32)\n",
    "        label = torch.tensor([1], dtype=torch.int64)  \n",
    "\n",
    "        target = {\"boxes\": tensor_bbox, \"labels\": label}\n",
    "\n",
    "        if target[\"boxes\"].shape[0] != target[\"labels\"].shape[0]:\n",
    "            raise ValueError(f\"Mismatch in number of boxes and labels for file: {file}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "train_dataset = CCPD_Dataset(train_path2, transforms=transform)\n",
    "eval_dataset = CCPD_Dataset(eval_path2, transforms=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0) # watch out for the batch size, it is set to 1 for compatibility with the model\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)  # watch out for the batch size, it is set to 1 for compatibility with the model\n",
    "\n",
    "# EXAMPLE OF THE OUTPUT:\n",
    "\n",
    "image_path_example=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train/04-90_267-158&448_542&553-541&553_162&551_158&448_542&450-0_1_3_24_27_33_30_24-99-116.jpg\"\n",
    "images, targets =next(iter(train_dataset))\n",
    "print(f\"images: {images}, targets: {targets}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torch.optim as optim\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "model.rpn.post_nms_top_n_train = 500\n",
    "model.rpn.post_nms_top_n_test = 500\n",
    "model.roi_heads.detections_per_img = 50  \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "best_boxes=[]\n",
    "best_avg_iou = 0.0\n",
    "num_epochs=1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    count=1\n",
    "    for images, targets in train_dataloader:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "        print(f\"Batch: {count} Training Loss: {losses.item():.4f}\")\n",
    "        count+=1\n",
    "\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in eval_dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes'].to(device)\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    best_iou = 0.0\n",
    "                    best_box = None\n",
    "                    for box, score in zip(pred['boxes'], pred['scores']):\n",
    "                        if score > 0.0:\n",
    "                            iou = compute_IoU(box, gt_boxes[0])\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_box = box\n",
    "                    if best_iou > 0:\n",
    "                        total_iou += best_iou\n",
    "                        count += 1\n",
    "                        best_boxes.append(best_box.cpu())\n",
    "\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "\n",
    "    if avg_iou > best_avg_iou:\n",
    "        best_avg_iou = avg_iou\n",
    "        torch.save(model.state_dict(), \"best_frcnn_model.pth\")\n",
    "        print(f\"Saved best model with IoU: {best_avg_iou:.4f}\")\n",
    "\n",
    "    print(f\"Average IoU on eval set: {avg_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b143031",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "model.load_state_dict(torch.load('best_frcnn_model.pth'))\n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\")\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()\n",
    "        print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE TRAINED MODEL USING THE BEST BOUNDING BOX\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "model.load_state_dict(torch.load('best_frcnn_model.pth'))\n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\")\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4459ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE UNTRAINED MODEL USING THE BEST BOUNDING BOX\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\")\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Prove/test2\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING AVERAGE IOU ON TEST SET USING THE TRAINED MODEL\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and image\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "model.load_state_dict(torch.load('best_frcnn_model.pth'))\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\")\n",
    "total_iou=0\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        if len(output[0]['scores'])>0:\n",
    "            score = output[0]['scores'][0].item()        \n",
    "            best_bb=pred['boxes'][0].to(device)\n",
    "            gt_box=get_bounding_box(image_path)\n",
    "            tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "            iou=compute_IoU(best_bb, tensor_bbox)\n",
    "            total_iou+=iou\n",
    "print(f\"Average IoU: {total_iou/len(folder)}\") # 0.58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP FUNCTION WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_RCNN(file):\n",
    "    \n",
    "    image = Image.open(file).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)[0]\n",
    "        best_bb=prediction['boxes'][0].to(device)\n",
    "        best_bb=best_bb.int()\n",
    "        cropped_image = img_tensor[0, :, best_bb[1]:best_bb[3], best_bb[0]:best_bb[2]]\n",
    "    return cropped_image\n",
    "\n",
    "def crop_folder_with_RCNN(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        cropped_image = crop_image_with_RCNN(full_path)\n",
    "        cropped_folder.append(cropped_image)\n",
    "    return cropped_folder\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    cropped_image = image.crop(bb)\n",
    "    return cropped_image\n",
    "\n",
    "def crop_folder_with_ground_truth(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append(cropped_image)\n",
    "    return cropped_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE RESULTS OF THE TRAINED MODEL\n",
    "\n",
    "for image in os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\"):\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data_transf/test\", image)\n",
    "        cropped = crop_image_with_RCNN(image_path)\n",
    "        plt.imshow(cropped.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Cropped Image\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcd811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a173180",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a8d82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a93459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# SETUP & IMPORTS\n",
    "# ------------------------\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.optim as optim\n",
    "\n",
    "# ------------------------\n",
    "# DATASET\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "class ResizeTransform:\n",
    "    def __call__(self, image):\n",
    "        return transforms.functional.resize(image, [512, 512])\n",
    "\n",
    "class CCPD_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, transforms=None):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.images = [f for f in os.listdir(path) if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        full_path = os.path.join(self.path, file)\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "        bbox = get_bounding_box(file)\n",
    "        tensor_bbox = torch.tensor([bbox], dtype=torch.float32)\n",
    "        label = torch.tensor([1], dtype=torch.int64)\n",
    "        target = {\"boxes\": tensor_bbox, \"labels\": label}\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# ------------------------\n",
    "# DATA LOADING\n",
    "# ------------------------\n",
    "\n",
    "train_path='/content/drive/My Drive/CV Project/CCPD2020/ccpd_green/train'\n",
    "eval_path='/content/drive/My Drive/CV Project/CCPD2020/ccpd_green/val'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    ResizeTransform(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CCPD_Dataset(train_path, transforms=transform)\n",
    "eval_dataset = CCPD_Dataset(eval_path, transforms=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "# eval_dataloader = DataLoader(eval_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# ------------------------\n",
    "# MODEL\n",
    "# ------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# Custom head\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "\n",
    "# Reduce number of proposals\n",
    "model.rpn.post_nms_top_n_train = 100\n",
    "model.rpn.post_nms_top_n_test = 100\n",
    "model.roi_heads.detections_per_img = 50\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer & mixed precision\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ------------------------\n",
    "# TRAINING LOOP\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "best_avg_iou = 0.0\n",
    "best_boxes = []\n",
    "\n",
    "for epoch in range(1):\n",
    "    print(f\"\\nEpoch {epoch+1}\")\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, targets) in enumerate(train_dataloader):\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        with autocast():\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        scaler.scale(losses).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        train_loss += losses.item()\n",
    "        print(f\"Batch {i+1} - Loss: {losses.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1} Total Training Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # ------------------------\n",
    "    # EVALUATION\n",
    "    # ------------------------\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in eval_dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes'].to(device)\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    best_iou = 0.0\n",
    "                    best_box = None\n",
    "                    for box, score in zip(pred['boxes'], pred['scores']):\n",
    "                        if score > 0.0:\n",
    "                            iou = compute_IoU(box, gt_boxes[0])\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_box = box\n",
    "                    if best_iou > 0:\n",
    "                        total_iou += best_iou\n",
    "                        count += 1\n",
    "                        best_boxes.append(best_box.cpu())\n",
    "\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "    print(f\"Average IoU: {avg_iou:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if avg_iou > best_avg_iou:\n",
    "        best_avg_iou = avg_iou\n",
    "        torch.save(model.state_dict(), \"/content/drive/My Drive/best_frcnn_model.pth\")\n",
    "        print(f\"Saved best model with IoU: {best_avg_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d6054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_detection(image_path, boxes, scores):\n",
    "    \"\"\"\n",
    "    Visualize detected license plates on the image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the original image\n",
    "        boxes: Detected bounding boxes\n",
    "        scores: Confidence scores for each detection\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.patches as patches\n",
    "    \n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    # Add bounding boxes\n",
    "    for box, score in zip(boxes, scores):\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), x2-x1, y2-y1,\n",
    "            linewidth=2, edgecolor='r', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1-10, f'Conf: {score:.2f}', color='red')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3298b4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7d90da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from utils import detect_license_plate, visualize_detection\n",
    "\n",
    "# 1. Load the trained model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  # 2 classes: background and license plate\n",
    "model.load_state_dict(torch.load('best_frcnn_model.pth'))\n",
    "model.to(device)\n",
    "\n",
    "# 2. Run inference on an image\n",
    "image_path = 'path/to/your/image.jpg'\n",
    "boxes, scores = detect_license_plate(model, image_path, device=device, confidence_threshold=0.5)\n",
    "\n",
    "# 3. Visualize the results\n",
    "visualize_detection(image_path, boxes, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b66d3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "model.eval()\n",
    "\n",
    "img_path = \"/content/drive/My Drive/CV Project/CCPD2020/ccpd_green/train_small/Copia di 015-90_260-228&437_444&507-444&507_238&502_228&439_435&437-0_0_3_24_28_26_28_32-111-64.jpg\"\n",
    "image = Image.open(img_path).convert(\"RGB\")\n",
    "transform = T.ToTensor()\n",
    "img_tensor = transform(image).unsqueeze(0).to(device)\n",
    "with torch.no_grad():\n",
    "    prediction = model(img_tensor)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(image)  \n",
    "ax = plt.gca()\n",
    "if len(best_boxes) > 1:\n",
    "    x1, y1, x2, y2 = best_boxes[1]\n",
    "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                         fill=False, edgecolor='red', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "plt.title(\"Predicted Car Plate Bounding Box\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RECOGNITION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315abd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66008c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION = \"with_ground_truth\"  \n",
    "\n",
    "match FUNCTION:\n",
    "    case \"with_ground_truth\":\n",
    "        selected_function = crop_folder_with_ground_truth\n",
    "    case \"with_RCNN\":\n",
    "        selected_function = crop_folder_with_RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2f3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "\n",
    "all_characters = sorted(set(provinces + alphabet + ads))\n",
    "char2idx = {char: idx + 1 for idx, char in enumerate(all_characters)}  \n",
    "\n",
    "class CCPD_dataset_recognition(Dataset):\n",
    "    \n",
    "    def __init__(self, path, char2idx, transformations):\n",
    "\n",
    "        self.path = path\n",
    "        self.char2idx = char2idx\n",
    "        self.transformations = transformations\n",
    "        self.images = [f for f in os.listdir(path) if f.endswith(\"jpg\")]\n",
    "        self.cropped_images = selected_function(self.path)  \n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        file = self.images[idx]\n",
    "        image = self.cropped_images[idx]\n",
    "        gt_text = get_text(file)\n",
    "        gt_text_encoded = torch.tensor([self.char2idx[c] for c in gt_text], dtype=torch.long)\n",
    "        if self.transformations:\n",
    "            image = self.transformations(image)\n",
    "        return image, gt_text_encoded\n",
    "    \n",
    "def crnn_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    image_batch = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "    labels_concat = torch.cat(labels)\n",
    "    return image_batch, labels_concat, label_lengths\n",
    "\n",
    "transformations=T.Compose([T.Grayscale(num_output_channels=1),T.Resize((64,256)),T.ToTensor()])\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "\n",
    "train_recognition_dataset=CCPD_dataset_recognition(train_path,char2idx,transformations=transformations)\n",
    "eval_recognition_dataset=CCPD_dataset_recognition(eval_path,char2idx,transformations=transformations)\n",
    "\n",
    "train_dataloader=DataLoader(train_recognition_dataset,batch_size=16, shuffle=True, collate_fn=crnn_collate_fn)\n",
    "eval_dataloader=DataLoader(eval_recognition_dataset,batch_size=8, shuffle=False, collate_fn=crnn_collate_fn)\n",
    "\n",
    "batch=next(iter(train_dataloader))\n",
    "batch\n",
    "# returns a batch containing the images as tensors and the ground truth of the text once treated with char2idx (all stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6d533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2, 1)),\n",
    "            nn.AdaptiveAvgPool2d((2, None)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(1024, 256, bidirectional=True, num_layers=2,dropout=0.5, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_classes + 1)  # +1 for CTC blank\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.cnn(x)                          # [B, 512, 2, W]\n",
    "        b, c, h, w = x.size()\n",
    "        assert h == 2, f\"Expected height to be 2 after CNN, got {h}\"\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()   # [B, W, C, H]\n",
    "        x = x.view(b, w, c * h)                  # [B, W, 1024]\n",
    "        x, _ = self.rnn(x)                       # [B, W, 512]\n",
    "        x = self.fc(x)                           # [B, W, num_classes + 1]\n",
    "        return x.permute(1, 0, 2)                # [T, B, C] for CTC\n",
    "    \n",
    "recognition_model=CRNN(67)\n",
    "recognition_model(batch[0]).shape\n",
    "# returns a tensor of size 64X16X68 --> this output has to be decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ec92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND EVALUATION LOOP\n",
    "\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = recognition_model.to(device)\n",
    "\n",
    "loss_fn = CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(recognition_model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch + 1}\\n-------')\n",
    "    recognition_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch, (images, targets, target_lengths) in enumerate(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        target_lengths = target_lengths.to(device)\n",
    "        preds = recognition_model(images)  # preds: [T, B, C]\n",
    "        T, B, C = preds.size()\n",
    "        input_lengths = torch.full(size=(B,), fill_value=T, dtype=torch.long)  # Each sample has T time steps\n",
    "        log_probs = preds.log_softmax(2)  # dim=2 is for classes\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(f'Batch {batch}: Loss = {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = train_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {avg_loss:.4f}')\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, targets, target_lengths in eval_dataloader:\n",
    "        preds = model(images)\n",
    "        log_probs = preds.log_softmax(2)\n",
    "        T, B, C = preds.size()\n",
    "        input_lengths = torch.full(size=(B,), fill_value=T, dtype=torch.long)\n",
    "        loss = loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(eval_dataloader)\n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), \"best_crnn_ctc_model.pth\")\n",
    "\n",
    "    print(f'Evaluation loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODING RESULTS\n",
    "\n",
    "idx2char = {v: k for k, v in char2idx.items()}\n",
    "\n",
    "def greedy_decode(ctc_output, idx2char):\n",
    "    # ctc_output: [T, B, C] (output from the model)\n",
    "    preds = ctc_output.permute(1, 0, 2)  # [B, T, C]\n",
    "    pred_strings = []\n",
    "    for pred in preds:\n",
    "        best_path = torch.argmax(pred, dim=1).tolist()  # get index with max prob at each timestep\n",
    "        prev = -1\n",
    "        decoded = []\n",
    "        for p in best_path:\n",
    "            if p != prev and p != 0:  # 0 is the CTC blank\n",
    "                decoded.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(\"\".join(decoded))\n",
    "    return pred_strings\n",
    "\n",
    "recognition_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, targets, target_lengths in eval_dataloader:\n",
    "        outputs = recognition_model(images)  # [T, B, C]\n",
    "        predictions = greedy_decode(outputs, idx2char)\n",
    "\n",
    "        # Compare to original labels (you need original ground truth strings)\n",
    "        # This depends on how your DataLoader is structured\n",
    "\n",
    "        # Example (if you stored gt_texts during dataset loading):\n",
    "        for pred, true in zip(predictions, gt_texts):\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
