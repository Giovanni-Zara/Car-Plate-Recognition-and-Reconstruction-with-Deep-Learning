{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66a3593",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f290448e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68e0bc",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15ea5994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS FOR THE TEXT OF THE CAR PLATE\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "# FULL DATASETS\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "eval_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "test_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test\"\n",
    "\n",
    "train_folder=os.listdir(train_path) \n",
    "eval_folder=os.listdir(eval_path_medium)\n",
    "test_folder=os.listdir(test_path_medium)\n",
    "\n",
    "# MEDIUM DATASETS\n",
    "\n",
    "train_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train_medium\"\n",
    "eval_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval_medium\"\n",
    "test_path_medium=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1b18dc",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a549a55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOUNDING BOX FUNCTION \n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]\n",
    "\n",
    "# INTERSECTION OVER UNION FUNCTION\n",
    "\n",
    "def compute_IoU(box1, box2):\n",
    "\n",
    "    box1=box1.squeeze()\n",
    "    box2=box2.squeeze()\n",
    "    xA = max(box1[0], box2[0])\n",
    "    yA = max(box1[1], box2[1])\n",
    "    xB = min(box1[2], box2[2])\n",
    "    yB = min(box1[3], box2[3])\n",
    "    area_of_intersection = max(0, xB - xA) * max(0, yB - yA)\n",
    "    area_box1 = (box1[2] - box1[0]) * (box1[3]- box1[1])\n",
    "    area_box2 = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    IoU = area_of_intersection / float(area_box1 + area_box2 - area_of_intersection)\n",
    "\n",
    "    return IoU\n",
    "\n",
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    \n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d38499",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11695bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATASET STRUCTURE\n",
    "\n",
    "print(f\"Nmumber of training samples: {len(train_folder)}\")\n",
    "print(f\"Nmumber of evaluation samples: {len(eval_folder)}\")\n",
    "print(f\"Nmumber of testing samples: {len(test_folder)}\")\n",
    "print(f\"Total number of samples: {len(train_folder)+len(test_folder)+len(eval_folder)}\")\n",
    "\n",
    "# CHECKING ERRORS FOR FUNCTION: \"get_bounding_box\"\n",
    "\n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "\n",
    "    # if len(values) != 31:                                                                   questo controllo non serve in quanto dipende da quante cifre ogni numero ha\n",
    "    #     raise ValueError(f\" 1 st exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v2=values.split(\"&\")\n",
    "\n",
    "    if len(values_v2) != 5:\n",
    "        raise ValueError(f\"2 nd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "\n",
    "    if len(values_v3) != 3:\n",
    "        raise ValueError(f\"3 rd exception: Invalid plate text format in filename: {file}\")\n",
    "    \n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "\n",
    "    if len(x_coords) != 4:\n",
    "        raise ValueError(f\"4 th exception: Invalid plate text format in filename: {file}\")\n",
    "\n",
    " # CHECKING ERRORS FOR FUNCTION: \"get_text\"\n",
    " \n",
    "for file in train_folder: # executed also for eval and test set with no Error raised\n",
    "        values=file.split(\"-\")\n",
    "\n",
    "        if len(values)!=7:\n",
    "            raise ValueError(f\"1 st exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        text=str(values[4])\n",
    "\n",
    "        # if len(text)!=7:                                                                                questo controllo non serve in quanto dipende da quante cifre ogni numero ha cifre diverse\n",
    "        #     raise ValueError(f\"2 nd exception occurred: Invalid plate text format in filename: {file}\") \n",
    "        \n",
    "        indices=text.split(\"_\")\n",
    "\n",
    "        if len(indices) != 8:\n",
    "            raise ValueError(f\"3 rd exception occurred: Invalid plate text format in filename: {file}\")\n",
    "        \n",
    "        province_character=provinces[int(indices[0])]\n",
    "        alphabet_character=alphabet[int(indices[1])]\n",
    "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "        if len(plate_text) != 8:\n",
    "            raise ValueError(f\"4 th exception occurred: Invalid plate text format in filename: {file}\")\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: \"get_bounding_box\"\n",
    "\n",
    "file=\"308069444444444444-91_94-8&424_589&564-578&564_27&553_8&431_589&424-0_0_3_24_32_25_32_25-100-241.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI VALUES\n",
    "numbers=file.split(\"-\")\n",
    "values=numbers[3]\n",
    "values_v2=values.split(\"&\")\n",
    "values_v3=[]\n",
    "for i in range(len(values_v2)):\n",
    "    if \"_\" in values_v2[i]:\n",
    "        values_v3.append(values_v2[i].split(\"_\"))\n",
    "t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "x_min = min(x_coords)\n",
    "y_min = min(y_coords)\n",
    "x_max = max(x_coords)\n",
    "y_max = max(y_coords)\n",
    "\n",
    "# ESEMPIO DI PROVA CON DUE FILE PER FUNZIONE: get_text\n",
    "\n",
    "file=\"30237890625-87_88-197&472_449&567-449&553_201&567_197&472_448&477-10_2_3_24_31_33_26_24-192-75.jpg\" # FILE CHE NON DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)\n",
    "\n",
    "file2=\"0115711805556-88_269-312&388_484&456-484&445_314&456_312&396_481&388-0_0_3_24_28_28_28_28-154-41.jpg\" # FILE CHE DA PROBLEMI CON LA LUNGHEZZA DI TEXT\n",
    "values=file2.split(\"-\")\n",
    "text=str(values[4])\n",
    "indices=text.split(\"_\")\n",
    "if len(indices) != 8:\n",
    "    raise ValueError(f\"Invalid plate text format in filename: {file}\")\n",
    "province_character=provinces[int(indices[0])]\n",
    "alphabet_character=alphabet[int(indices[1])]\n",
    "ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "print(values,text,len(text),indices,plate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "377dc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET  CLASS  \n",
    "\n",
    "class CCPD_Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, transforms=None):\n",
    "        self.path = path\n",
    "        self.transforms = transforms\n",
    "        self.folder = os.listdir(path)\n",
    "        self.images = [f for f in self.folder if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.images[idx]\n",
    "        full_path = os.path.join(self.path, file)\n",
    "        image = Image.open(full_path).convert(\"RGB\")\n",
    "\n",
    "        bbox = get_bounding_box(file)\n",
    "        tensor_bbox = torch.tensor([bbox], dtype=torch.float32)\n",
    "        label = torch.tensor([1], dtype=torch.int64)  \n",
    "\n",
    "        target = {\"boxes\": tensor_bbox, \"labels\": label}\n",
    "\n",
    "        if target[\"boxes\"].shape[0] != target[\"labels\"].shape[0]:\n",
    "            raise ValueError(f\"Mismatch in number of boxes and labels for file: {file}\")\n",
    "\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, target\n",
    "    \n",
    "# COLLATE FUNCTION\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a45ba39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "transform = T.ToTensor()\n",
    "train_dataset_medium = CCPD_Dataset(train_path_medium, transforms=transform)\n",
    "eval_dataset_medium = CCPD_Dataset(eval_path_medium, transforms=transform)\n",
    "test_dataset_medium = CCPD_Dataset(test_path_medium, transforms=transform)\n",
    "\n",
    "train_dataloader_medium = DataLoader(train_dataset_medium, batch_size=32, shuffle=True, collate_fn=collate_fn, num_workers=0) \n",
    "eval_dataloader_medium = DataLoader(eval_dataset_medium, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)  \n",
    "test_dataloader_medium = DataLoader(test_dataset_medium, batch_size=16, shuffle=False, collate_fn=collate_fn, num_workers=0)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbd0ca",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c2e4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/filippo/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/filippo/miniconda3/envs/virtualenv/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes=2)\n",
    "model.rpn.post_nms_top_n_train = 500\n",
    "model.rpn.post_nms_top_n_test = 500\n",
    "model.roi_heads.detections_per_img = 50  \n",
    "device=\"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9def8649",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512bb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "best_boxes=[]\n",
    "best_avg_iou = 0.0\n",
    "num_epochs=1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    count=1\n",
    "    for images, targets in train_dataloader_medium:\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "        print(f\"Batch: {count} Training Loss: {losses.item():.4f}\")\n",
    "        count+=1\n",
    "\n",
    "    model.eval()\n",
    "    total_iou = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in eval_dataloader_medium:\n",
    "            images = [img.to(device) for img in images]\n",
    "            outputs = model(images)\n",
    "\n",
    "            for pred, target in zip(outputs, targets):\n",
    "                pred_boxes = pred['boxes'].to(device)\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "\n",
    "                if len(pred_boxes) > 0:\n",
    "                    best_iou = 0.0\n",
    "                    best_box = None\n",
    "                    for box, score in zip(pred['boxes'], pred['scores']):\n",
    "                        if score > 0.0:\n",
    "                            iou = compute_IoU(box, gt_boxes[0])\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_box = box\n",
    "                    if best_iou > 0:\n",
    "                        total_iou += best_iou\n",
    "                        count += 1\n",
    "                        best_boxes.append(best_box.cpu())\n",
    "\n",
    "    avg_iou = total_iou / count if count > 0 else 0\n",
    "\n",
    "    if avg_iou > best_avg_iou:\n",
    "        best_avg_iou = avg_iou\n",
    "        torch.save(model.state_dict(), \"best_frcnn_model.pth\")\n",
    "        print(f\"Saved best model with IoU: {best_avg_iou:.4f}\")\n",
    "\n",
    "    print(f\"Average IoU on eval set: {avg_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b143031",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7573/3135391269.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION TO LOAD THE MODEL\n",
    "\n",
    "def load_Fasterrcnn(device):\n",
    "    model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "    model.load_state_dict(torch.load('model_weights/best_frcnn_model_final_version.pth', map_location=\"cpu\"))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec09b2ba",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec1621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROP FUNCTIONS WITH PREDICTED BOUNDING BOX\n",
    "\n",
    "def crop_image_with_RCNN(file):\n",
    "    image = Image.open(file).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image).unsqueeze(0).to(device) \n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)[0]\n",
    "        if len(prediction['boxes']) == 0:\n",
    "            print(f\"No box found for image: {file}\")\n",
    "            return None\n",
    "        best_bb = prediction['boxes'][0].to(device).int()\n",
    "        cropped = img_tensor[0, :, best_bb[1]:best_bb[3], best_bb[0]:best_bb[2]]\n",
    "        cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "        return cropped_resized.squeeze(0)  \n",
    "\n",
    "def crop_folder_with_RCNN(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text=get_text(full_path)\n",
    "        cropped_image = crop_image_with_RCNN(full_path)\n",
    "        if cropped_image is not None:\n",
    "            cropped_folder.append([cropped_image, gt_text])\n",
    "    return cropped_folder\n",
    "\n",
    "# CROP FUNCTIONS WITH GROUND TRUTH BOUNDING BOX\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  \n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    img_tensor = transform(image)\n",
    "    cropped = img_tensor[:, int(bb[1]):int(bb[3]), int(bb[0]):int(bb[2])]\n",
    "    cropped_resized = F.interpolate(cropped.unsqueeze(0), size=(48, 144), mode='bilinear', align_corners=False)\n",
    "    return cropped_resized.squeeze(0)\n",
    "\n",
    "def crop_folder_with_ground_truth(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        gt_text = get_text(file)  # Get ground truth text\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append([cropped_image, gt_text])  # Store image and text pair\n",
    "    return cropped_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c99a85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE UNTRAINED MODEL USING THE BEST BOUNDING BOX (OUTPUTS THE IMAGE WITH THE PREDICTED BOUNDING BOX)\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(num_classes=2)  \n",
    "device=\"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "folder=os.listdir(test_path_medium)\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(test_path_medium, image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE TRAINED MODEL USING THE BEST BOUNDING BOX (OUTPUTS THE IMAGE WITH THE PREDICTED BOUNDING BOX)\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "folder=os.listdir(test_path_medium)\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(test_path_medium, image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        score = output[0]['scores'][0].item()        \n",
    "        best_bb=pred['boxes'][0].cpu()\n",
    "        gt_box=get_bounding_box(image_path)\n",
    "        tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "        iou=compute_IoU(best_bb, tensor_bbox)\n",
    "        plt.imshow(image)\n",
    "        ax = plt.gca()\n",
    "        x1, y1, x2, y2 = best_bb\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1, f\" score: {score:.2f}, IoU: {iou:.2f}\", color='white', bbox=dict(facecolor='red', alpha=0.5))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1281f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTING AVERAGE INTESECTION OVER UNION ON TEST SET USING THE TRAINED MODEL\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "folder=os.listdir(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\")\n",
    "total_iou=0\n",
    "for image in folder:\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/test_medium\", image)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "    transform = T.ToTensor()\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        pred = output[0]\n",
    "        if len(output[0]['scores'])>0:\n",
    "            score = output[0]['scores'][0].item()        \n",
    "            best_bb=pred['boxes'][0].to(device)\n",
    "            gt_box=get_bounding_box(image_path)\n",
    "            tensor_bbox = torch.tensor([gt_box], dtype=torch.float32)\n",
    "            iou=compute_IoU(best_bb, tensor_bbox)\n",
    "            total_iou+=iou\n",
    "print(f\"Average IoU: {total_iou/len(folder)}\") # 0.87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d38589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE RESULTS OF THE TRAINED MODEL USING \"crop_image_with_RCNN\" (OUTPUTS THE CROPPED IMAGE AROUND THE PREDICTED BOUNDING BOX)\n",
    "\n",
    "for image in os.listdir(test_path_medium):\n",
    "    if image.endswith(\".jpg\"):\n",
    "        image_path = os.path.join(test_path_medium, image)\n",
    "        cropped = crop_image_with_RCNN(image_path)\n",
    "        plt.imshow(cropped.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Cropped Image\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcd811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOWING THE RESULTS OF THE TRAINED MODEL USING \"crop_folder_with_RCNN\" (OUTPUTS THE CROPPED IMAGE AROUND THE PREDICTED BOUNDING BOX FOR A WHOLE SET OF IMAGES)\n",
    "\n",
    "model=load_Fasterrcnn(\"cpu\")\n",
    "cropped_folder=crop_folder_with_RCNN(test_path_medium)\n",
    "image=cropped_folder[0][0]\n",
    "image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "plt.imshow(image_np)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "print(f\"ground truth text: {cropped_folder[0][1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
