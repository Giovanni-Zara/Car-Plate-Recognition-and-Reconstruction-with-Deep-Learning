{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HAN26sOdF-L_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAN26sOdF-L_",
        "outputId": "f8466499-fcbf-427c-a99f-693ebd245a35"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nKjooigyJFeQ",
      "metadata": {
        "id": "nKjooigyJFeQ"
      },
      "source": [
        "# COMPLETE PAPER PIPELINE **YOLOV5 + PDLPR**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nUeVsx3sJRZh",
      "metadata": {
        "id": "nUeVsx3sJRZh"
      },
      "source": [
        "#YOLOV5 INFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "icMPePYSGBkf",
      "metadata": {
        "id": "icMPePYSGBkf"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/drive/MyDrive/ccpd_green /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VLfaezaiJT7_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLfaezaiJT7_",
        "outputId": "eb0027bc-4152-4c2c-a954-828a62cfc1d1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iWgqV7ywJXO-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWgqV7ywJXO-",
        "outputId": "3e76597c-be6b-4ae4-cae2-03bccc46e1cf"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/yolov5/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EnPb9vc6Jcml",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnPb9vc6Jcml",
        "outputId": "cc4407f7-ec07-4882-e86b-0bd6c80c46b2"
      },
      "outputs": [],
      "source": [
        "!python yolov5/detect.py --weights yolo_finetuned_model.pt --source \"/content/ccpd_green/train\" --data inference_data.yaml --save-txt --save-crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U09E8cdZJowr",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U09E8cdZJowr",
        "outputId": "e93b178d-dc47-4ca3-f2f4-f11eb6aa0799"
      },
      "outputs": [],
      "source": [
        "!python yolov5/detect.py --weights yolo_finetuned_model.pt --source \"/content/ccpd_green/val\" --data inference_data.yaml --save-txt --save-crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BhzGPMtGJpCt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhzGPMtGJpCt",
        "outputId": "f7c23bad-257c-49a5-9e3b-579672b3a7bf"
      },
      "outputs": [],
      "source": [
        "!python yolov5/detect.py --weights yolo_finetuned_model.pt --source \"/content/ccpd_green/test\" --data inference_data.yaml --save-txt --save-crop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ian4wiKUJPPV",
      "metadata": {
        "id": "Ian4wiKUJPPV"
      },
      "source": [
        "# PDLPR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10ce997",
      "metadata": {
        "id": "a10ce997"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02304482",
      "metadata": {
        "id": "02304482"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#import cv2\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "187bea42",
      "metadata": {
        "id": "187bea42"
      },
      "source": [
        "# global"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7152fe86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7152fe86",
        "outputId": "bfd77c3c-1501-4f6f-91c5-8a789b03e915"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 256)), # Aspect ratio più realistico per targhe\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "#saving fields of the licence plate as global variables, i'm gonna use them later on\n",
        "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
        "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
        "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
        "\n",
        "# dict\n",
        "seen = set()    #avoid duplicates\n",
        "MY_DICTIONARY = []\n",
        "for char_list in [provinces, alphabets, ads]:\n",
        "    for char in char_list:\n",
        "        if char not in seen:\n",
        "            MY_DICTIONARY.append(char)\n",
        "            seen.add(char)\n",
        "\n",
        "print(f\"dictionary has {len(MY_DICTIONARY)} unique chars: {MY_DICTIONARY}\")\n",
        "\n",
        "# Create character to index and index to character mappings\n",
        "char2idx = {c: i for i, c in enumerate(MY_DICTIONARY)}\n",
        "idx2char = {i: c for i, c in enumerate(MY_DICTIONARY)}\n",
        "BLANK_IDX = len(MY_DICTIONARY)  # CTC needs +1 for \"blank\" , so keep the len\n",
        "\n",
        "print(f\"so i got (+1 for blank): {BLANK_IDX + 1} classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f8ae945",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f8ae945",
        "outputId": "f56e5e40-d751-46ad-970e-db89dbe3c616"
      },
      "outputs": [],
      "source": [
        "# sanity check of dictionary, it is really important dictionary is ok for the parsing\n",
        "def validate_dictionary():\n",
        "    print(\"=== validating dict ===\")\n",
        "    print(f\"Provinces: {len(provinces)} chars\")\n",
        "    print(f\"Alphabets: {len(alphabets)} chars\")\n",
        "    print(f\"Ads: {len(ads)} chars\")\n",
        "    print(f\"Final dict: {len(MY_DICTIONARY)} unique chars\")\n",
        "\n",
        "    # Verifica che tutti i caratteri siano mappabili\n",
        "    all_chars = set(provinces + alphabets + ads)\n",
        "    dict_chars = set(MY_DICTIONARY)\n",
        "\n",
        "    if all_chars == dict_chars:\n",
        "        print(\"all chars are included in the dict\")\n",
        "    else:\n",
        "        missing = all_chars - dict_chars\n",
        "        extra = dict_chars - all_chars\n",
        "        if missing:\n",
        "            print(f\"missing chars: {missing}\")\n",
        "        if extra:\n",
        "            print(f\"there's extra chars!: {extra}\")\n",
        "\n",
        "    # example file\n",
        "    test_filename = \"025-95_113-154&383_386&473-386&473_177&454_154&383_363&402-0_0_22_27_27_33_16-37-15.jpg\"\n",
        "    print(f\"\\n=== Test Parsing ===\")\n",
        "    print(f\"Test filename: {test_filename}\")\n",
        "\n",
        "    # parsing plate (method from the class)\n",
        "    fields = test_filename.split('-')\n",
        "    indices = fields[4].split(\"_\")\n",
        "    test_plate = provinces[int(indices[0])] + alphabets[int(indices[1])] + \"\".join([ads[int(i)] for i in indices[2:]])\n",
        "    print(f\"parsed plate: '{test_plate}'\")\n",
        "\n",
        "    # Verify i have all the chars in the dict\n",
        "    missing_chars = [c for c in test_plate if c not in char2idx]\n",
        "    if missing_chars:\n",
        "        print(f\"missing chars in dict: {missing_chars}\")\n",
        "    else:\n",
        "        print(\"the dictionary is ok!\")\n",
        "\n",
        "validate_dictionary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d075bc2",
      "metadata": {
        "id": "1d075bc2"
      },
      "source": [
        "# model architecture and utils"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d9d630",
      "metadata": {
        "id": "e6d9d630"
      },
      "source": [
        "## model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e69c062",
      "metadata": {
        "id": "5e69c062"
      },
      "outputs": [],
      "source": [
        "# PDLPR MODEL FOLLOWING PAPER ARCHITECTURE\n",
        "\n",
        "# --- Focus Structure Module ---\n",
        "class Focus(nn.Module):\n",
        "    def __init__(self, in_channels=1, out_channels=64, dropout=0.1):  # Fixed __init__\n",
        "        super(Focus, self).__init__()  # Fixed __init__\n",
        "        self.conv = nn.Conv2d(in_channels * 4, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.act = nn.LeakyReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Slice and concat - Focus structure downsampling\n",
        "        patch1 = x[..., ::2, ::2]\n",
        "        patch2 = x[..., ::2, 1::2]\n",
        "        patch3 = x[..., 1::2, ::2]\n",
        "        patch4 = x[..., 1::2, 1::2]\n",
        "        x = torch.cat([patch1, patch2, patch3, patch4], dim=1)\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- CNN Block used in RESBLOCK and downsampling ---\n",
        "def conv_block(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.LeakyReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "# --- Residual Block ---\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, channels):  # Fixed __init__\n",
        "        super(ResBlock, self).__init__()  # Fixed __init__\n",
        "        self.block = nn.Sequential(\n",
        "            conv_block(channels, channels),\n",
        "            conv_block(channels, channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "\n",
        "# --- IGFE Module ---\n",
        "class IGFE(nn.Module):\n",
        "    def __init__(self, dropout=0.1):  # Fixed __init__\n",
        "        super(IGFE, self).__init__()  # Fixed __init__\n",
        "        self.focus = Focus(1, 64, dropout)  # Changed to 1 channel for grayscale\n",
        "        self.down1 = conv_block(64, 128, stride=2)\n",
        "        self.res1 = ResBlock(128)\n",
        "        self.res2 = ResBlock(128)\n",
        "        self.down2 = conv_block(128, 256, stride=2)\n",
        "        self.res3 = ResBlock(256)\n",
        "        self.res4 = ResBlock(256)\n",
        "        self.final_conv = nn.Conv2d(256, 512, kernel_size=1)\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 18))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.focus(x)\n",
        "        x = self.down1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.res1(x)\n",
        "        x = self.res2(x)\n",
        "        x = self.down2(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.res3(x)\n",
        "        x = self.res4(x)\n",
        "        x = self.final_conv(x)\n",
        "        x = self.adaptive_pool(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# --- Positional Encoding for 2D feature maps ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, height, width):  # Fixed __init__\n",
        "        super().__init__()  # Fixed __init__\n",
        "        self.pe = nn.Parameter(torch.randn(1, d_model, height, width))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe\n",
        "\n",
        "\n",
        "# --- Transformer Encoder Block ---\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, d_model):  # Fixed __init__\n",
        "        super(EncoderBlock, self).__init__()  # Fixed __init__\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, num_heads=8, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model * 4, d_model)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, HW, C]\n",
        "\n",
        "        # Self-attention\n",
        "        attn_out, _ = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + self.dropout(attn_out))\n",
        "\n",
        "        # FFN\n",
        "        ffn_out = self.ffn(x)\n",
        "        x = self.norm2(x + self.dropout(ffn_out))\n",
        "\n",
        "        return x.transpose(1, 2).view(B, C, H, W)\n",
        "\n",
        "\n",
        "# --- PDLPR Recognition Model (CTC-based) ---\n",
        "class PDLPR(nn.Module):\n",
        "    def __init__(self, num_classes, dropout=0.1):  # Fixed __init__\n",
        "        super(PDLPR, self).__init__()  # Fixed __init__\n",
        "        self.igfe = IGFE(dropout)\n",
        "        self.pos_encoding = PositionalEncoding(512, 6, 18)\n",
        "        self.encoder = nn.Sequential(*[EncoderBlock(512) for _ in range(3)])\n",
        "\n",
        "        # CTC head - outputs sequence of characters\n",
        "        # After flattening, we have [B, W, C*H] = [B, 18, 512*6] = [B, 18, 3072]\n",
        "        self.ctc_head = nn.Sequential(\n",
        "            nn.Linear(512 * 6, 256),  # Fixed dimension: 512*6 = 3072\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feature extraction\n",
        "        x = self.igfe(x)  # [B, 512, 6, 18]\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.encoder(x)  # [B, 512, 6, 18]\n",
        "\n",
        "        # Flatten to sequence for CTC\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.permute(0, 3, 1, 2)  # [B, W, C, H]\n",
        "        x = x.contiguous().view(B, W, -1)  # [B, W, C*H] = [B, 18, 3072]\n",
        "\n",
        "        # Linear projection to reduce dimension\n",
        "        x = self.ctc_head(x)  # [B, W, num_classes]\n",
        "        x = x.permute(1, 0, 2)  # [W, B, num_classes] for CTC\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb257e2",
      "metadata": {
        "id": "4bb257e2"
      },
      "source": [
        "## utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8999ef11",
      "metadata": {
        "id": "8999ef11"
      },
      "outputs": [],
      "source": [
        "def ctc_collate_fn(batch):\n",
        "    '''\n",
        "    basically what I do here is stacking all the images in a batch into a single tensor and\n",
        "    then computing the len of each label (assuming different lenght plate can happen). (I could actually avoid this but it's more general)\n",
        "    Finally just concatenating all the labels into a vector (pytorch CTC wantres them in a line, not list)\n",
        "    then returning image-label-its lenght.\n",
        "    I need this to tell CTC where labels finish and i do not care padding as CTC deals with that internally (NICE)\n",
        "    '''\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images)\n",
        "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
        "    labels = torch.cat(labels)\n",
        "    return images, labels, label_lengths\n",
        "\n",
        "\n",
        "'''def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels, label_lengths in dataloader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        label_lengths = label_lengths.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # [W, B, num_classes]\n",
        "        log_probs = outputs.log_softmax(2)\n",
        "        input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long).to(device)\n",
        "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    print(f\"Epoch average loss: {avg_loss:.4f}\")\n",
        "    return avg_loss'''\n",
        "\n",
        "\n",
        "\n",
        "def get_plate(filename):\n",
        "    fields = filename.split('-')\n",
        "\n",
        "    text=str(fields[4])\n",
        "    indices=text.split(\"_\")\n",
        "    province_character=provinces[int(indices[0])]\n",
        "    alphabet_character=alphabets[int(indices[1])]\n",
        "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
        "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
        "\n",
        "    return plate_text\n",
        "\n",
        "\n",
        "\n",
        "def ctc_greedy_decoder(output, idx2char, blank=BLANK_IDX):\n",
        "    '''\n",
        "    Now, I know the network returns probabilities, as it does a softmax with logits of characters.\n",
        "    I need to transform that probability into an actual char to compose the plate.\n",
        "    I take the argmax of the softmax (most prob char), remove blanks used by CTC and possible\n",
        "    duplicates CTC can actually produce.\n",
        "    At the end I simply use the  mappings char-index index-char deefined at the beginning to compose the plate.\n",
        "    This is greedy as it just takes the argmax of every step, I think it's more than enough here.\n",
        "    '''\n",
        "    # output: [seq_len, batch, num_classes]\n",
        "    out = output.permute(1, 0, 2)  # [batch, seq_len, num_classes]\n",
        "    pred_strings = []\n",
        "    for probs in out:\n",
        "        pred = probs.argmax(1).cpu().numpy()\n",
        "        prev = -1\n",
        "        pred_str = []\n",
        "        for p in pred:\n",
        "            if p != blank and p != prev:\n",
        "                pred_str.append(idx2char[p])\n",
        "            prev = p\n",
        "        pred_strings.append(''.join(pred_str))\n",
        "    return pred_strings\n",
        "\n",
        "\n",
        "'''def evaluate(model, dataloader, device, verbose=False):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_chars = 0\n",
        "    correct_chars = 0\n",
        "\n",
        "    # Metriche aggiuntive per analisi dettagliata\n",
        "    length_errors = 0\n",
        "    province_correct = 0\n",
        "    alphabet_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels, label_lengths) in enumerate(dataloader):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            pred_strings = ctc_greedy_decoder(outputs, idx2char)\n",
        "            labels_cpu = labels.cpu().numpy()   #need to move tensors tu cpu memory for numpy\n",
        "            lengths_cpu = label_lengths.cpu().numpy()\n",
        "            idx = 0\n",
        "            gt_strings = []\n",
        "            for l in lengths_cpu:\n",
        "                gt = ''.join([idx2char[i] for i in labels_cpu[idx:idx+l]])\n",
        "                gt_strings.append(gt)\n",
        "                idx += l\n",
        "\n",
        "            for pred, gt in zip(pred_strings, gt_strings):\n",
        "                # Accuracy completa\n",
        "                if pred == gt:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "                # Accuracy per carattere\n",
        "                min_len = min(len(pred), len(gt))\n",
        "                correct_chars += sum([p == g for p, g in zip(pred[:min_len], gt[:min_len])])\n",
        "                total_chars += len(gt)\n",
        "\n",
        "                # Metriche aggiuntive\n",
        "                if len(pred) != len(gt):\n",
        "                    length_errors += 1\n",
        "\n",
        "                # Accuracy per provincia (primo carattere)\n",
        "                if len(pred) > 0 and len(gt) > 0 and pred[0] == gt[0]:\n",
        "                    province_correct += 1\n",
        "\n",
        "                # Accuracy per alfabeto (secondo carattere)\n",
        "                if len(pred) > 1 and len(gt) > 1 and pred[1] == gt[1]:\n",
        "                    alphabet_correct += 1\n",
        "\n",
        "                # Stampa esempi di errore se richiesto\n",
        "                if verbose and pred != gt and batch_idx == 0:\n",
        "                    print(f\"Pred: '{pred}' | GT: '{gt}'\")\n",
        "\n",
        "    acc = correct / total if total > 0 else 0\n",
        "    acc_char = correct_chars / total_chars if total_chars > 0 else 0\n",
        "    length_error_rate = length_errors / total if total > 0 else 0\n",
        "    province_acc = province_correct / total if total > 0 else 0\n",
        "    alphabet_acc = alphabet_correct / total if total > 0 else 0\n",
        "\n",
        "    print(f\"Eval accuracy (full plate): {acc:.4f} | Char accuracy: {acc_char:.4f}\")\n",
        "    print(f\"Length error rate: {length_error_rate:.4f} | Province acc: {province_acc:.4f} | Alphabet acc: {alphabet_acc:.4f}\")\n",
        "\n",
        "    return acc, acc_char'''\n",
        "\n",
        "\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (images, labels, label_lengths) in enumerate(dataloader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        label_lengths = label_lengths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)  # [W, B, num_classes]\n",
        "        log_probs = outputs.log_softmax(2)\n",
        "        input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long).to(device)\n",
        "\n",
        "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Log batch loss every 100 batches\n",
        "        if batch_idx % 100 == 0:\n",
        "            wandb.log({\n",
        "                \"batch_loss\": loss.item(),\n",
        "                \"epoch\": epoch,\n",
        "                \"batch\": batch_idx\n",
        "            })\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    epoch_time = time.time() - start_time\n",
        "\n",
        "    # Log epoch metrics\n",
        "    wandb.log({\n",
        "        \"train_loss\": avg_loss,\n",
        "        \"epoch_time\": epoch_time,\n",
        "        \"epoch\": epoch,\n",
        "        \"learning_rate\": optimizer.param_groups[0]['lr']\n",
        "    })\n",
        "\n",
        "    print(f\"Epoch {epoch} - Train Loss: {avg_loss:.4f} - Time: {epoch_time:.2f}s\")\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def evaluate(model, dataloader, device, epoch=None, verbose = False,):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    total_chars = 0\n",
        "    correct_chars = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Metriche aggiuntive\n",
        "    length_errors = 0\n",
        "    province_correct = 0\n",
        "    alphabet_correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels, label_lengths in dataloader:\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            pred_strings = ctc_greedy_decoder(outputs, idx2char)\n",
        "\n",
        "            # Get ground truth\n",
        "            labels_cpu = labels.cpu().numpy()\n",
        "            lengths_cpu = label_lengths.cpu().numpy()\n",
        "            idx = 0\n",
        "            gt_strings = []\n",
        "            for l in lengths_cpu:\n",
        "                gt = ''.join([idx2char[i] for i in labels_cpu[idx:idx+l]])\n",
        "                gt_strings.append(gt)\n",
        "                idx += l\n",
        "\n",
        "            # Calculate metrics\n",
        "            for pred, gt in zip(pred_strings, gt_strings):\n",
        "                if pred == gt:\n",
        "                    correct += 1\n",
        "                total += 1\n",
        "\n",
        "                # Character-level accuracy\n",
        "                min_len = min(len(pred), len(gt))\n",
        "                correct_chars += sum([p == g for p, g in zip(pred[:min_len], gt[:min_len])])\n",
        "                total_chars += len(gt)\n",
        "\n",
        "                # Metriche aggiuntive\n",
        "                if len(pred) != len(gt):\n",
        "                    length_errors += 1\n",
        "\n",
        "                # Accuracy per provincia (primo carattere)\n",
        "                if len(pred) > 0 and len(gt) > 0 and pred[0] == gt[0]:\n",
        "                    province_correct += 1\n",
        "\n",
        "                # Accuracy per alfabeto (secondo carattere)\n",
        "                if len(pred) > 1 and len(gt) > 1 and pred[1] == gt[1]:\n",
        "                    alphabet_correct += 1\n",
        "\n",
        "\n",
        "    # Calculate final metrics\n",
        "    acc = correct / total if total > 0 else 0\n",
        "    char_acc = correct_chars / total_chars if total_chars > 0 else 0\n",
        "    eval_time = time.time() - start_time\n",
        "    length_error_rate = length_errors / total if total > 0 else 0\n",
        "    province_acc = province_correct / total if total > 0 else 0\n",
        "    alphabet_acc = alphabet_correct / total if total > 0 else 0\n",
        "\n",
        "    # Log validation metrics\n",
        "    if epoch is not None:\n",
        "        wandb.log({\n",
        "            \"val_accuracy\": acc,\n",
        "            \"val_char_accuracy\": char_acc,\n",
        "            \"val_time\": eval_time,\n",
        "            \"epoch\": epoch\n",
        "        })\n",
        "\n",
        "    print(f\"Val Accuracy: {acc:.4f} - Char Accuracy: {char_acc:.4f} - Time: {eval_time:.2f}s\")\n",
        "    print(f\"Length error rate: {length_error_rate:.4f} | Province acc: {province_acc:.4f} | Alphabet acc: {alphabet_acc:.4f}\")\n",
        "    return acc, char_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d243219",
      "metadata": {
        "id": "7d243219"
      },
      "source": [
        "# data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29857708",
      "metadata": {
        "id": "29857708"
      },
      "outputs": [],
      "source": [
        "class CarPlateDataset(Dataset):\n",
        "\n",
        "    def __init__(self, img_dir, transform=None, cropped = False):\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.image_names = os.listdir(img_dir)\n",
        "        self.cropped = cropped\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_names)\n",
        "\n",
        "    def parse_filename(self, filename):\n",
        "        fields = filename.split('-')\n",
        "        area = float(fields[0]) / 100  #filename encodes the area in percentage (ratio plate-no plate area), so divising by 100 gives me a 0-1 range\n",
        "        tilt_degree = fields[1].split('_')\n",
        "        h_tilt = int(tilt_degree[0])    #horizontal tilt degree\n",
        "        v_tilt = int(tilt_degree[1])    #vertical tilt degree\n",
        "        tilt_list = np.array([h_tilt, v_tilt], dtype=np.float32)\n",
        "\n",
        "\n",
        "        bbox_coords = fields[2].split('_')  #bounding box coordinates\n",
        "        leftUp_bbox = bbox_coords[0].split('&')\n",
        "        leftUp_bbox_x = int(leftUp_bbox[0])\n",
        "        leftUp_bbox_y = int(leftUp_bbox[1])\n",
        "        rightBottom_bbox = bbox_coords[1].split('&')\n",
        "        rightDown_bbox_x = int(rightBottom_bbox[0])\n",
        "        rightDown_bbox_y = int(rightBottom_bbox[1])\n",
        "        bbox_coords_list = np.array([(leftUp_bbox_x, leftUp_bbox_y),\n",
        "                                    (rightDown_bbox_x, rightDown_bbox_y)], dtype=np.float32)\n",
        "\n",
        "        vertices = fields[3].split('_')  #vertices of the plate\n",
        "        right_bottom_vertex = vertices[0].split('&')\n",
        "        right_bottom_vertex_x = int(right_bottom_vertex[0])\n",
        "        right_bottom_vertex_y = int(right_bottom_vertex[1])\n",
        "        left_bottom_vertex = vertices[1].split('&')\n",
        "        left_bottom_vertex_x = int(left_bottom_vertex[0])\n",
        "        left_bottom_vertex_y = int(left_bottom_vertex[1])\n",
        "        left_up_vertex = vertices[2].split('&')\n",
        "        left_up_vertex_x = int(left_up_vertex[0])\n",
        "        left_up_vertex_y = int(left_up_vertex[1])\n",
        "        right_up_vertex = vertices[3].split('&')\n",
        "        right_up_vertex_x = int(right_up_vertex[0])\n",
        "        right_up_vertex_y = int(right_up_vertex[1])\n",
        "\n",
        "        vertices_list = np.array([(left_bottom_vertex_x, left_bottom_vertex_y),\n",
        "                                (right_bottom_vertex_x, right_bottom_vertex_y),\n",
        "                                (right_up_vertex_x, right_up_vertex_y),\n",
        "                                (left_up_vertex_x, left_up_vertex_y)], dtype=np.float32)\n",
        "\n",
        "        # Usa le variabili globali invece di ridefinirle\n",
        "        text=str(fields[4])\n",
        "        indices=text.split(\"_\")\n",
        "        province_character=provinces[int(indices[0])]\n",
        "        alphabet_character=alphabets[int(indices[1])]\n",
        "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
        "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
        "\n",
        "        brightness = int(fields[5])\n",
        "        #blurriness = int(fields[6].strip('.jpg'))  # Remove .jpg, it's end of filename\n",
        "        #it gave problems, try with this\n",
        "        blurriness_str = fields[6].replace('.jpg', '')\n",
        "        match = re.match(r'\\d+', blurriness_str)\n",
        "        if match:\n",
        "            blurriness = int(match.group())\n",
        "        else:\n",
        "            print(f\"[WARNING] File '{filename}': blurriness non standard '{fields[6]}', imposto a 0.\")\n",
        "            blurriness = 0\n",
        "\n",
        "        # Convert license plate text to indices for CTC training\n",
        "        # Aggiungi controllo per caratteri non riconosciuti\n",
        "        lp_indexes = []\n",
        "        for c in plate_text:\n",
        "            if c in char2idx:\n",
        "                lp_indexes.append(char2idx[c])\n",
        "            else:\n",
        "                print(f\"[WARNING] Carattere non riconosciuto '{c}' in '{plate_text}'\")\n",
        "\n",
        "        return {\n",
        "            'area': area,\n",
        "            'tilt': tilt_list,\n",
        "            'bbox_coords': bbox_coords_list,\n",
        "            'vertices': vertices_list,\n",
        "            'lp': plate_text,\n",
        "            'lp_indexes': lp_indexes,\n",
        "            'brightness': brightness,\n",
        "            'blurriness': blurriness,\n",
        "        }\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.image_names[idx]\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # Load the image\n",
        "        image = Image.open(img_path)\n",
        "\n",
        "        # Parse the filename to get the associated metadata\n",
        "        metadata = self.parse_filename(img_name)\n",
        "\n",
        "        if self.cropped:    #I use this dataset for both baselines, so I check if I need to skip detection part and use dataset bbox.\n",
        "            #I can use the crop method of PIL, that crops the image using coords in this way: (left, upper, right, lower)\n",
        "            '''\n",
        "            left is the x-coordinate of the left edge.\n",
        "\n",
        "            upper is the y-coordinate of the top edge.\n",
        "\n",
        "            right is the x-coordinate of the right edge.\n",
        "\n",
        "            lower is the y-coordinate of the bottom edge.\n",
        "            seen on the online odcs of pillow\n",
        "            '''\n",
        "            bbox_coords = metadata['bbox_coords']\n",
        "\n",
        "            left = int(bbox_coords[0][0])   # x-coordinate of the left edge\n",
        "            upper = int(bbox_coords[0][1])  # y-coordinate of the top edge\n",
        "            right = int(bbox_coords[1][0])  # x-coordinate of the right edge\n",
        "            lower = int(bbox_coords[1][1])  # y-coordinate of the bottom edge\n",
        "\n",
        "            image = image.crop((left, upper, right, lower))\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(metadata['lp_indexes'], dtype=torch.long)  # Return the image and the license plate indexes as a tensor, for the CNN to elaborate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inC64Hm4I0V3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inC64Hm4I0V3",
        "outputId": "94c9be9f-dea5-452f-de05-b82934c33b83"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create datasets\n",
        "train_dataset = CarPlateDataset(\n",
        "    img_dir='/content/yolov5/runs/detect/yolo_train/crops/license_plate',\n",
        "    transform=transform,\n",
        "    cropped=False\n",
        ")\n",
        "\n",
        "val_dataset = CarPlateDataset(\n",
        "    img_dir='/content/yolov5/runs/detect/yolo_val/crops/license_plate',\n",
        "    transform=transform,\n",
        "    cropped=False\n",
        ")\n",
        "\n",
        "test_dataset = CarPlateDataset(\n",
        "    img_dir='/content/yolov5/runs/detect/yolo_test/crops/license_plate',\n",
        "    transform=transform,\n",
        "    cropped=False\n",
        ")\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=ctc_collate_fn,\n",
        "    num_workers = 0\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=ctc_collate_fn,\n",
        "    num_workers = 0\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    collate_fn=ctc_collate_fn,\n",
        "    num_workers = 0\n",
        ")\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(f\"Test samples: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "449ff7e0",
      "metadata": {
        "id": "449ff7e0"
      },
      "source": [
        "# network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe68d4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "ebe68d4a",
        "outputId": "e5df1a8b-73aa-4761-ad52-b15738fc316c"
      },
      "outputs": [],
      "source": [
        "wandb.init(\n",
        "    project=\"pdlpr-license-plate-recognition\",\n",
        "    config={\n",
        "        \"architecture\": \"PDLPR\",\n",
        "        \"dataset\": \"CCPD_green\",\n",
        "        \"batch_size\": 32,\n",
        "        \"learning_rate\": 0.001,\n",
        "        \"epochs\": 50,\n",
        "        \"optimizer\": \"Adam\",\n",
        "        \"image_size\": (64, 256),\n",
        "        \"num_classes\": 69,\n",
        "        \"dropout\": 0.1\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create model\n",
        "model = PDLPR(num_classes=BLANK_IDX + 1, dropout=0.1).to(device)\n",
        "\n",
        "# Print model summary\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"Model has {count_parameters(model):,} trainable parameters\")\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CTCLoss(blank=BLANK_IDX, zero_infinity=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "print(\"Model initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f539e66",
      "metadata": {
        "id": "8f539e66"
      },
      "source": [
        "# train"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VH-CklbbFeFJ",
      "metadata": {
        "id": "VH-CklbbFeFJ"
      },
      "source": [
        "## complete train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "496S8x-5Odpc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "496S8x-5Odpc",
        "outputId": "afa6615a-809b-47ad-e783-dccce1289415"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03817781",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "03817781",
        "outputId": "b9b1af26-1162-4c35-cf49-6e56329d640d"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "num_epochs = 15\n",
        "best_val_acc = 0.0\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
        "\n",
        "    # Training\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # Validation\n",
        "    val_acc, val_char_acc = evaluate(model, val_loader, device, epoch, verbose=(epoch % 10 == 0))\n",
        "    val_accuracies.append(val_acc)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_acc': val_acc,\n",
        "            'val_char_acc': val_char_acc,\n",
        "            'train_loss': train_loss,\n",
        "        }, 'best_pdlpr_model.pth')\n",
        "\n",
        "        # Log best model info to wandb\n",
        "        wandb.log({\n",
        "            \"best_val_acc\": best_val_acc,\n",
        "            \"best_epoch\": epoch\n",
        "        })\n",
        "\n",
        "        print(f\"New best model saved! Validation accuracy: {val_acc:.4f}\")\n",
        "\n",
        "     # Log summary metrics\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"best_val_acc_so_far\": best_val_acc\n",
        "        })\n",
        "\n",
        "    print(f\"Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "\n",
        "wandb.summary[\"final_best_val_acc\"] = best_val_acc\n",
        "wandb.summary[\"total_epochs\"] = num_epochs\n",
        "\n",
        "print(f\"\\nTraining completed! Best validation accuracy: {best_val_acc:.4f}\")\n",
        "\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e23df049",
      "metadata": {
        "id": "e23df049"
      },
      "source": [
        "# test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6416ddcf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6416ddcf",
        "outputId": "cefe1b35-b301-49b7-8a4f-2975d5533718"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load best model\n",
        "if os.path.exists('best_pdlpr_model.pth'):\n",
        "    checkpoint = torch.load('best_pdlpr_model.pth', map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Loaded best model from epoch {checkpoint['epoch']} with validation accuracy: {checkpoint['val_acc']:.4f}\")\n",
        "else:\n",
        "    print(\"No saved model found, using current model\")\n",
        "\n",
        "# Test the model\n",
        "print(\"\\n=== Final Test Results ===\")\n",
        "test_acc, test_char_acc = evaluate(model, test_loader, device, verbose=True)\n",
        "\n",
        "print(f\"\\nFinal Test Accuracy: {test_acc:.4f}\")\n",
        "print(f\"Final Test Character Accuracy: {test_char_acc:.4f}\")\n",
        "\n",
        "# Test on a few sample images\n",
        "def test_sample_predictions(model, test_loader, num_samples=5):\n",
        "    model.eval()\n",
        "    samples_shown = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels, label_lengths) in enumerate(test_loader):\n",
        "            if samples_shown >= num_samples:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            pred_strings = ctc_greedy_decoder(outputs, idx2char)\n",
        "\n",
        "            # Get ground truth\n",
        "            labels_cpu = labels.cpu().numpy()\n",
        "            lengths_cpu = label_lengths.cpu().numpy()\n",
        "            idx = 0\n",
        "            gt_strings = []\n",
        "            for l in lengths_cpu:\n",
        "                gt = ''.join([idx2char[i] for i in labels_cpu[idx:idx+l]])\n",
        "                gt_strings.append(gt)\n",
        "                idx += l\n",
        "\n",
        "            # Show predictions\n",
        "            for i, (pred, gt) in enumerate(zip(pred_strings, gt_strings)):\n",
        "                if samples_shown >= num_samples:\n",
        "                    break\n",
        "                print(f\"Sample {samples_shown+1}: Predicted: '{pred}' | Ground Truth: '{gt}' | Match: {pred == gt}\")\n",
        "                samples_shown += 1\n",
        "\n",
        "            if samples_shown >= num_samples:\n",
        "                break\n",
        "\n",
        "print(\"\\n=== Sample Predictions ===\")\n",
        "test_sample_predictions(model, test_loader, num_samples=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4ac941e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "a4ac941e",
        "outputId": "d4162d71-f436-4055-a71c-c8ca27980b98"
      },
      "outputs": [],
      "source": [
        "# Performance visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_curves(train_losses, val_accuracies):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot training loss\n",
        "    ax1.plot(train_losses, 'b-', label='Training Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot validation accuracy\n",
        "    ax2.plot(val_accuracies, 'r-', label='Validation Accuracy')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.set_title('Validation Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot if we have training data\n",
        "if 'train_losses' in locals() and 'val_accuracies' in locals():\n",
        "    plot_training_curves(train_losses, val_accuracies)\n",
        "else:\n",
        "    print(\"No training data available for plotting\")\n",
        "\n",
        "# Model inference time test\n",
        "import time\n",
        "\n",
        "def test_inference_speed(model, test_loader, num_batches=10):\n",
        "    model.eval()\n",
        "    times = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels, label_lengths) in enumerate(test_loader):\n",
        "            if batch_idx >= num_batches:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Time the forward pass\n",
        "            start_time = time.time()\n",
        "            outputs = model(images)\n",
        "            end_time = time.time()\n",
        "\n",
        "            batch_time = end_time - start_time\n",
        "            times.append(batch_time)\n",
        "\n",
        "    avg_time = sum(times) / len(times)\n",
        "    avg_per_image = avg_time / images.size(0)\n",
        "\n",
        "    print(f\"\\nInference Speed Test:\")\n",
        "    print(f\"Average time per batch ({images.size(0)} images): {avg_time:.4f} seconds\")\n",
        "    print(f\"Average time per image: {avg_per_image:.4f} seconds\")\n",
        "    print(f\"Estimated FPS: {1/avg_per_image:.2f}\")\n",
        "\n",
        "# Test inference speed if test_loader exists\n",
        "if 'test_loader' in locals():\n",
        "    test_inference_speed(model, test_loader)\n",
        "else:\n",
        "    print(\"No test loader available for speed test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NnW5_m2Uc1Kd",
      "metadata": {
        "id": "NnW5_m2Uc1Kd"
      },
      "source": [
        "So yolo inference time was roughly 10ms per image on average, plus the average 1ms of pdlpr inference = about 11ms for the complete pipeline.\n",
        "So about 110 FPS"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ye6Bmv0peo2b",
      "metadata": {
        "id": "Ye6Bmv0peo2b"
      },
      "source": [
        "# ONLINE COMPLETE PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "p0uYVCAVr9Rx",
      "metadata": {
        "id": "p0uYVCAVr9Rx"
      },
      "outputs": [],
      "source": [
        "class LicensePlateRecognitionPipeline:\n",
        "    def __init__(self, yolo_model_path, pdlpr_model_path, device='cuda'):\n",
        "        \"\"\"\n",
        "        Pipeline completa per detection e riconoscimento targhe\n",
        "\n",
        "        Args:\n",
        "            yolo_model_path: Path al modello YOLOv5 addestrato\n",
        "            pdlpr_model_path: Path al modello PDLPR addestrato\n",
        "            device: Device da usare ('cuda' o 'cpu')\n",
        "        \"\"\"\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Carica modello YOLOv5\n",
        "        self.yolo_model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_model_path)\n",
        "        self.yolo_model.to(self.device)\n",
        "        self.yolo_model.eval()\n",
        "\n",
        "        # Carica modello PDLPR\n",
        "        self.pdlpr_model = PDLPR(num_classes=BLANK_IDX + 1, dropout=0.1).to(self.device)\n",
        "        checkpoint = torch.load(pdlpr_model_path, map_location=self.device)\n",
        "        self.pdlpr_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.pdlpr_model.eval()\n",
        "\n",
        "        # Transform per PDLPR\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((64, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "        ])\n",
        "\n",
        "        print(\"Pipeline initialized successfully!\")\n",
        "\n",
        "    def yolo_crop(self, image_path):\n",
        "        \"\"\"\n",
        "        detecting plates\n",
        "\n",
        "        Args:\n",
        "            image_path\n",
        "\n",
        "        Returns:\n",
        "            cropped image\n",
        "        \"\"\"\n",
        "        img = Image.open(image_path)\n",
        "        ris = self.yolo_model(image_path)  # Usa self.yolo_model\n",
        "        metadata = ris.pred[0]\n",
        "\n",
        "        # Prendi solo la prima detection (o la migliore)\n",
        "        if len(metadata) > 0:\n",
        "            det = metadata[0]  # Prima detection\n",
        "            x1, y1, x2, y2 = map(int, det[:4])\n",
        "            cropped = img.crop((x1, y1, x2, y2))\n",
        "            return cropped\n",
        "        else:\n",
        "            print(\"Nessuna targa rilevata!\")\n",
        "            return None\n",
        "\n",
        "    def recognize_plate(self, plate_crop):\n",
        "        \"\"\"\n",
        "        Riconosce il testo di una targa usando PDLPR\n",
        "\n",
        "        Args:\n",
        "            plate_crop: Crop della targa (PIL Image)\n",
        "\n",
        "        Returns:\n",
        "            Testo riconosciuto\n",
        "        \"\"\"\n",
        "        if plate_crop is None:\n",
        "            return \"\"\n",
        "\n",
        "        plate_crop_tr = self.transform(plate_crop).unsqueeze(0)\n",
        "        plate_crop_tr = plate_crop_tr.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.pdlpr_model(plate_crop_tr)  # Usa self.pdlpr_model\n",
        "            pred_string = ctc_greedy_decoder(outputs, idx2char)\n",
        "\n",
        "        return pred_string[0] if pred_string else \"\"\n",
        "\n",
        "    def __call__(self, img_path):\n",
        "        \"\"\"\n",
        "        Rende l'oggetto callable - metodo principale della pipeline\n",
        "\n",
        "        Args:\n",
        "            img_path: Path all'immagine\n",
        "\n",
        "        Returns:\n",
        "            Testo della targa riconosciuto\n",
        "        \"\"\"\n",
        "        crop = self.yolo_crop(img_path)\n",
        "        plate = self.recognize_plate(crop)\n",
        "        return plate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JrQhXyWhqT-J",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrQhXyWhqT-J",
        "outputId": "d631587e-d462-4dc2-a547-78d4d2720f62"
      },
      "outputs": [],
      "source": [
        "online_model = LicensePlateRecognitionPipeline(yolo_model_path = \"/content/yolo_finetuned_model.pt\", pdlpr_model_path = \"/content/best_pdlpr_model.pth\", device = \"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZSoYhhZvqsI1",
      "metadata": {
        "id": "ZSoYhhZvqsI1"
      },
      "outputs": [],
      "source": [
        "plate = online_model(\"/content/ccpd_green/test/0026484674329501916-93_90-283&513_362&543-360&543_283&539_283&513_362&518-0_0_3_25_32_32_33_33-93-14.jpg\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hylqnj92thpP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hylqnj92thpP",
        "outputId": "bb75d1ae-0050-4426-8539-3af2c55cde09"
      },
      "outputs": [],
      "source": [
        "get_plate(\"0026484674329501916-93_90-283&513_362&543-360&543_283&539_283&513_362&518-0_0_3_25_32_32_33_33-93-14.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9yZ8KaposDok",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9yZ8KaposDok",
        "outputId": "76a63a94-36e6-450a-ec3f-2eec581d2617"
      },
      "outputs": [],
      "source": [
        "plate"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "e6d9d630"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
