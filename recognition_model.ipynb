{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46c6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOUNDING BOX FUNCTION --> IT DOES NOT LET ME IMPORT IT\n",
    "\n",
    "def get_bounding_box(file):\n",
    "    numbers=file.split(\"-\")\n",
    "    values=numbers[3]\n",
    "    values_v2=values.split(\"&\")\n",
    "    values_v3=[]\n",
    "    for i in range(len(values_v2)):\n",
    "        if \"_\" in values_v2[i]:\n",
    "            values_v3.append(values_v2[i].split(\"_\"))\n",
    "    t=[values_v2[0],values_v3[0],values_v3[1],values_v3[2],values_v2[-1]]\n",
    "    final_values = [int(x) for item in t for x in (item if isinstance(item, list) else [item])]\n",
    "    x_coords=[final_values[0],final_values[2],final_values[4],final_values[6]]\n",
    "    y_coords=[final_values[1],final_values[3],final_values[5],final_values[7]]\n",
    "    x_min = min(x_coords)\n",
    "    y_min = min(y_coords)\n",
    "    x_max = max(x_coords)\n",
    "    y_max = max(y_coords)\n",
    "    \n",
    "    return [float(x_min), float(y_min), float(x_max), float(y_max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baeeaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAR PLATE TEXT FUNCTION\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "def get_text(file):\n",
    "    values=file.split(\"-\")\n",
    "    text=str(values[4])\n",
    "    indices=text.split(\"_\")\n",
    "    province_character=provinces[int(indices[0])]\n",
    "    alphabet_character=alphabet[int(indices[1])]\n",
    "    ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "    plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "    return plate_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET \n",
    "\n",
    "all_characters = sorted(set(provinces + alphabet + ads))\n",
    "char2idx = {char: idx + 1 for idx, char in enumerate(all_characters)}  # +1 for CTC blank\n",
    "\n",
    "class CCPD_dataset_recognition(Dataset):\n",
    "    \n",
    "    def __init__(self,path,char2idx,transformations):\n",
    "        self.path=path\n",
    "        self.char2idx=char2idx\n",
    "        self.transformations=transformations\n",
    "        self.images=[f for f in os.listdir(path) if f.endswith(\"jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file=self.images[idx]\n",
    "        gt_bb=get_bounding_box(file)\n",
    "        gt_text=get_text(file)\n",
    "        gt_text_tensor= torch.tensor([self.char2idx[c] for c in gt_text], dtype=torch.long)\n",
    "        full_path=os.path.join(self.path,file)\n",
    "        image=Image.open(full_path)\n",
    "        cropped_image=image.crop(gt_bb)\n",
    "        if self.transformations:\n",
    "            cropped_image=self.transformations(cropped_image)\n",
    "        return cropped_image, gt_text_tensor\n",
    "    \n",
    "def crnn_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    image_batch = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n",
    "    labels_concat = torch.cat(labels)\n",
    "    return image_batch, labels_concat, label_lengths\n",
    "\n",
    "path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "transformations=T.Compose([T.Grayscale(num_output_channels=1),T.Resize((64,256)),T.ToTensor()])\n",
    "recognition_dataset=CCPD_dataset_recognition(path,char2idx,transformations=transformations)\n",
    "train_dataloader=DataLoader(recognition_dataset,batch_size=8, shuffle=True, collate_fn=crnn_collate_fn)\n",
    "\n",
    "batch=next(iter(train_dataloader))\n",
    "batch[0].shape\n",
    "# returns a batch containing the images as tensors and the ground truth of the text once treated with char2idx (all stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a38f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CRNN, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2, 1)),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(), nn.MaxPool2d((2, 1)),\n",
    "            nn.AdaptiveAvgPool2d((2, None)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(1024, 256, bidirectional=True, num_layers=2,dropout=0.5, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_classes + 1)  # +1 for CTC blank\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        b, c, h, w = x.size()\n",
    "        assert h == 2, f\"Expected height to be 2 after CNN, got {h}\"\n",
    "        x = x.permute(0, 3, 1, 2).contiguous().view(b, w, c * h)  # [B, W, 1024]\n",
    "        x, _ = self.rnn(x)\n",
    "\n",
    "        return x.permute(1, 0, 2)  # for CTC loss: [T, B, C]\n",
    "\n",
    "recognition_model=CRNN(67)\n",
    "# recognition_model(batch[0])\n",
    "# returns a tensor of size 25X1X68 --> this output has to be decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16836509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "from torch.nn import CTCLoss\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = recognition_model.to(device)\n",
    "\n",
    "loss_fn = CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(recognition_model.parameters(), lr=0.001)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch + 1}\\n-------')\n",
    "    recognition_model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch, (images, targets, target_lengths) in enumerate(train_dataloader):\n",
    "        # images = images.to(device)\n",
    "        # targets = targets.to(device)\n",
    "        # target_lengths = target_lengths.to(device)\n",
    "          # Forward pass\n",
    "        preds = recognition_model(images)  # preds: [T, B, C]\n",
    "        T, B, C = preds.size()\n",
    "        input_lengths = torch.full(size=(B,), fill_value=T, dtype=torch.long)  # Each sample has T time steps\n",
    "        log_probs = preds.log_softmax(2)  # dim=2 is for classes\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(f'Batch {batch}: Loss = {loss.item():.4f}')\n",
    "\n",
    "    avg_loss = train_loss / len(train_dataloader)\n",
    "    print(f'Average training loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f53e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(recognition_model.state_dict(), \"crnn_ctc_model.pth\") # LOSS: 33\n",
    "\n",
    "# TO LOAD IT \n",
    "\n",
    "# from crnn_model import CRNN  # or wherever your model is defined\n",
    "# model = CRNN(imgH=32, nc=1, nclass=NUM_CLASSES, nh=256)  # example args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09987aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVAL LOOP\n",
    "\n",
    "eval_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/eval\"\n",
    "recognition_dataset_eval=CCPD_dataset_recognition(eval_path,char2idx,transformations=transformations)\n",
    "eval_dataloader=DataLoader(recognition_dataset_eval,batch_size=8, shuffle=False, collate_fn=crnn_collate_fn)\n",
    "\n",
    "\n",
    "recognition_model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, targets, target_lengths in eval_dataloader:\n",
    "        preds = recognition_model(images)\n",
    "        log_probs = preds.log_softmax(2)\n",
    "        T, B, C = preds.size()\n",
    "        input_lengths = torch.full(size=(B,), fill_value=T, dtype=torch.long)\n",
    "        loss = loss_fn(log_probs, targets, input_lengths, target_lengths)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    test_loss /= len(eval_dataloader)\n",
    "    print(f'Test loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8da83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECODING RESULTS\n",
    "\n",
    "def greedy_decode(ctc_output, idx2char):\n",
    "    # ctc_output: [T, B, C] (output from the model)\n",
    "    preds = ctc_output.permute(1, 0, 2)  # [B, T, C]\n",
    "    pred_strings = []\n",
    "    for pred in preds:\n",
    "        best_path = torch.argmax(pred, dim=1).tolist()  # get index with max prob at each timestep\n",
    "        prev = -1\n",
    "        decoded = []\n",
    "        for p in best_path:\n",
    "            if p != prev and p != 0:  # 0 is the CTC blank\n",
    "                decoded.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(\"\".join(decoded))\n",
    "    return pred_strings\n",
    "\n",
    "recognition_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for images, targets, target_lengths in eval_dataloader:\n",
    "        outputs = recognition_model(images)  # [T, B, C]\n",
    "        predictions = greedy_decode(outputs, idx2char)\n",
    "\n",
    "        # Compare to original labels (you need original ground truth strings)\n",
    "        # This depends on how your DataLoader is structured\n",
    "\n",
    "        # Example (if you stored gt_texts during dataset loading):\n",
    "        for pred, true in zip(predictions, gt_texts):\n",
    "            if pred == true:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "print(f\"Accuracy: {correct / total * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c3d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE TUNING trOCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2334a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-stage1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32bbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCPD_dataset_recognition2(Dataset):\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "        self.images=[f for f in os.listdir(path) if f.endswith(\"jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file=self.images[idx]\n",
    "        gt_bb=get_bounding_box(file)\n",
    "        gt_text=get_text(file)\n",
    "        full_path=os.path.join(self.path,file)\n",
    "        image=Image.open(full_path).convert(\"RGB\")\n",
    "        cropped_image=image.crop(gt_bb)\n",
    "        return {\"image\":cropped_image, \"text\": gt_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0381fed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_trocr(example):\n",
    "    encoding = processor(images=example[\"image\"], text=example[\"text\"], return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "    encoding[\"labels\"] = encoding[\"input_ids\"]\n",
    "    return {k: v.squeeze(0) for k, v in encoding.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b48d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "train_path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\"\n",
    "\n",
    "raw_data = CCPD_dataset_recognition2(train_path)\n",
    "hf_dataset = HFDataset.from_list([raw_data[i] for i in range(len(raw_data))])\n",
    "hf_dataset = hf_dataset.map(preprocess_for_trocr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fcb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./trocr-ccpd\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    evaluation_strategy=\"no\",\n",
    "    logging_dir=\"./logs\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINE TUNING EASY OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c491527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"/home/filippo/Documents/Visual Studio Code/Computer_Vision/Data/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29399f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CCPD_dataset_recognition2(Dataset):\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "        self.images=[f for f in os.listdir(path) if f.endswith(\"jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file=self.images[idx]\n",
    "        gt_bb=get_bounding_box(file)\n",
    "        gt_text=get_text(file)\n",
    "        full_path=os.path.join(self.path,file)\n",
    "        image=Image.open(full_path).convert(\"RGB\")\n",
    "        cropped_image=image.crop(gt_bb)\n",
    "        return {\"image\":cropped_image, \"text\": gt_text}\n",
    "\n",
    "dataset_prova=CCPD_dataset_recognition2(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d904b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_folder(folder_path):\n",
    "    cropped_folder = []\n",
    "    files = os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        cropped_image = crop_image_with_ground_truth(full_path)\n",
    "        cropped_folder.append(cropped_image)\n",
    "    return cropped_folder\n",
    "\n",
    "def crop_image_with_ground_truth(full_path):\n",
    "    filename = os.path.basename(full_path)  # extract just the filename for parsing\n",
    "    bb = get_bounding_box(filename)\n",
    "    image = Image.open(full_path).convert(\"RGB\")\n",
    "    cropped_image = image.crop(bb)\n",
    "    return cropped_image\n",
    "\n",
    "output_image_directory='train_cropped_images'\n",
    "os.makedirs(output_image_directory,exist_ok=True)\n",
    "\n",
    "cropped_folder=crop_folder(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
