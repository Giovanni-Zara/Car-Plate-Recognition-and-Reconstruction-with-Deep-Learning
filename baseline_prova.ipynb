{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base idea is:\n",
    "* **FIRST BASELINE** -> no detection, usage of ground truth for the bounding box | recognition part with CNN or CRNN\n",
    "\n",
    "* **SECOND BASELINE** -> detection through groundedSAM2 and recognition with CNN/CRNN\n",
    "\n",
    "* **SECOND BASELINE** -> detection I have to understand how and recognition with the paper model rpnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eb6YUgKM5H_i"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5352QNWXuuf3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce GTX 1070\n",
      "Number of GPUs: 1\n",
      "Current Device Index: 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current Device Index:\", torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2bPw-5W4hJe"
   },
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "vjsGOQfMuf5-"
   },
   "outputs": [],
   "source": [
    "#transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)), # Resize to a fixed size\n",
    "    #transforms.ColorJitter(brightness=0.2, contrast=0.2), # Augmentation, DUNNO ABOUT THIS, MAYBE LATER\n",
    "    transforms.ToTensor(), # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]), # Normalize the image to have mean 0.5 and std 0.5\n",
    "    transforms.Grayscale(num_output_channels=1), #allows the model to focus on plate numbers without color distraction\n",
    "    #transforms.RandomRotation(degrees=3), # small tilt to simulate real-world scenarios, already present, commenting for now\n",
    "    #transforms.RandomPerspective(distortion_scale=0.2, p=0.5) #this as well, to simulate real world random perspective distortions. have to check other dataset folders, maybe already present\n",
    "])\n",
    "\n",
    "\n",
    "#saving fields of the licence plate as global variables, i'm gonna use them later on\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nm2TAq6B5UBI"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rT1Q2y0kuuRT"
   },
   "outputs": [],
   "source": [
    "class PlateCNN(nn.Module):\n",
    "    def __init__(self, num_provinces, num_alphabets, num_ads):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))\n",
    "        )\n",
    "        self.fc_province = nn.Linear(128, num_provinces)\n",
    "        self.fc_alpha = nn.Linear(128, num_alphabets)\n",
    "        self.fc_ads = nn.ModuleList([nn.Linear(128, num_ads) for _ in range(5)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        out_prov = self.fc_province(x)\n",
    "        out_alpha = self.fc_alpha(x)\n",
    "        out_ads = [fc(x) for fc in self.fc_ads]\n",
    "        return out_prov, out_alpha, out_ads\n",
    "    \n",
    "\n",
    "\n",
    "'''def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in dataloader:\n",
    "        #print(\"TYPE:\", type(labels))       DEBUG\n",
    "        #print(\"SHAPE:\", getattr(labels, 'shape', None))    # DEBUG\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out_prov, out_alpha, out_ads = model(images)\n",
    "        loss = criterion(out_prov, labels[:, 0]) + \\\n",
    "               criterion(out_alpha, labels[:, 1])\n",
    "        for i in range(5):\n",
    "            loss += criterion(out_ads[i], labels[:, i+2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)'''\n",
    "\n",
    "\n",
    "\n",
    "'''def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            out_prov, out_alpha, out_ads = model(images)\n",
    "            pred_prov = out_prov.argmax(1)\n",
    "            pred_alpha = out_alpha.argmax(1)\n",
    "            pred_ads = [out.argmax(1) for out in out_ads]\n",
    "            preds = torch.stack([pred_prov, pred_alpha] + pred_ads, dim=1)\n",
    "            correct += (preds == labels).all(dim=1).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out_prov, out_alpha, out_ads = model(images)\n",
    "        loss = criterion(out_prov, labels[:, 0]) + \\\n",
    "               criterion(out_alpha, labels[:, 1])\n",
    "        for i in range(5):\n",
    "            loss += criterion(out_ads[i], labels[:, i+2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Verbose output\n",
    "        if batch_idx == 0:\n",
    "            print(f\"[Batch {batch_idx}] Loss: {loss.item():.4f}\")\n",
    "            print(\"labels[0]:\", labels[0].cpu().numpy())\n",
    "            print(\"pred_prov[0]:\", out_prov[0].argmax().item(), \"label:\", labels[0,0].item())\n",
    "            print(\"pred_alpha[0]:\", out_alpha[0].argmax().item(), \"label:\", labels[0,1].item())\n",
    "            print(\"pred_ads[0]:\", [out_ads[j][0].argmax().item() for j in range(5)], \"label:\", [labels[0,2+i].item() for i in range(5)])\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    correct_per_char = 0\n",
    "    total_chars = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            out_prov, out_alpha, out_ads = model(images)\n",
    "            pred_prov = out_prov.argmax(1)\n",
    "            pred_alpha = out_alpha.argmax(1)\n",
    "            pred_ads = [out.argmax(1) for out in out_ads]\n",
    "            preds = torch.stack([pred_prov, pred_alpha] + pred_ads, dim=1)\n",
    "            correct += (preds == labels).all(dim=1).sum().item()\n",
    "            total += labels.size(0)\n",
    "            correct_per_char += (preds == labels).sum().item()\n",
    "            total_chars += labels.numel()\n",
    "    acc_all = correct / total\n",
    "    acc_char = correct_per_char / total_chars\n",
    "    return acc_all, acc_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoQYEe4V5j5E"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample image name is \"025-95_113-154&383_386&473-386&473_177&454_154&383_363&402-0_0_22_27_27_33_16-37-15.jpg\".\n",
    "Each name can be splited by \"-\" into seven fields. Those fields are explained as follows.\n",
    "\n",
    "1) Area: Area ratio of license plate area to the entire picture area.\n",
    "\n",
    "2) Tilt degree: Horizontal tilt degree and vertical tilt degree.\n",
    "\n",
    "3) Bounding box coordinates: The coordinates of the left-up and the right-bottom vertices.\n",
    "\n",
    "4) Four vertices locations: The exact (x, y) coordinates of the four vertices of LP in the whole image. These coordinates start from the right-bottom vertex.\n",
    "\n",
    "5) License plate number: Each image in CCPD has only one LP. Each LP number is comprised of a Chinese character, a letter, and five letters or numbers. A valid Chinese license plate consists of seven characters:\n",
    "\n",
    "       - province (1 character),\n",
    "       - alphabets (1 character),\n",
    "       - alphabets+digits (5 characters).\n",
    "\n",
    "\"0_0_22_27_27_33_16\" is the index of each character. These three arrays are defined as follows. The last character of each array is letter O rather than a digit 0. We use O as a sign of \"no character\" because there is no O in Chinese license plate characters.\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "\n",
    "\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W',\n",
    "             'X', 'Y', 'Z', 'O']\n",
    "\n",
    "\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X',\n",
    "       'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "\n",
    "6) Brightness: The brightness of the license plate region.\n",
    "\n",
    "7) Blurriness: The Blurriness of the license plate region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "QYypMNabutxW"
   },
   "outputs": [],
   "source": [
    "class CarPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, transform=None, cropped = False):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(img_dir)\n",
    "        self.cropped = cropped\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def parse_filename(self, filename):\n",
    "        fields = filename.split('-')\n",
    "        area = float(fields[0]) / 100  #filename encodes the area in percentage (ratio plate-no plate area), so divising by 100 gives me a 0-1 range\n",
    "        tilt_degree = fields[1].split('_')\n",
    "        h_tilt = int(tilt_degree[0])    #horizontal tilt degree\n",
    "        v_tilt = int(tilt_degree[1])    #vertical tilt degree\n",
    "        tilt_list = np.array([h_tilt, v_tilt], dtype=np.float32)\n",
    "\n",
    "\n",
    "        bbox_coords = fields[2].split('_')  #buonding box coordinates\n",
    "        leftUp_bbox = bbox_coords[0].split('&')\n",
    "        leftUp_bbox_x = int(leftUp_bbox[0])\n",
    "        leftUp_bbox_y = int(leftUp_bbox[1])\n",
    "        rightBottom_bbox = bbox_coords[1].split('&')\n",
    "        rightDown_bbox_x = int(rightBottom_bbox[0])\n",
    "        rightDown_bbox_y = int(rightBottom_bbox[1])\n",
    "        bbox_coords_list = np.array([(leftUp_bbox_x, leftUp_bbox_y),\n",
    "                                    (rightDown_bbox_x, rightDown_bbox_y)], dtype=np.float32)\n",
    "\n",
    "        vertices = fields[3].split('_')  #vertices of the plate\n",
    "        left_bottom_vertex = vertices[0].split('&')\n",
    "        left_bottom_vertex_x = int(left_bottom_vertex[0])\n",
    "        left_bottom_vertex_y = int(left_bottom_vertex[1])\n",
    "        right_bottom_vertex = vertices[1].split('&')\n",
    "        right_bottom_vertex_x = int(right_bottom_vertex[0])\n",
    "        right_bottom_vertex_y = int(right_bottom_vertex[1])\n",
    "        right_up_vertex = vertices[2].split('&')\n",
    "        right_up_vertex_x = int(right_up_vertex[0])\n",
    "        right_up_vertex_y = int(right_up_vertex[1])\n",
    "        left_up_vertex = vertices[3].split('&')\n",
    "        left_up_vertex_x = int(left_up_vertex[0])\n",
    "        left_up_vertex_y = int(left_up_vertex[1])\n",
    "        vertices_list = np.array([(left_bottom_vertex_x, left_bottom_vertex_y),\n",
    "                                (right_bottom_vertex_x, right_bottom_vertex_y),\n",
    "                                (right_up_vertex_x, right_up_vertex_y),\n",
    "                                (left_up_vertex_x, left_up_vertex_y)], dtype=np.float32)\n",
    "        \n",
    "        lp_numbers = list(map(int, fields[4].split('_')))[:7]  #license plate number\n",
    "        provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "        alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "        ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "        \n",
    "        lp = provinces[lp_numbers[0]] + alphabets[lp_numbers[1]] + ads[lp_numbers[2]] + ads[lp_numbers[3]] + ads[lp_numbers[4]] + ads[lp_numbers[5]]\n",
    "\n",
    "        brightness = int(fields[5])\n",
    "        blurriness = int(fields[6].strip('.jpg'))  # Remove .jpg, it's end of filename\n",
    "\n",
    "        return {\n",
    "            'area': area,\n",
    "            'tilt': tilt_list,\n",
    "            'bbox_coords': bbox_coords_list,\n",
    "            'vertices': vertices_list,\n",
    "            'lp': str(lp),\n",
    "            'brightness': brightness,\n",
    "            'blurriness': blurriness,\n",
    "            'lp_indexes': lp_numbers\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "\n",
    "        # Parse the filename to get the associated metadata\n",
    "        metadata = self.parse_filename(img_name)\n",
    "\n",
    "        if self.cropped:    #I use this dataset for both baselines, so I check if I need to skip detection part and use dataset bbox.\n",
    "            #I can use the crop method of PIL, that crops the image using coords in this way: (left, upper, right, lower)\n",
    "            '''\n",
    "            left is the x-coordinate of the left edge.\n",
    "\n",
    "            upper is the y-coordinate of the top edge.\n",
    "\n",
    "            right is the x-coordinate of the right edge.\n",
    "\n",
    "            lower is the y-coordinate of the bottom edge.\n",
    "            seen on the online odcs of pillow\n",
    "            '''\n",
    "            bboox_coords = metadata['bbox_coords']\n",
    "            \n",
    "            left = int(bboox_coords[0][0])   # x-coordinate of the left edge\n",
    "            upper = int(bboox_coords[0][1])  # y-coordinate of the top edge\n",
    "            right = int(bboox_coords[1][0])  # x-coordinate of the right edge\n",
    "            lower = int(bboox_coords[1][1])  # y-coordinate of the bottom edge\n",
    "\n",
    "            image = image.crop((left, upper, right, lower))\n",
    "\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(metadata['lp_indexes'], dtype=torch.long)  # Return the image and the license plate indexes as a tensor, for the CNN to elaborate\n",
    "\n",
    "    #I included this method in the above one, with the if cropped check. I dunno if i'm gonna need it anymore. COMMENTED FOR NOW\n",
    "    '''def get_cropped_plate(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        #now I crop the image using the bbox coords that I have in the metadata\n",
    "        metadata = self.parse_filename(img_name)\n",
    "        #I can use the crop method of PIL, that crops the image using coords in this way: (left, upper, right, lower)\n",
    "        \n",
    "        #left is the x-coordinate of the left edge.\n",
    "\n",
    "        #upper is the y-coordinate of the top edge.\n",
    "\n",
    "        #right is the x-coordinate of the right edge.\n",
    "\n",
    "        #lower is the y-coordinate of the bottom edge.\n",
    "        #seen on the online odcs of pillow\n",
    "        \n",
    "        bboox_coords = metadata['bbox_coords']\n",
    "        \n",
    "        left = int(bboox_coords[0][0])   # x-coordinate of the left edge\n",
    "        upper = int(bboox_coords[0][1])  # y-coordinate of the top edge\n",
    "        right = int(bboox_coords[1][0])  # x-coordinate of the right edge\n",
    "        lower = int(bboox_coords[1][1])  # y-coordinate of the bottom edge\n",
    "\n",
    "        cropped_lp = image.crop((left, upper, right, lower))\n",
    "\n",
    "        if self.transform:\n",
    "            cropped_lp = self.transform(cropped_lp)\n",
    "\n",
    "        return cropped_lp, metadata['lp']'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Iterate through data, to check if everything is working\\nfor images, metadata in dataloader_train:\\n    # Process your data here\\n    break'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train = CarPlateDataset(img_dir='./Data/train', transform=transform, cropped=True)\n",
    "dataset_eval = CarPlateDataset(img_dir='./Data/eval', transform=transform, cropped=True)\n",
    "dataset_test = CarPlateDataset(img_dir='./Data/test', transform=transform, cropped=True)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n",
    "dataloader_eval = DataLoader(dataset_eval, batch_size=32, shuffle=False)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False)\n",
    "\n",
    "'''# Iterate through data, to check if everything is working\n",
    "for images, metadata in dataloader_train:\n",
    "    # Process your data here\n",
    "    break'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhLCflR28fxo"
   },
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "0vF1JP4dutN_"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = PlateCNN(num_provinces=len(provinces), num_alphabets=len(alphabets), num_ads=len(ads)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjiqGK4q42zP"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "fv-GRhwZutAf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.6187, Eval Acc: 0.0000, Char Acc: 0.5408\n",
      "Epoch 2, Loss: 8.6190, Eval Acc: 0.0000, Char Acc: 0.5471\n",
      "Epoch 3, Loss: 8.5933, Eval Acc: 0.0000, Char Acc: 0.5440\n",
      "Epoch 4, Loss: 8.5878, Eval Acc: 0.0000, Char Acc: 0.5432\n",
      "Epoch 5, Loss: 8.5816, Eval Acc: 0.0000, Char Acc: 0.5463\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    loss = train_one_epoch(model, dataloader_train, optimizer, criterion, device)\n",
    "    #acc = evaluate(model, dataloader_eval, device)\n",
    "    acc, acc_char = evaluate(model, dataloader_eval, device)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Eval Acc: {acc:.4f}, Char Acc: {acc_char:.4f}\")\n",
    "    #print(f\"Epoch {epoch+1}, Loss: {loss:.4f}, Eval Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A05HEglkxQPy"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lUZt02eiusq0"
   },
   "outputs": [],
   "source": [
    "test_acc = evaluate(model, dataloader_test, device)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels shape: torch.Size([32, 7])\n",
      "labels min: 0 labels max: 33\n",
      "province max: 33 alpha max: 24 ads max: 34\n"
     ]
    }
   ],
   "source": [
    "for images, labels in dataloader_train:\n",
    "    print(\"labels shape:\", labels.shape)\n",
    "    print(\"labels min:\", labels.min().item(), \"labels max:\", labels.max().item())\n",
    "    print(\"province max:\", len(provinces)-1, \"alpha max:\", len(alphabets)-1, \"ads max:\", len(ads)-1)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
