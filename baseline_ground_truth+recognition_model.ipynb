{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08b2799f",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec4ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "!cp -r /content/drive/MyDrive/ccpd_green/ /content/ \n",
    "!cp -r /content/drive/MyDrive/ccpd_green/test /content/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3515",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBALS FOR THE TEXT OF THE CAR PLATE\n",
    "\n",
    "provinces = [\"皖\", \"沪\", \"津\", \"渝\", \"冀\", \"晋\", \"蒙\", \"辽\", \"吉\", \"黑\", \"苏\", \"浙\", \"京\", \"闽\", \"赣\", \"鲁\", \"豫\", \"鄂\", \"湘\", \"粤\", \"桂\", \"琼\", \"川\", \"贵\", \"云\", \"藏\", \"陕\", \"甘\", \"青\", \"宁\", \"新\", \"警\", \"学\", \"O\"]\n",
    "alphabets = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'O']\n",
    "ads = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'J', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'O']\n",
    "\n",
    "# PREPARING THE ENCONDING AND DECODING\n",
    "\n",
    "seen = set()    \n",
    "MY_DICTIONARY = []\n",
    "for char_list in [provinces, alphabets, ads]:\n",
    "    for char in char_list:\n",
    "        if char not in seen:\n",
    "            MY_DICTIONARY.append(char)\n",
    "            seen.add(char)\n",
    "\n",
    "# ENCODING AND DECODING            \n",
    "\n",
    "char2idx = {c: i for i, c in enumerate(MY_DICTIONARY)}\n",
    "idx2char = {i: c for i, c in enumerate(MY_DICTIONARY)}\n",
    "BLANK_IDX = len(MY_DICTIONARY) \n",
    "\n",
    "# TRANSFORMATIONS\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 256)), # Aspect ratio più realistico per targhe\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72ac0c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203bc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING DATASET STRUCTURE\n",
    "\n",
    "def validate_dictionary():\n",
    "    print(\"=== validating dict ===\")\n",
    "    print(f\"Provinces: {len(provinces)} chars\")\n",
    "    print(f\"Alphabets: {len(alphabets)} chars\")\n",
    "    print(f\"Ads: {len(ads)} chars\")\n",
    "    print(f\"Final dict: {len(MY_DICTIONARY)} unique chars\")\n",
    "\n",
    "    all_chars = set(provinces + alphabets + ads)\n",
    "    dict_chars = set(MY_DICTIONARY)\n",
    "\n",
    "    if all_chars == dict_chars:\n",
    "        print(\"all chars are included in the dict\")\n",
    "    else:\n",
    "        missing = all_chars - dict_chars\n",
    "        extra = dict_chars - all_chars\n",
    "        if missing:\n",
    "            print(f\"missing chars: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"there's extra chars!: {extra}\")\n",
    "\n",
    "    test_filename = \"025-95_113-154&383_386&473-386&473_177&454_154&383_363&402-0_0_22_27_27_33_16-37-15.jpg\"\n",
    "    print(f\"\\n=== Test Parsing ===\")\n",
    "    print(f\"Test filename: {test_filename}\")\n",
    "\n",
    "    fields = test_filename.split('-')\n",
    "    indices = fields[4].split(\"_\")\n",
    "    test_plate = provinces[int(indices[0])] + alphabets[int(indices[1])] + \"\".join([ads[int(i)] for i in indices[2:]])\n",
    "    print(f\"parsed plate: '{test_plate}'\")\n",
    "\n",
    "    missing_chars = [c for c in test_plate if c not in char2idx]\n",
    "    if missing_chars:\n",
    "        print(f\"missing chars in dict: {missing_chars}\")\n",
    "    else:\n",
    "        print(\"the dictionary is ok!\")\n",
    "\n",
    "validate_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e91ec",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fa69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CLASS\n",
    "\n",
    "class CarPlateDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, transform=None, cropped = False):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.image_names = os.listdir(img_dir)\n",
    "        self.cropped = cropped\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def parse_filename(self, filename):\n",
    "        fields = filename.split('-')\n",
    "        area = float(fields[0]) / 100  #filename encodes the area in percentage (ratio plate-no plate area), so divising by 100 gives me a 0-1 range\n",
    "        tilt_degree = fields[1].split('_')\n",
    "        h_tilt = int(tilt_degree[0])    #horizontal tilt degree\n",
    "        v_tilt = int(tilt_degree[1])    #vertical tilt degree\n",
    "        tilt_list = np.array([h_tilt, v_tilt], dtype=np.float32)\n",
    "\n",
    "\n",
    "        bbox_coords = fields[2].split('_')  #bounding box coordinates\n",
    "        leftUp_bbox = bbox_coords[0].split('&')\n",
    "        leftUp_bbox_x = int(leftUp_bbox[0])\n",
    "        leftUp_bbox_y = int(leftUp_bbox[1])\n",
    "        rightBottom_bbox = bbox_coords[1].split('&')\n",
    "        rightDown_bbox_x = int(rightBottom_bbox[0])\n",
    "        rightDown_bbox_y = int(rightBottom_bbox[1])\n",
    "        bbox_coords_list = np.array([(leftUp_bbox_x, leftUp_bbox_y),\n",
    "                                    (rightDown_bbox_x, rightDown_bbox_y)], dtype=np.float32)\n",
    "\n",
    "        vertices = fields[3].split('_')  #vertices of the plate\n",
    "        right_bottom_vertex = vertices[0].split('&')\n",
    "        right_bottom_vertex_x = int(right_bottom_vertex[0])\n",
    "        right_bottom_vertex_y = int(right_bottom_vertex[1])\n",
    "        left_bottom_vertex = vertices[1].split('&')\n",
    "        left_bottom_vertex_x = int(left_bottom_vertex[0])\n",
    "        left_bottom_vertex_y = int(left_bottom_vertex[1])\n",
    "        left_up_vertex = vertices[2].split('&')\n",
    "        left_up_vertex_x = int(left_up_vertex[0])\n",
    "        left_up_vertex_y = int(left_up_vertex[1])\n",
    "        right_up_vertex = vertices[3].split('&')\n",
    "        right_up_vertex_x = int(right_up_vertex[0])\n",
    "        right_up_vertex_y = int(right_up_vertex[1])\n",
    "\n",
    "        vertices_list = np.array([(left_bottom_vertex_x, left_bottom_vertex_y),\n",
    "                                (right_bottom_vertex_x, right_bottom_vertex_y),\n",
    "                                (right_up_vertex_x, right_up_vertex_y),\n",
    "                                (left_up_vertex_x, left_up_vertex_y)], dtype=np.float32)\n",
    "\n",
    "        text=str(fields[4])\n",
    "        indices=text.split(\"_\")\n",
    "        province_character=provinces[int(indices[0])]\n",
    "        alphabet_character=alphabets[int(indices[1])]\n",
    "        ads_charachters=[ads[int(i)] for i in indices[2:]]\n",
    "        plate_text=province_character+alphabet_character+\"\".join(ads_charachters)\n",
    "\n",
    "        brightness = int(fields[5])\n",
    "        blurriness_str = fields[6].replace('.jpg', '')\n",
    "        match = re.match(r'\\d+', blurriness_str)\n",
    "        if match:\n",
    "            blurriness = int(match.group())\n",
    "        else:\n",
    "            print(f\"[WARNING] File '{filename}': blurriness non standard '{fields[6]}', imposto a 0.\")\n",
    "            blurriness = 0\n",
    "\n",
    "        # Convert license plate text to indices for CTC training\n",
    "        lp_indexes = []\n",
    "        for c in plate_text:\n",
    "            if c in char2idx:\n",
    "                lp_indexes.append(char2idx[c])\n",
    "            else:\n",
    "                print(f\"[WARNING] Carattere non riconosciuto '{c}' in '{plate_text}'\")\n",
    "\n",
    "        return {\n",
    "            'area': area,\n",
    "            'tilt': tilt_list,\n",
    "            'bbox_coords': bbox_coords_list,\n",
    "            'vertices': vertices_list,\n",
    "            'lp': plate_text,\n",
    "            'lp_indexes': lp_indexes,\n",
    "            'brightness': brightness,\n",
    "            'blurriness': blurriness,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        metadata = self.parse_filename(img_name)\n",
    "        if self.cropped:    #I use this dataset for both baselines, so I check if I need to skip detection part and use dataset bbox.\n",
    "            #I can use the crop method of PIL, that crops the image using coords in this way: (left, upper, right, lower)\n",
    "            '''\n",
    "            left is the x-coordinate of the left edge.\n",
    "\n",
    "            upper is the y-coordinate of the top edge.\n",
    "\n",
    "            right is the x-coordinate of the right edge.\n",
    "\n",
    "            lower is the y-coordinate of the bottom edge.\n",
    "            seen on the online odcs of pillow\n",
    "            '''\n",
    "            bbox_coords = metadata['bbox_coords']\n",
    "\n",
    "            left = int(bbox_coords[0][0])   # x-coordinate of the left edge\n",
    "            upper = int(bbox_coords[0][1])  # y-coordinate of the top edge\n",
    "            right = int(bbox_coords[1][0])  # x-coordinate of the right edge\n",
    "            lower = int(bbox_coords[1][1])  # y-coordinate of the bottom edge\n",
    "\n",
    "            image = image.crop((left, upper, right, lower))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(metadata['lp_indexes'], dtype=torch.long)  # Return the image and the license plate indexes as a tensor, for the CNN to elaborate\n",
    "\n",
    "# COLLATE FUNCTION\n",
    "\n",
    "def ctc_collate_fn(batch):\n",
    "    '''\n",
    "    basically what I do here is stacking all the images in a batch into a single tensor and\n",
    "    then computing the len of each label (assuming different lenght plate can happen). (I could actually avoid this but it's more general)\n",
    "    Finally just concatenating all the labels into a vector (pytorch CTC wantres them in a line, not list)\n",
    "    then returning image-label-its lenght.\n",
    "    I need this to tell CTC where labels finish and i do not care padding as CTC deals with that internally (NICE)\n",
    "    '''\n",
    "    images, labels = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    label_lengths = torch.tensor([len(l) for l in labels], dtype=torch.long)\n",
    "    labels = torch.cat(labels)\n",
    "    return images, labels, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fd252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOICE OF THE DATASET\n",
    "\n",
    "dataset_train = CarPlateDataset(img_dir='/content/ccpd_green/train', transform=transform, cropped=True)\n",
    "dataset_eval = CarPlateDataset(img_dir='/content/ccpd_green/val', transform=transform, cropped=True)\n",
    "dataset_test = CarPlateDataset(img_dir='/content/ccpd_green/test', transform=transform, cropped=True)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True, collate_fn=ctc_collate_fn)\n",
    "dataloader_eval = DataLoader(dataset_eval, batch_size=32, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=False, collate_fn=ctc_collate_fn)\n",
    "\n",
    "print(f\"Train set size: {len(dataset_train)}\")\n",
    "print(f\"Eval set size: {len(dataset_eval)}\")\n",
    "print(f\"Test set size: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42967f28",
   "metadata": {},
   "source": [
    "# Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed66c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_classes, input_channels=1):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, 3, 1, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, None))  # (height=1, width stays)\n",
    "        )\n",
    "        self.rnn = nn.LSTM(256, 128, num_layers=2, bidirectional=True)\n",
    "        self.fc = nn.Linear(128*2, num_classes)  # bidirectional\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)  # [B, C, 1, W]\n",
    "        x = x.squeeze(2) # [B, C, W]\n",
    "        x = x.permute(2, 0, 1)  # [W, B, C]\n",
    "        x, _ = self.rnn(x)\n",
    "        x = self.fc(x)  # [W, B, num_classes]\n",
    "        return x  # output for CTC: [seq_len, batch, num_classes]\n",
    "\n",
    "# DECODER FUNCTION\n",
    "\n",
    "def ctc_greedy_decoder(output, idx2char, blank=BLANK_IDX):\n",
    "    '''\n",
    "    Now, I know the network returns probabilities, as it does a softmax with logits of characters.\n",
    "    I need to transform that probability into an actual char to compose the plate.\n",
    "    I take the argmax of the softmax (most prob char), remove blanks used by CTC and possible\n",
    "    duplicates CTC can actually produce.\n",
    "    At the end I simply use the  mappings char-index index-char deefined at the beginning to compose the plate.\n",
    "    This is greedy as it just takes the argmax of every step, I think it's more than enough here.\n",
    "    '''\n",
    "    # output: [seq_len, batch, num_classes]\n",
    "    out = output.permute(1, 0, 2)  # [batch, seq_len, num_classes]\n",
    "    pred_strings = []\n",
    "    for probs in out:\n",
    "        pred = probs.argmax(1).cpu().numpy()\n",
    "        prev = -1\n",
    "        pred_str = []\n",
    "        for p in pred:\n",
    "            if p != blank and p != prev:\n",
    "                pred_str.append(idx2char[p])\n",
    "            prev = p\n",
    "        pred_strings.append(''.join(pred_str))\n",
    "    return pred_strings\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels, label_lengths in dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        label_lengths = label_lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)  # [W, B, num_classes]\n",
    "        log_probs = outputs.log_softmax(2)\n",
    "        input_lengths = torch.full(size=(images.size(0),), fill_value=log_probs.size(0), dtype=torch.long).to(device)\n",
    "        loss = criterion(log_probs, labels, input_lengths, label_lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch average loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# EVLUATE FUNCTION\n",
    "\n",
    "def evaluate(model, dataloader, device, verbose=False):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    # Metriche aggiuntive per analisi dettagliata\n",
    "    length_errors = 0\n",
    "    province_correct = 0\n",
    "    alphabet_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, label_lengths) in enumerate(dataloader):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            pred_strings = ctc_greedy_decoder(outputs, idx2char)\n",
    "            labels_cpu = labels.cpu().numpy()   #need to move tensors tu cpu memory for numpy\n",
    "            lengths_cpu = label_lengths.cpu().numpy()\n",
    "            idx = 0\n",
    "            gt_strings = []\n",
    "            for l in lengths_cpu:\n",
    "                gt = ''.join([idx2char[i] for i in labels_cpu[idx:idx+l]])\n",
    "                gt_strings.append(gt)\n",
    "                idx += l\n",
    "\n",
    "            for pred, gt in zip(pred_strings, gt_strings):\n",
    "                # Accuracy completa\n",
    "                if pred == gt:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "                # Accuracy per carattere\n",
    "                min_len = min(len(pred), len(gt))\n",
    "                correct_chars += sum([p == g for p, g in zip(pred[:min_len], gt[:min_len])])\n",
    "                total_chars += len(gt)\n",
    "\n",
    "                # Metriche aggiuntive\n",
    "                if len(pred) != len(gt):\n",
    "                    length_errors += 1\n",
    "\n",
    "                # Accuracy per provincia (primo carattere)\n",
    "                if len(pred) > 0 and len(gt) > 0 and pred[0] == gt[0]:\n",
    "                    province_correct += 1\n",
    "\n",
    "                # Accuracy per alfabeto (secondo carattere)\n",
    "                if len(pred) > 1 and len(gt) > 1 and pred[1] == gt[1]:\n",
    "                    alphabet_correct += 1\n",
    "\n",
    "                # Stampa esempi di errore se richiesto\n",
    "                if verbose and pred != gt and batch_idx == 0:\n",
    "                    print(f\"Pred: '{pred}' | GT: '{gt}'\")\n",
    "\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    acc_char = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    length_error_rate = length_errors / total if total > 0 else 0\n",
    "    province_acc = province_correct / total if total > 0 else 0\n",
    "    alphabet_acc = alphabet_correct / total if total > 0 else 0\n",
    "\n",
    "    print(f\"Eval accuracy (full plate): {acc:.4f} | Char accuracy: {acc_char:.4f}\")\n",
    "    print(f\"Length error rate: {length_error_rate:.4f} | Province acc: {province_acc:.4f} | Alphabet acc: {alphabet_acc:.4f}\")\n",
    "\n",
    "    return acc, acc_char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa1bb1",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36435d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING SET UP\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CRNN(num_classes=len(MY_DICTIONARY)+1).to(device)  # +1 per blank\n",
    "print(f\"Model created with {len(MY_DICTIONARY)+1} output classes\")\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  \n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=5, verbose=True)\n",
    "ctc_loss = nn.CTCLoss(blank=BLANK_IDX, zero_infinity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab5433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING PARAMETERS\n",
    "\n",
    "NUM_EPOCHS = 80 # I did a run on 400 epochs tracked with wandb (see metrics pictures). I found out best epoch is around 75, after that the loss stayies steady, along with acc.\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "patience_counter = 0\n",
    "early_stopping_patience = 15  # Stop training if no improvement for 15 epochs\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training for {NUM_EPOCHS} epochs with early stopping (patience={early_stopping_patience})\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "\n",
    "    # Training\n",
    "    train_loss = train_one_epoch(model, dataloader_train, optimizer, ctc_loss, device)\n",
    "\n",
    "    # Validation\n",
    "    val_acc, val_acc_char = evaluate(model, dataloader_eval, device, verbose=(epoch % 10 == 0)) # basically every 10 epochs i print some examples of bad predictions (see val function)\n",
    "\n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_acc)\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Accuracy (full plate): {val_acc:.4f} | Char Accuracy: {val_acc_char:.4f} | LR: {current_lr:.6f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "        patience_counter = 0\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'val_acc': val_acc,\n",
    "            'val_acc_char': val_acc_char,\n",
    "            'train_loss': train_loss\n",
    "        }, \"best_crnn_ctc_model.pth\")\n",
    "        print(f\"==> New best model saved at epoch {best_epoch} with acc {best_val_acc:.4f}\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        print(f\"No improvement. Patience: {patience_counter}/{early_stopping_patience}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if patience_counter >= early_stopping_patience:\n",
    "        print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "        break\n",
    "\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best model at epoch {best_epoch} with val acc {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31120908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING THE MODEL\n",
    "\n",
    "source_path = 'best_crnn_ctc_model.pth'\n",
    "\n",
    "# Destination directory in Google Drive (replace with your desired path)\n",
    "destination_dir = '/content/drive/MyDrive/SavedModels/'\n",
    "\n",
    "# Create the destination directory if it doesn't exist\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Construct the full destination path\n",
    "destination_path = os.path.join(destination_dir, 'best_crnn_ctc_model.pth')\n",
    "\n",
    "# Copy the file\n",
    "shutil.copyfile(source_path, destination_path)\n",
    "\n",
    "print(f\"Model saved to: {destination_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268d62a6",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE MODEL\n",
    "\n",
    "model_path = '/content/drive/MyDrive/SavedModels/best_crnn_ctc_model.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(model_path, map_location=device)\n",
    "\n",
    "# model instance\n",
    "model = CRNN(num_classes=len(MY_DICTIONARY)+1).to(device)\n",
    "\n",
    "# Load wheights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "print(f\"Model loaded successfully from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Best validation accuracy was: {checkpoint['val_acc']:.4f}\")\n",
    "print(f\"Best validation char accuracy was: {checkpoint['val_acc_char']:.4f}\")\n",
    "\n",
    "# Test\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Testing on test set with loaded model:\")\n",
    "print(\"=\"*50)\n",
    "test_acc, test_acc_char = evaluate(model, dataloader_test, device, verbose=True)\n",
    "print(f\"\\nFinal Test Results:\")\n",
    "print(f\"Test Accuracy (full plate): {test_acc:.4f}\")\n",
    "print(f\"Test Character Accuracy: {test_acc_char:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
